2017-07-19 19:13:11.538762: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-19 19:13:11.538832: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-19 19:13:11.538836: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-07-19 19:13:11.538840: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-19 19:13:11.538843: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-07-19 19:13:36.406402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-07-19 19:13:36.408338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.562
pciBusID 0000:00:04.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-07-19 19:13:36.608830: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x421cf40 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2017-07-19 19:13:36.609554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-07-19 19:13:36.611532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 1 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.562
pciBusID 0000:00:05.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-07-19 19:13:36.801424: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x4220d90 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2017-07-19 19:13:36.802431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-07-19 19:13:36.804042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 2 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.562
pciBusID 0000:00:06.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-07-19 19:13:36.994969: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x42455e0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2017-07-19 19:13:36.995753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-07-19 19:13:36.997586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 3 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.562
pciBusID 0000:00:07.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-07-19 19:13:37.018251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 1 2 3 
2017-07-19 19:13:37.018292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y Y Y Y 
2017-07-19 19:13:37.018296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y Y Y Y 
2017-07-19 19:13:37.018299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 2:   Y Y Y Y 
2017-07-19 19:13:37.018303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 3:   Y Y Y Y 
2017-07-19 19:13:37.018337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0)
2017-07-19 19:13:37.018343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:00:05.0)
2017-07-19 19:13:37.018347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:00:06.0)
2017-07-19 19:13:37.018350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:00:07.0)
2017-07-19 19:13:37.593666: I tensorflow/core/common_runtime/simple_placer.cc:675] Ignoring device specification /device:GPU:3 for node 'tower_3/fifo_queue_Dequeue' because the input edge from 'prefetch_queue/fifo_queue' is a reference connection and already has a device field set to /device:CPU:0
2017-07-19 19:13:37.593808: I tensorflow/core/common_runtime/simple_placer.cc:675] Ignoring device specification /device:GPU:2 for node 'tower_2/fifo_queue_Dequeue' because the input edge from 'prefetch_queue/fifo_queue' is a reference connection and already has a device field set to /device:CPU:0
2017-07-19 19:13:37.593825: I tensorflow/core/common_runtime/simple_placer.cc:675] Ignoring device specification /device:GPU:1 for node 'tower_1/fifo_queue_Dequeue' because the input edge from 'prefetch_queue/fifo_queue' is a reference connection and already has a device field set to /device:CPU:0
2017-07-19 19:13:37.593843: I tensorflow/core/common_runtime/simple_placer.cc:675] Ignoring device specification /device:GPU:0 for node 'tower_0/fifo_queue_Dequeue' because the input edge from 'prefetch_queue/fifo_queue' is a reference connection and already has a device field set to /device:CPU:0
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-07-19 19:13:40.798581: step 0, loss = 4.68 (177.6 examples/sec; 0.721 sec/batch)
2017-07-19 19:13:41.621697: step 10, loss = 4.61 (11907.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:42.057662: step 20, loss = 4.48 (11773.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:42.496098: step 30, loss = 4.35 (11758.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:42.927872: step 40, loss = 4.33 (11969.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:43.364390: step 50, loss = 4.38 (11796.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:43.798293: step 60, loss = 4.44 (11742.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:44.232783: step 70, loss = 4.41 (11922.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:44.663733: step 80, loss = 4.12 (11726.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:45.094783: step 90, loss = 4.12 (12018.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:45.523926: step 100, loss = 4.09 (11986.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:46.136829: step 110, loss = 4.03 (11350.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:46.570154: step 120, loss = 3.96 (11745.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:47.004115: step 130, loss = 3.89 (12017.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:47.437456: step 140, loss = 3.97 (11929.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:47.870256: step 150, loss = 3.94 (11889.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:48.305676: step 160, loss = 3.79 (11690.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:48.738736: step 170, loss = 3.87 (11917.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:49.171678: step 180, loss = 3.93 (11982.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:49.606507: step 190, loss = 3.91 (11605.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:50.043041: step 200, loss = 3.75 (11943.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:50.653428: step 210, loss = 3.69 (11777.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:51.083993: step 220, loss = 3.88 (11956.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:51.514250: step 230, loss = 3.58 (12016.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:51.945784: step 240, loss = 3.70 (11899.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:52.386488: step 250, loss = 3.59 (12035.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:52.824820: step 260, loss = 3.49 (11806.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:53.258285: step 270, loss = 3.58 (11837.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:53.691587: step 280, loss = 3.53 (11845.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:54.125930: step 290, loss = 3.48 (11787.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:54.556484: step 300, loss = 3.38 (11812.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:55.183816: step 310, loss = 3.42 (11077.5 examples/sec; 0.012 sec/batch)
2017-07-19 19:13:55.616357: step 320, loss = 3.40 (11792.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:56.049783: step 330, loss = 3.39 (11775.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:56.480829: step 340, loss = 3.49 (11925.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:56.912884: step 350, loss = 3.46 (11865.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:57.342309: step 360, loss = 3.38 (11935.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:57.777731: step 370, loss = 3.33 (11940.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:58.211460: step 380, loss = 3.43 (11821.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:58.644897: step 390, loss = 3.43 (11903.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:59.078312: step 400, loss = 3.37 (11715.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:13:59.682781: step 410, loss = 3.28 (11911.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:00.116947: step 420, loss = 3.23 (11686.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:00.551487: step 430, loss = 3.31 (11760.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:00.987053: step 440, loss = 3.18 (11937.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:01.417397: step 450, loss = 3.31 (11816.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:01.852024: step 460, loss = 3.26 (12075.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:02.283062: step 470, loss = 3.28 (11775.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:02.713450: step 480, loss = 3.10 (12081.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:03.143360: step 490, loss = 2.96 (11835.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:03.573827: step 500, loss = 2.94 (11865.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:04.173411: step 510, loss = 3.12 (12112.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:04.608817: step 520, loss = 2.97 (10666.5 examples/sec; 0.012 sec/batch)
2017-07-19 19:14:05.041474: step 530, loss = 3.05 (11696.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:05.472405: step 540, loss = 2.86 (11858.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:05.903464: step 550, loss = 2.91 (12087.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:06.337066: step 560, loss = 2.97 (11930.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:06.769536: step 570, loss = 3.01 (11619.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:07.201899: step 580, loss = 2.88 (11813.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:07.632455: step 590, loss = 2.97 (11845.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:08.061395: step 600, loss = 3.21 (11844.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:08.664331: step 610, loss = 2.70 (11732.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:09.104174: step 620, loss = 2.84 (11697.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:09.539285: step 630, loss = 2.67 (11798.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:09.972800: step 640, loss = 2.82 (11943.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:10.402775: step 650, loss = 2.96 (11819.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:10.839456: step 660, loss = 2.89 (11925.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:11.269970: step 670, loss = 2.90 (11563.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:11.704124: step 680, loss = 2.71 (11714.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:12.135088: step 690, loss = 2.90 (11986.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:12.568225: step 700, loss = 2.72 (11772.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:13.173641: step 710, loss = 2.78 (11995.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:13.602596: step 720, loss = 2.65 (11915.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:14.036658: step 730, loss = 2.62 (11938.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:14.470571: step 740, loss = 2.64 (11843.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:14.905069: step 750, loss = 2.72 (11687.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:15.337156: step 760, loss = 2.77 (11918.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:15.767676: step 770, loss = 2.84 (11948.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:16.202066: step 780, loss = 2.47 (11831.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:16.631826: step 790, loss = 2.72 (12021.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:17.063014: step 800, loss = 2.55 (11883.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:17.662548: step 810, loss = 2.57 (11902.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:18.097649: step 820, loss = 2.80 (11653.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:18.528992: step 830, loss = 2.75 (11750.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:18.968354: step 840, loss = 2.49 (11785.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:19.405577: step 850, loss = 2.49 (11724.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:19.835809: step 860, loss = 2.47 (11954.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:20.268354: step 870, loss = 2.49 (11814.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:20.702054: step 880, loss = 2.58 (11775.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:21.132655: step 890, loss = 2.38 (11883.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:21.564104: step 900, loss = 2.38 (11718.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:22.178345: step 910, loss = 2.54 (11947.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:22.606374: step 920, loss = 2.44 (11931.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:23.038529: step 930, loss = 2.33 (11818.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:23.465552: step 940, loss = 2.60 (12202.3 examples/sec; 0.010 sec/batch)
2017-07-19 19:14:23.892720: step 950, loss = 2.34 (12083.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:24.322795: step 960, loss = 2.56 (12073.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:24.750037: step 970, loss = 2.27 (11960.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:25.179762: step 980, loss = 2.48 (11974.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:25.609669: step 990, loss = 2.39 (11824.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:26.053302: step 1000, loss = 2.43 (9259.9 examples/sec; 0.014 sec/batch)
2017-07-19 19:14:26.764775: step 1010, loss = 2.52 (11920.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:27.196781: step 1020, loss = 2.39 (11873.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:27.624562: step 1030, loss = 2.21 (11820.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:28.053925: step 1040, loss = 2.21 (11996.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:28.483532: step 1050, loss = 2.21 (11740.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:28.912895: step 1060, loss = 2.23 (11858.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:29.342460: step 1070, loss = 2.33 (11998.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:29.774797: step 1080, loss = 2.38 (11888.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:30.204902: step 1090, loss = 2.24 (12038.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:30.643914: step 1100, loss = 2.10 (12044.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:31.250682: step 1110, loss = 2.27 (11672.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:31.688450: step 1120, loss = 2.21 (11942.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:32.116993: step 1130, loss = 2.19 (11773.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:32.553844: step 1140, loss = 2.05 (11749.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:32.984488: step 1150, loss = 2.04 (11945.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:33.418699: step 1160, loss = 2.26 (11976.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:33.849325: step 1170, loss = 2.13 (11935.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:34.281103: step 1180, loss = 2.20 (11911.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:34.713593: step 1190, loss = 2.13 (12063.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:35.146302: step 1200, loss = 2.10 (11754.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:35.747273: step 1210, loss = 2.09 (11954.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:36.179515: step 1220, loss = 2.24 (11951.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:36.609685: step 1230, loss = 2.08 (12013.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:37.041778: step 1240, loss = 1.89 (11887.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:37.470864: step 1250, loss = 2.00 (12021.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:37.904534: step 1260, loss = 2.05 (11870.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:38.337489: step 1270, loss = 2.12 (11994.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:38.771035: step 1280, loss = 2.13 (11875.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:39.207872: step 1290, loss = 2.13 (11486.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:39.638289: step 1300, loss = 1.89 (11843.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:40.227905: step 1310, loss = 1.94 (11788.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:40.658908: step 1320, loss = 1.79 (11642.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:41.093495: step 1330, loss = 1.97 (11864.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:41.530117: step 1340, loss = 1.99 (11799.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:41.959689: step 1350, loss = 2.12 (11948.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:42.387357: step 1360, loss = 2.02 (11961.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:42.817419: step 1370, loss = 2.00 (11850.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:43.248393: step 1380, loss = 2.09 (11591.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:43.680486: step 1390, loss = 2.13 (11639.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:44.113020: step 1400, loss = 1.77 (11887.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:44.709353: step 1410, loss = 1.76 (11836.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:45.151983: step 1420, loss = 1.78 (10593.2 examples/sec; 0.012 sec/batch)
2017-07-19 19:14:45.584368: step 1430, loss = 1.71 (11917.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:46.013570: step 1440, loss = 1.76 (11975.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:46.442123: step 1450, loss = 1.87 (11988.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:46.872275: step 1460, loss = 1.86 (11987.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:47.302197: step 1470, loss = 1.78 (11983.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:47.735437: step 1480, loss = 1.87 (11939.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:48.169244: step 1490, loss = 1.87 (11881.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:48.598552: step 1500, loss = 1.95 (12137.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:49.192105: step 1510, loss = 1.80 (11832.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:49.621338: step 1520, loss = 1.78 (11821.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:50.050769: step 1530, loss = 1.83 (11992.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:50.483402: step 1540, loss = 1.72 (11847.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:50.914386: step 1550, loss = 1.77 (12000.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:51.343432: step 1560, loss = 1.85 (11801.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:51.772421: step 1570, loss = 1.75 (11990.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:52.203187: step 1580, loss = 1.71 (11892.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:52.633552: step 1590, loss = 1.90 (11909.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:53.065487: step 1600, loss = 1.80 (11907.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:53.652841: step 1610, loss = 1.71 (11947.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:54.081680: step 1620, loss = 1.59 (11860.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:54.510515: step 1630, loss = 1.86 (11993.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:54.939147: step 1640, loss = 1.89 (11928.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:55.368270: step 1650, loss = 1.80 (11948.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:55.797501: step 1660, loss = 1.72 (11745.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:56.227885: step 1670, loss = 1.70 (11752.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:56.656375: step 1680, loss = 1.69 (11962.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:57.087469: step 1690, loss = 1.72 (11847.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:57.519999: step 1700, loss = 1.66 (11945.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:58.123595: step 1710, loss = 1.85 (12103.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:58.554179: step 1720, loss = 1.60 (11623.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:58.986041: step 1730, loss = 1.59 (11896.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:59.423929: step 1740, loss = 1.90 (11825.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:14:59.856422: step 1750, loss = 1.79 (11807.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:00.288382: step 1760, loss = 1.64 (12032.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:00.718102: step 1770, loss = 1.62 (11976.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:01.151640: step 1780, loss = 1.59 (11923.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:01.589872: step 1790, loss = 1.56 (11911.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:02.020376: step 1800, loss = 1.54 (11855.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:02.615669: step 1810, loss = 1.74 (11920.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:03.048363: step 1820, loss = 1.58 (11985.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:03.480077: step 1830, loss = 1.59 (12073.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:03.910089: step 1840, loss = 1.48 (11847.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:04.338981: step 1850, loss = 1.82 (11709.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:04.771283: step 1860, loss = 1.64 (11733.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:05.202064: step 1870, loss = 1.76 (11639.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:05.635008: step 1880, loss = 1.62 (11846.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:06.066016: step 1890, loss = 1.62 (11986.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:06.501624: step 1900, loss = 1.67 (11926.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:07.090215: step 1910, loss = 1.54 (11899.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:07.518050: step 1920, loss = 1.55 (12067.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:07.950153: step 1930, loss = 1.54 (11829.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:08.384997: step 1940, loss = 1.36 (11148.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:08.823219: step 1950, loss = 1.91 (11856.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:09.251215: step 1960, loss = 1.67 (12017.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:09.681660: step 1970, loss = 1.63 (11781.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:10.112276: step 1980, loss = 1.54 (11607.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:10.544860: step 1990, loss = 1.52 (12020.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:10.975970: step 2000, loss = 1.57 (12047.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:11.684338: step 2010, loss = 1.60 (11812.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:12.113819: step 2020, loss = 1.35 (11799.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:12.542388: step 2030, loss = 1.27 (11962.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:12.974504: step 2040, loss = 1.35 (11693.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:13.404989: step 2050, loss = 1.47 (11545.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:13.835678: step 2060, loss = 1.45 (11612.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:14.271176: step 2070, loss = 1.38 (12051.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:14.700666: step 2080, loss = 1.47 (11978.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:15.131182: step 2090, loss = 1.65 (12017.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:15.559737: step 2100, loss = 1.56 (12144.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:16.148748: step 2110, loss = 1.46 (11968.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:16.578710: step 2120, loss = 1.48 (11919.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:17.005384: step 2130, loss = 1.41 (12007.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:17.433472: step 2140, loss = 1.39 (12029.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:17.862640: step 2150, loss = 1.38 (11978.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:18.293545: step 2160, loss = 1.48 (12060.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:18.727784: step 2170, loss = 1.37 (11791.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:19.159707: step 2180, loss = 1.44 (11909.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:19.589141: step 2190, loss = 1.37 (12077.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:20.021206: step 2200, loss = 1.34 (11988.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:20.619175: step 2210, loss = 1.38 (11930.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:21.050334: step 2220, loss = 1.35 (11922.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:21.487846: step 2230, loss = 1.39 (10493.1 examples/sec; 0.012 sec/batch)
2017-07-19 19:15:21.917217: step 2240, loss = 1.45 (12081.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:22.352470: step 2250, loss = 1.39 (12028.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:22.783620: step 2260, loss = 1.25 (12020.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:23.215381: step 2270, loss = 1.32 (11937.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:23.646223: step 2280, loss = 1.51 (11851.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:24.076418: step 2290, loss = 1.28 (12025.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:24.506621: step 2300, loss = 1.42 (12067.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:25.100285: step 2310, loss = 1.47 (11958.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:25.529013: step 2320, loss = 1.39 (12050.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:25.957754: step 2330, loss = 1.37 (11848.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:26.386932: step 2340, loss = 1.25 (12008.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:26.814645: step 2350, loss = 1.33 (11958.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:27.246327: step 2360, loss = 1.57 (11989.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:27.674971: step 2370, loss = 1.36 (11939.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:28.109289: step 2380, loss = 1.36 (11934.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:28.538612: step 2390, loss = 1.34 (11750.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:28.967603: step 2400, loss = 1.42 (12015.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:29.555107: step 2410, loss = 1.27 (12065.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:29.984599: step 2420, loss = 1.30 (11761.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:30.415473: step 2430, loss = 1.34 (11948.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:30.850661: step 2440, loss = 1.21 (12046.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:31.279924: step 2450, loss = 1.20 (11864.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:31.706661: step 2460, loss = 1.30 (12057.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:32.135758: step 2470, loss = 1.28 (11908.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:32.564208: step 2480, loss = 1.14 (11991.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:32.995250: step 2490, loss = 1.29 (12032.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:33.422978: step 2500, loss = 1.32 (11941.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:34.009417: step 2510, loss = 1.35 (12196.9 examples/sec; 0.010 sec/batch)
2017-07-19 19:15:34.436386: step 2520, loss = 1.31 (11939.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:34.865354: step 2530, loss = 1.21 (11924.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:35.295800: step 2540, loss = 1.19 (11906.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:35.726119: step 2550, loss = 1.34 (11899.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:36.155899: step 2560, loss = 1.33 (11796.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:36.584246: step 2570, loss = 1.42 (12069.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:37.012554: step 2580, loss = 1.24 (12040.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:37.440831: step 2590, loss = 1.25 (12002.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:37.869642: step 2600, loss = 1.27 (11965.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:38.459138: step 2610, loss = 1.23 (11806.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:38.890118: step 2620, loss = 1.26 (11959.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:39.320640: step 2630, loss = 1.22 (11967.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:39.749510: step 2640, loss = 1.28 (11941.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:40.184961: step 2650, loss = 1.31 (11726.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:40.616774: step 2660, loss = 1.20 (11771.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:41.045904: step 2670, loss = 1.22 (11882.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:41.483008: step 2680, loss = 1.26 (11674.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:41.920017: step 2690, loss = 1.05 (11942.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:42.359276: step 2700, loss = 1.20 (11991.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:42.953278: step 2710, loss = 1.27 (11982.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:43.386892: step 2720, loss = 1.07 (11785.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:43.817933: step 2730, loss = 1.23 (12052.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:44.249948: step 2740, loss = 1.12 (11755.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:44.678979: step 2750, loss = 1.15 (11922.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:45.108056: step 2760, loss = 1.37 (12002.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:45.534143: step 2770, loss = 1.16 (11948.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:45.965273: step 2780, loss = 1.28 (12082.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:46.390383: step 2790, loss = 1.17 (11982.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:46.830973: step 2800, loss = 1.26 (12042.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:47.444822: step 2810, loss = 1.24 (12014.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:47.870098: step 2820, loss = 1.32 (12123.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:48.299896: step 2830, loss = 1.13 (12019.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:48.728167: step 2840, loss = 1.24 (12093.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:49.157742: step 2850, loss = 1.06 (11743.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:49.588549: step 2860, loss = 1.09 (11915.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:50.017149: step 2870, loss = 1.09 (12117.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:50.445242: step 2880, loss = 1.18 (11827.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:50.875949: step 2890, loss = 1.04 (11771.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:51.304466: step 2900, loss = 1.06 (11836.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:51.904371: step 2910, loss = 1.29 (11966.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:52.335604: step 2920, loss = 1.25 (11878.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:52.764862: step 2930, loss = 1.07 (11959.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:53.198780: step 2940, loss = 1.06 (11849.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:53.630834: step 2950, loss = 1.18 (10941.7 examples/sec; 0.012 sec/batch)
2017-07-19 19:15:54.060464: step 2960, loss = 1.07 (12055.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:54.486710: step 2970, loss = 1.18 (11960.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:54.916997: step 2980, loss = 1.01 (11851.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:55.345148: step 2990, loss = 1.06 (11813.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:55.774263: step 3000, loss = 1.25 (11974.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:56.496658: step 3010, loss = 1.06 (11972.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:56.925389: step 3020, loss = 1.20 (11914.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:57.355582: step 3030, loss = 1.03 (11730.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:57.785994: step 3040, loss = 0.94 (11794.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:58.216043: step 3050, loss = 1.03 (11915.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:58.644813: step 3060, loss = 1.09 (11946.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:59.076522: step 3070, loss = 1.12 (11995.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:59.509807: step 3080, loss = 1.03 (11750.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:15:59.943266: step 3090, loss = 1.00 (12046.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:00.374993: step 3100, loss = 1.16 (11870.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:00.970391: step 3110, loss = 1.11 (11829.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:01.406499: step 3120, loss = 1.11 (11946.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:01.843319: step 3130, loss = 1.14 (11911.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:02.272807: step 3140, loss = 1.07 (12047.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:02.702585: step 3150, loss = 1.09 (12034.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:03.132651: step 3160, loss = 1.15 (11970.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:03.565311: step 3170, loss = 1.30 (11958.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:03.993302: step 3180, loss = 1.10 (11990.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:04.425810: step 3190, loss = 1.03 (11867.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:04.858961: step 3200, loss = 1.16 (11871.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:05.451203: step 3210, loss = 1.20 (12170.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:05.884675: step 3220, loss = 1.01 (11701.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:06.311876: step 3230, loss = 1.03 (12001.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:06.742836: step 3240, loss = 1.15 (11829.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:07.174559: step 3250, loss = 1.30 (11964.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:07.606463: step 3260, loss = 1.10 (11878.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:08.038562: step 3270, loss = 1.03 (11859.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:08.470844: step 3280, loss = 1.22 (12041.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:08.904407: step 3290, loss = 1.09 (11848.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:09.336939: step 3300, loss = 1.08 (11728.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:09.941699: step 3310, loss = 1.10 (11979.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:10.376614: step 3320, loss = 0.94 (11734.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:10.809156: step 3330, loss = 1.06 (11891.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:11.236644: step 3340, loss = 0.99 (12012.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:11.675112: step 3350, loss = 0.99 (11967.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:12.106014: step 3360, loss = 1.05 (12065.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:12.537880: step 3370, loss = 1.05 (11942.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:12.975247: step 3380, loss = 1.08 (11874.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:13.403163: step 3390, loss = 0.92 (11923.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:13.830956: step 3400, loss = 1.19 (11876.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:14.419958: step 3410, loss = 1.04 (12024.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:14.851256: step 3420, loss = 0.90 (11913.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:15.278985: step 3430, loss = 1.06 (11710.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:15.708947: step 3440, loss = 1.09 (12016.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:16.140203: step 3450, loss = 0.98 (12012.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:16.575315: step 3460, loss = 1.00 (12093.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:17.002946: step 3470, loss = 1.04 (12005.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:17.431124: step 3480, loss = 1.01 (11962.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:17.866211: step 3490, loss = 1.12 (12084.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:18.295120: step 3500, loss = 1.39 (11979.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:18.885263: step 3510, loss = 1.11 (12194.0 examples/sec; 0.010 sec/batch)
2017-07-19 19:16:19.320487: step 3520, loss = 1.13 (11995.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:19.749180: step 3530, loss = 1.06 (11918.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:20.178652: step 3540, loss = 1.01 (11982.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:20.610523: step 3550, loss = 1.08 (11869.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:21.045900: step 3560, loss = 1.00 (11568.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:21.493384: step 3570, loss = 1.06 (11828.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:21.922910: step 3580, loss = 0.96 (11856.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:22.353024: step 3590, loss = 1.13 (11847.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:22.785508: step 3600, loss = 1.07 (11892.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:23.375748: step 3610, loss = 0.87 (12059.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:23.806427: step 3620, loss = 0.87 (11939.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:24.236815: step 3630, loss = 1.08 (11952.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:24.667454: step 3640, loss = 0.85 (11973.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:25.096893: step 3650, loss = 1.00 (11974.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:25.534248: step 3660, loss = 0.86 (11731.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:25.964780: step 3670, loss = 1.00 (11917.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:26.395460: step 3680, loss = 1.12 (11924.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:26.827742: step 3690, loss = 1.13 (11788.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:27.256475: step 3700, loss = 0.91 (12025.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:27.848021: step 3710, loss = 1.04 (11911.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:28.277100: step 3720, loss = 0.94 (11923.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:28.710290: step 3730, loss = 1.25 (11891.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:29.142968: step 3740, loss = 0.87 (11814.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:29.576413: step 3750, loss = 1.06 (12002.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:30.008631: step 3760, loss = 1.04 (11852.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:30.437412: step 3770, loss = 0.81 (11858.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:30.867940: step 3780, loss = 1.04 (11822.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:31.297993: step 3790, loss = 1.18 (11862.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:31.731100: step 3800, loss = 0.80 (11991.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:32.331702: step 3810, loss = 0.92 (11351.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:32.768403: step 3820, loss = 0.96 (12007.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:33.197833: step 3830, loss = 0.83 (11896.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:33.629160: step 3840, loss = 1.36 (12029.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:34.060275: step 3850, loss = 0.92 (11999.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:34.491848: step 3860, loss = 1.22 (11931.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:34.922265: step 3870, loss = 0.95 (11951.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:35.350746: step 3880, loss = 0.99 (12090.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:35.780217: step 3890, loss = 1.13 (11899.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:36.210639: step 3900, loss = 0.97 (12068.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:36.797815: step 3910, loss = 0.88 (11981.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:37.226292: step 3920, loss = 1.06 (11842.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:37.652950: step 3930, loss = 0.88 (11895.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:38.082589: step 3940, loss = 0.99 (11933.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:38.513384: step 3950, loss = 0.97 (11876.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:38.947140: step 3960, loss = 0.92 (11084.0 examples/sec; 0.012 sec/batch)
2017-07-19 19:16:39.380980: step 3970, loss = 1.10 (11871.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:39.811119: step 3980, loss = 0.99 (12028.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:40.239811: step 3990, loss = 0.88 (12036.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:40.667775: step 4000, loss = 0.87 (12077.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:41.378515: step 4010, loss = 0.78 (12020.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:41.805881: step 4020, loss = 0.99 (11930.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:42.233939: step 4030, loss = 0.91 (11911.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:42.662295: step 4040, loss = 0.95 (11982.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:43.089613: step 4050, loss = 1.07 (11976.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:43.519821: step 4060, loss = 0.95 (12017.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:43.950682: step 4070, loss = 0.85 (12071.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:44.379358: step 4080, loss = 0.81 (11984.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:44.809659: step 4090, loss = 0.89 (12045.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:45.240471: step 4100, loss = 1.00 (11952.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:45.834068: step 4110, loss = 0.81 (11797.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:46.269165: step 4120, loss = 1.06 (12001.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:46.697293: step 4130, loss = 0.89 (11897.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:47.126797: step 4140, loss = 1.12 (11949.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:47.554105: step 4150, loss = 0.97 (12107.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:47.986411: step 4160, loss = 0.99 (11787.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:48.422973: step 4170, loss = 1.00 (11967.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:48.858881: step 4180, loss = 0.89 (11880.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:49.290949: step 4190, loss = 0.72 (11853.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:49.727348: step 4200, loss = 0.93 (11599.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:50.327746: step 4210, loss = 0.87 (11805.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:50.758779: step 4220, loss = 0.95 (11940.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:51.191419: step 4230, loss = 0.83 (11845.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:51.620310: step 4240, loss = 0.75 (11942.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:52.049537: step 4250, loss = 1.15 (11976.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:52.477302: step 4260, loss = 0.95 (12081.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:52.905306: step 4270, loss = 0.90 (11965.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:53.333371: step 4280, loss = 1.02 (11942.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:53.765645: step 4290, loss = 0.83 (11982.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:54.194196: step 4300, loss = 0.86 (12152.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:54.781207: step 4310, loss = 0.95 (11725.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:55.212697: step 4320, loss = 0.76 (12000.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:55.641374: step 4330, loss = 0.83 (11974.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:56.072130: step 4340, loss = 0.79 (11921.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:56.500290: step 4350, loss = 0.98 (11986.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:56.928748: step 4360, loss = 0.91 (11786.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:57.359914: step 4370, loss = 0.89 (12090.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:57.797553: step 4380, loss = 0.97 (10463.5 examples/sec; 0.012 sec/batch)
2017-07-19 19:16:58.228952: step 4390, loss = 1.09 (11917.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:58.660166: step 4400, loss = 0.91 (11830.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:59.260167: step 4410, loss = 0.95 (11928.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:16:59.689022: step 4420, loss = 0.88 (11982.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:00.120711: step 4430, loss = 0.88 (11975.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:00.550430: step 4440, loss = 0.91 (12050.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:00.978952: step 4450, loss = 0.88 (11807.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:01.407146: step 4460, loss = 0.92 (11801.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:01.835948: step 4470, loss = 0.93 (11927.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:02.263117: step 4480, loss = 0.97 (12036.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:02.694140: step 4490, loss = 0.86 (12058.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:03.121058: step 4500, loss = 0.90 (11937.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:03.712061: step 4510, loss = 0.84 (12077.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:04.145414: step 4520, loss = 1.05 (12009.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:04.574717: step 4530, loss = 0.84 (11958.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:05.010659: step 4540, loss = 0.90 (11915.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:05.440132: step 4550, loss = 0.86 (11887.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:05.870050: step 4560, loss = 1.06 (11891.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:06.298965: step 4570, loss = 1.03 (12024.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:06.727900: step 4580, loss = 1.11 (11799.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:07.154305: step 4590, loss = 0.83 (11928.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:07.589224: step 4600, loss = 0.87 (11961.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:08.178928: step 4610, loss = 0.77 (11836.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:08.608624: step 4620, loss = 0.89 (11953.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:09.036135: step 4630, loss = 0.89 (12180.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:09.465508: step 4640, loss = 0.83 (11897.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:09.895521: step 4650, loss = 0.77 (11912.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:10.323993: step 4660, loss = 0.93 (11930.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:10.750796: step 4670, loss = 0.92 (12018.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:11.182947: step 4680, loss = 0.88 (12013.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:11.612554: step 4690, loss = 0.68 (11788.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:12.039729: step 4700, loss = 0.97 (12203.6 examples/sec; 0.010 sec/batch)
2017-07-19 19:17:12.636692: step 4710, loss = 0.90 (11607.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:13.072315: step 4720, loss = 0.85 (11786.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:13.503983: step 4730, loss = 0.74 (12046.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:13.934967: step 4740, loss = 0.60 (12031.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:14.370108: step 4750, loss = 0.73 (11929.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:14.800554: step 4760, loss = 0.77 (12001.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:15.231348: step 4770, loss = 0.66 (11822.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:15.660198: step 4780, loss = 0.84 (11972.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:16.091261: step 4790, loss = 0.71 (11907.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:16.522801: step 4800, loss = 0.85 (12055.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:17.115866: step 4810, loss = 1.05 (11970.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:17.548453: step 4820, loss = 0.66 (11906.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:17.990652: step 4830, loss = 0.91 (11946.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:18.421621: step 4840, loss = 0.98 (11983.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:18.853642: step 4850, loss = 1.02 (11971.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:19.281431: step 4860, loss = 0.83 (11813.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:19.709440: step 4870, loss = 0.94 (11958.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:20.138508: step 4880, loss = 0.73 (11825.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:20.568005: step 4890, loss = 0.92 (11890.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:20.997576: step 4900, loss = 0.69 (11818.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:21.582780: step 4910, loss = 0.84 (12028.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:22.011455: step 4920, loss = 0.84 (11995.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:22.439988: step 4930, loss = 0.86 (12042.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:22.869871: step 4940, loss = 0.92 (12051.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:23.295390: step 4950, loss = 0.79 (12153.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:23.723452: step 4960, loss = 0.89 (11799.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:24.157964: step 4970, loss = 0.98 (12026.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:24.586914: step 4980, loss = 0.77 (11828.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:25.017758: step 4990, loss = 0.83 (11700.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:25.450205: step 5000, loss = 0.64 (11910.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:26.225743: step 5010, loss = 0.68 (11916.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:26.656231: step 5020, loss = 0.96 (11966.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:27.083092: step 5030, loss = 0.87 (12023.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:27.525713: step 5040, loss = 0.88 (11927.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:27.955528: step 5050, loss = 0.82 (11543.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:28.386169: step 5060, loss = 0.68 (11858.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:28.816142: step 5070, loss = 0.78 (11779.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:29.246871: step 5080, loss = 0.85 (11958.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:29.674703: step 5090, loss = 0.84 (11944.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:30.103257: step 5100, loss = 1.02 (12050.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:30.695132: step 5110, loss = 0.78 (11955.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:31.125238: step 5120, loss = 0.74 (11993.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:31.560308: step 5130, loss = 0.79 (12064.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:31.987858: step 5140, loss = 0.81 (11956.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:32.415815: step 5150, loss = 0.76 (12030.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:32.843805: step 5160, loss = 0.78 (12007.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:33.270437: step 5170, loss = 0.75 (12050.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:33.701737: step 5180, loss = 0.86 (11761.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:34.129965: step 5190, loss = 0.78 (11941.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:34.559331: step 5200, loss = 0.80 (12075.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:35.153032: step 5210, loss = 0.89 (12046.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:35.581737: step 5220, loss = 0.71 (11687.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:36.011891: step 5230, loss = 1.00 (11931.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:36.446961: step 5240, loss = 0.73 (11995.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:36.880430: step 5250, loss = 0.80 (11998.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:37.312602: step 5260, loss = 0.86 (11954.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:37.747546: step 5270, loss = 0.91 (11631.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:38.177226: step 5280, loss = 0.76 (12073.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:38.607424: step 5290, loss = 0.79 (11978.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:39.038885: step 5300, loss = 0.76 (11907.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:39.644197: step 5310, loss = 0.73 (11248.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:40.073632: step 5320, loss = 0.83 (11993.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:40.504974: step 5330, loss = 0.88 (11605.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:40.943084: step 5340, loss = 0.97 (11848.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:41.373168: step 5350, loss = 0.81 (11806.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:41.804573: step 5360, loss = 0.82 (12004.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:42.234594: step 5370, loss = 0.85 (11878.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:42.662856: step 5380, loss = 0.76 (12003.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:43.090741: step 5390, loss = 0.85 (11903.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:43.518044: step 5400, loss = 0.91 (12183.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:44.105907: step 5410, loss = 1.02 (11959.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:44.533247: step 5420, loss = 0.64 (12029.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:44.963811: step 5430, loss = 0.70 (11931.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:45.392598: step 5440, loss = 0.93 (11903.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:45.819653: step 5450, loss = 0.69 (11898.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:46.247867: step 5460, loss = 0.83 (12056.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:46.677227: step 5470, loss = 0.80 (11963.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:47.107626: step 5480, loss = 0.76 (12073.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:47.533073: step 5490, loss = 0.86 (12189.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:47.963127: step 5500, loss = 0.93 (11640.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:48.555919: step 5510, loss = 0.84 (12152.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:48.986572: step 5520, loss = 0.69 (11989.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:49.415580: step 5530, loss = 0.69 (12017.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:49.846081: step 5540, loss = 0.88 (11895.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:50.274517: step 5550, loss = 0.89 (11965.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:50.704447: step 5560, loss = 0.74 (12092.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:51.132423: step 5570, loss = 0.79 (11925.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:51.565061: step 5580, loss = 0.83 (11893.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:51.995372: step 5590, loss = 0.78 (11953.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:52.420816: step 5600, loss = 0.77 (11918.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:53.007804: step 5610, loss = 0.78 (11949.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:53.437234: step 5620, loss = 0.75 (11972.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:53.866198: step 5630, loss = 0.81 (11946.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:54.295859: step 5640, loss = 0.75 (11827.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:54.723730: step 5650, loss = 0.72 (11913.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:55.150671: step 5660, loss = 0.71 (11907.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:55.579613: step 5670, loss = 0.72 (11867.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:56.009386: step 5680, loss = 0.88 (11956.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:56.438190: step 5690, loss = 0.74 (11995.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:56.867429: step 5700, loss = 0.99 (11929.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:57.455857: step 5710, loss = 0.72 (11961.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:57.887292: step 5720, loss = 0.98 (11871.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:58.317136: step 5730, loss = 0.67 (12010.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:58.749350: step 5740, loss = 0.74 (11837.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:59.180580: step 5750, loss = 0.65 (11860.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:17:59.628661: step 5760, loss = 0.77 (10425.9 examples/sec; 0.012 sec/batch)
2017-07-19 19:18:00.058752: step 5770, loss = 0.72 (11894.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:00.491730: step 5780, loss = 0.81 (11773.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:00.925353: step 5790, loss = 0.86 (12012.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:01.356277: step 5800, loss = 0.74 (11873.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:01.951873: step 5810, loss = 0.73 (11865.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:02.386159: step 5820, loss = 0.73 (11987.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:02.815110: step 5830, loss = 0.71 (12003.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:03.244964: step 5840, loss = 0.76 (11985.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:03.674708: step 5850, loss = 0.71 (11782.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:04.105688: step 5860, loss = 0.74 (11497.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:04.536617: step 5870, loss = 0.72 (11956.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:04.964398: step 5880, loss = 0.87 (12112.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:05.401264: step 5890, loss = 0.75 (10360.2 examples/sec; 0.012 sec/batch)
2017-07-19 19:18:05.832038: step 5900, loss = 0.98 (12023.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:06.424286: step 5910, loss = 0.67 (11854.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:06.855400: step 5920, loss = 0.84 (11920.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:07.284670: step 5930, loss = 0.80 (12021.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:07.715291: step 5940, loss = 0.83 (12005.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:08.145243: step 5950, loss = 0.73 (11788.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:08.576512: step 5960, loss = 0.83 (11976.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:09.007865: step 5970, loss = 0.71 (11788.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:09.442194: step 5980, loss = 0.92 (11853.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:09.874207: step 5990, loss = 0.74 (11959.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:10.303602: step 6000, loss = 0.93 (11798.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:11.039389: step 6010, loss = 0.69 (11767.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:11.468965: step 6020, loss = 0.92 (12084.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:11.899510: step 6030, loss = 0.68 (11982.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:12.326853: step 6040, loss = 0.75 (12013.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:12.759413: step 6050, loss = 0.77 (10923.1 examples/sec; 0.012 sec/batch)
2017-07-19 19:18:13.194720: step 6060, loss = 0.87 (11896.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:13.621588: step 6070, loss = 0.72 (12026.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:14.051186: step 6080, loss = 0.92 (11930.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:14.479200: step 6090, loss = 0.71 (11970.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:14.920001: step 6100, loss = 0.87 (11926.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:15.508060: step 6110, loss = 0.57 (11856.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:15.938277: step 6120, loss = 0.70 (12018.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:16.370853: step 6130, loss = 0.72 (10632.9 examples/sec; 0.012 sec/batch)
2017-07-19 19:18:16.800271: step 6140, loss = 0.78 (11929.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:17.229065: step 6150, loss = 0.87 (11850.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:17.658136: step 6160, loss = 0.66 (11863.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:18.087107: step 6170, loss = 0.76 (12079.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:18.513973: step 6180, loss = 0.73 (11887.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:18.942978: step 6190, loss = 0.84 (11912.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:19.377413: step 6200, loss = 0.70 (11763.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:19.966254: step 6210, loss = 0.76 (12096.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:20.401721: step 6220, loss = 0.77 (11933.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:20.833169: step 6230, loss = 0.85 (11595.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:21.260061: step 6240, loss = 0.90 (12085.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:21.688473: step 6250, loss = 0.65 (11981.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:22.119628: step 6260, loss = 0.70 (12168.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:22.546946: step 6270, loss = 0.69 (11823.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:22.974507: step 6280, loss = 0.73 (12070.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:23.401604: step 6290, loss = 1.01 (11944.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:23.831807: step 6300, loss = 0.68 (11834.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:24.431815: step 6310, loss = 0.81 (11951.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:24.861295: step 6320, loss = 0.58 (12119.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:25.291294: step 6330, loss = 0.62 (11993.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:25.721202: step 6340, loss = 0.69 (11917.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:26.159274: step 6350, loss = 0.73 (11926.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:26.596044: step 6360, loss = 0.68 (11737.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:27.025644: step 6370, loss = 0.85 (11823.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:27.451961: step 6380, loss = 0.74 (11941.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:27.880284: step 6390, loss = 0.59 (11954.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:28.314353: step 6400, loss = 0.95 (11915.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:28.904404: step 6410, loss = 0.74 (12025.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:29.339656: step 6420, loss = 0.65 (11775.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:29.768040: step 6430, loss = 0.70 (12038.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:30.195565: step 6440, loss = 0.76 (11982.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:30.625235: step 6450, loss = 0.93 (11890.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:31.054877: step 6460, loss = 0.64 (11974.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:31.490090: step 6470, loss = 0.88 (11923.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:31.918387: step 6480, loss = 0.86 (11967.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:32.347479: step 6490, loss = 0.65 (11962.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:32.781401: step 6500, loss = 0.99 (10453.5 examples/sec; 0.012 sec/batch)
2017-07-19 19:18:33.370325: step 6510, loss = 0.70 (12091.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:33.798213: step 6520, loss = 0.48 (11898.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:34.225952: step 6530, loss = 0.64 (11996.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:34.663969: step 6540, loss = 0.75 (11886.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:35.090017: step 6550, loss = 0.82 (11991.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:35.517844: step 6560, loss = 0.76 (11955.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:35.946921: step 6570, loss = 0.72 (11807.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:36.378130: step 6580, loss = 0.65 (11993.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:36.806418: step 6590, loss = 0.93 (12020.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:37.233551: step 6600, loss = 0.86 (12023.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:37.823570: step 6610, loss = 0.73 (12064.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:38.252123: step 6620, loss = 0.87 (11896.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:38.681834: step 6630, loss = 0.79 (11823.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:39.109208: step 6640, loss = 0.86 (12112.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:39.537693: step 6650, loss = 0.87 (12008.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:39.965754: step 6660, loss = 0.69 (12022.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:40.394517: step 6670, loss = 0.76 (11940.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:40.824464: step 6680, loss = 0.75 (11680.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:41.252111: step 6690, loss = 1.17 (12022.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:41.681897: step 6700, loss = 0.70 (11772.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:42.270087: step 6710, loss = 0.64 (11923.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:42.696691: step 6720, loss = 0.56 (12121.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:43.123559: step 6730, loss = 0.82 (11884.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:43.550364: step 6740, loss = 0.76 (11924.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:43.978405: step 6750, loss = 0.83 (12041.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:44.405185: step 6760, loss = 0.62 (11994.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:44.832532: step 6770, loss = 0.77 (12097.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:45.257999: step 6780, loss = 0.86 (12084.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:45.685575: step 6790, loss = 0.72 (12030.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:46.113881: step 6800, loss = 0.69 (11877.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:46.701025: step 6810, loss = 0.76 (11856.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:47.130105: step 6820, loss = 0.78 (11953.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:47.556706: step 6830, loss = 0.77 (12055.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:47.987035: step 6840, loss = 0.74 (12041.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:48.415366: step 6850, loss = 0.76 (12079.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:48.843762: step 6860, loss = 0.71 (11854.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:49.271771: step 6870, loss = 0.70 (11842.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:49.701894: step 6880, loss = 0.77 (11807.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:50.129538: step 6890, loss = 0.65 (11985.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:50.557422: step 6900, loss = 0.77 (12036.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:51.167576: step 6910, loss = 1.19 (11968.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:51.600441: step 6920, loss = 0.88 (11869.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:52.036253: step 6930, loss = 0.68 (12012.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:52.464596: step 6940, loss = 0.87 (12013.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:52.895562: step 6950, loss = 0.86 (11890.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:53.324903: step 6960, loss = 0.70 (11916.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:53.756054: step 6970, loss = 0.73 (11981.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:54.187419: step 6980, loss = 0.68 (11739.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:54.615420: step 6990, loss = 0.81 (11907.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:55.045766: step 7000, loss = 0.70 (11890.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:55.755900: step 7010, loss = 0.87 (11940.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:56.184450: step 7020, loss = 0.73 (11788.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:56.614086: step 7030, loss = 0.89 (11986.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:57.043345: step 7040, loss = 0.64 (11797.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:57.471877: step 7050, loss = 0.67 (11888.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:57.902095: step 7060, loss = 0.85 (11735.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:58.330505: step 7070, loss = 0.78 (11899.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:58.760615: step 7080, loss = 0.98 (11956.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:59.191052: step 7090, loss = 0.75 (11958.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:18:59.620203: step 7100, loss = 0.71 (11917.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:00.215037: step 7110, loss = 0.83 (11938.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:00.649603: step 7120, loss = 0.71 (11958.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:01.078976: step 7130, loss = 0.82 (11800.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:01.502811: step 7140, loss = 0.69 (12060.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:01.935871: step 7150, loss = 0.89 (12117.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:02.363888: step 7160, loss = 0.84 (12119.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:02.791929: step 7170, loss = 0.60 (11775.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:03.218130: step 7180, loss = 0.77 (12145.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:03.645585: step 7190, loss = 0.77 (11902.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:04.073127: step 7200, loss = 0.84 (11872.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:04.661486: step 7210, loss = 0.78 (11963.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:05.089954: step 7220, loss = 0.66 (12094.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:05.517359: step 7230, loss = 0.61 (12083.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:05.946787: step 7240, loss = 0.65 (11919.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:06.372678: step 7250, loss = 0.79 (11981.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:06.802297: step 7260, loss = 0.57 (12005.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:07.230627: step 7270, loss = 0.79 (12155.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:07.658288: step 7280, loss = 0.69 (12037.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:08.088926: step 7290, loss = 0.73 (11817.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:08.520757: step 7300, loss = 0.75 (11834.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:09.122468: step 7310, loss = 0.71 (11872.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:09.551787: step 7320, loss = 0.67 (12018.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:09.990234: step 7330, loss = 0.71 (11825.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:10.422941: step 7340, loss = 0.73 (11887.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:10.852883: step 7350, loss = 0.76 (11956.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:11.286696: step 7360, loss = 0.78 (11805.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:11.718500: step 7370, loss = 0.73 (11665.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:12.149355: step 7380, loss = 0.53 (11729.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:12.580168: step 7390, loss = 0.71 (11712.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:13.008121: step 7400, loss = 0.71 (12142.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:13.602048: step 7410, loss = 0.67 (11915.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:14.031613: step 7420, loss = 0.62 (11956.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:14.467025: step 7430, loss = 0.76 (11829.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:14.896718: step 7440, loss = 0.61 (12047.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:15.323852: step 7450, loss = 0.66 (12064.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:15.752086: step 7460, loss = 0.71 (11988.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:16.179333: step 7470, loss = 0.75 (11917.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:16.611117: step 7480, loss = 0.73 (11939.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:17.040067: step 7490, loss = 0.58 (11938.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:17.467970: step 7500, loss = 0.72 (12063.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:18.052076: step 7510, loss = 0.82 (11893.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:18.481613: step 7520, loss = 0.72 (11935.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:18.911036: step 7530, loss = 0.74 (11818.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:19.338421: step 7540, loss = 0.73 (12119.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:19.768470: step 7550, loss = 0.77 (11993.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:20.207039: step 7560, loss = 0.77 (10313.8 examples/sec; 0.012 sec/batch)
2017-07-19 19:19:20.638360: step 7570, loss = 0.63 (12027.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:21.069632: step 7580, loss = 0.75 (11854.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:21.497906: step 7590, loss = 0.65 (12073.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:21.926408: step 7600, loss = 0.62 (12084.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:22.516577: step 7610, loss = 0.64 (11955.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:22.945598: step 7620, loss = 0.64 (12022.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:23.374644: step 7630, loss = 0.83 (11975.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:23.800128: step 7640, loss = 0.61 (12016.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:24.228288: step 7650, loss = 0.75 (11918.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:24.657712: step 7660, loss = 0.62 (11906.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:25.085972: step 7670, loss = 0.62 (11890.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:25.514339: step 7680, loss = 0.96 (11917.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:25.946018: step 7690, loss = 0.74 (11861.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:26.380806: step 7700, loss = 0.86 (11744.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:26.969112: step 7710, loss = 0.71 (11928.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:27.396103: step 7720, loss = 0.65 (11976.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:27.823662: step 7730, loss = 0.66 (11916.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:28.250937: step 7740, loss = 0.74 (11963.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:28.679631: step 7750, loss = 0.75 (11782.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:29.109330: step 7760, loss = 0.83 (11981.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:29.536620: step 7770, loss = 0.74 (11980.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:29.965330: step 7780, loss = 0.93 (11908.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:30.392910: step 7790, loss = 0.81 (11962.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:30.819405: step 7800, loss = 0.76 (11967.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:31.412998: step 7810, loss = 0.69 (11998.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:31.840850: step 7820, loss = 0.81 (12019.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:32.268143: step 7830, loss = 0.92 (11997.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:32.701510: step 7840, loss = 0.68 (12150.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:33.128845: step 7850, loss = 0.76 (11996.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:33.556961: step 7860, loss = 0.66 (12021.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:33.985573: step 7870, loss = 0.56 (12069.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:34.414218: step 7880, loss = 0.70 (12081.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:34.842492: step 7890, loss = 0.82 (11967.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:35.273764: step 7900, loss = 0.73 (12040.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:35.875912: step 7910, loss = 0.78 (11903.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:36.307348: step 7920, loss = 0.70 (11990.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:36.735466: step 7930, loss = 0.76 (11826.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:37.165010: step 7940, loss = 0.71 (11937.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:37.595463: step 7950, loss = 0.72 (11776.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:38.024839: step 7960, loss = 0.75 (11864.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:38.454046: step 7970, loss = 0.56 (12034.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:38.880599: step 7980, loss = 0.82 (12064.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:39.308430: step 7990, loss = 0.63 (12063.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:39.738269: step 8000, loss = 0.84 (12046.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:40.469302: step 8010, loss = 0.56 (11940.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:40.901173: step 8020, loss = 0.69 (11919.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:41.334622: step 8030, loss = 0.78 (12002.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:41.764501: step 8040, loss = 0.67 (11876.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:42.194966: step 8050, loss = 0.79 (11712.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:42.623516: step 8060, loss = 0.65 (11926.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:43.053453: step 8070, loss = 0.86 (11825.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:43.481658: step 8080, loss = 0.72 (12014.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:43.911537: step 8090, loss = 0.79 (11866.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:44.343573: step 8100, loss = 0.79 (11798.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:44.932456: step 8110, loss = 0.89 (11894.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:45.360997: step 8120, loss = 0.59 (11957.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:45.787818: step 8130, loss = 0.65 (12040.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:46.216744: step 8140, loss = 0.67 (12088.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:46.652236: step 8150, loss = 0.56 (11901.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:47.081852: step 8160, loss = 0.82 (11976.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:47.508754: step 8170, loss = 0.65 (12000.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:47.941608: step 8180, loss = 0.69 (11699.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:48.371159: step 8190, loss = 0.75 (11866.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:48.801544: step 8200, loss = 0.82 (11982.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:49.396047: step 8210, loss = 0.62 (12030.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:49.826869: step 8220, loss = 0.65 (12204.9 examples/sec; 0.010 sec/batch)
2017-07-19 19:19:50.265650: step 8230, loss = 0.76 (11926.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:50.694794: step 8240, loss = 0.72 (12060.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:51.128187: step 8250, loss = 0.71 (11869.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:51.553003: step 8260, loss = 0.58 (12003.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:51.983480: step 8270, loss = 0.62 (11939.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:52.412542: step 8280, loss = 0.67 (11835.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:52.841535: step 8290, loss = 0.73 (11934.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:53.269926: step 8300, loss = 0.81 (12064.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:53.859655: step 8310, loss = 0.72 (11864.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:54.288896: step 8320, loss = 0.64 (11864.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:54.719673: step 8330, loss = 0.63 (11819.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:55.146174: step 8340, loss = 0.70 (12100.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:55.576392: step 8350, loss = 0.69 (11876.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:56.004672: step 8360, loss = 0.75 (12030.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:56.431338: step 8370, loss = 0.61 (11806.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:56.860887: step 8380, loss = 0.73 (11976.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:57.290333: step 8390, loss = 0.71 (12013.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:57.727427: step 8400, loss = 0.72 (11458.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:58.327722: step 8410, loss = 0.64 (11861.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:58.759999: step 8420, loss = 0.74 (12060.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:59.190289: step 8430, loss = 0.71 (11835.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:19:59.628170: step 8440, loss = 0.80 (11711.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:00.056462: step 8450, loss = 0.68 (11880.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:00.495708: step 8460, loss = 0.68 (11600.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:00.934130: step 8470, loss = 0.70 (11909.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:01.361706: step 8480, loss = 0.80 (12003.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:01.787972: step 8490, loss = 0.72 (11913.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:02.216966: step 8500, loss = 0.65 (11899.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:02.810589: step 8510, loss = 0.77 (11914.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:03.239539: step 8520, loss = 0.62 (11912.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:03.666543: step 8530, loss = 0.70 (12200.9 examples/sec; 0.010 sec/batch)
2017-07-19 19:20:04.094071: step 8540, loss = 0.65 (11904.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:04.521994: step 8550, loss = 0.68 (11964.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:04.950956: step 8560, loss = 0.74 (12069.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:05.378023: step 8570, loss = 0.68 (11965.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:05.806505: step 8580, loss = 0.61 (11946.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:06.237198: step 8590, loss = 0.74 (11887.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:06.666050: step 8600, loss = 0.77 (11752.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:07.291499: step 8610, loss = 0.56 (12062.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:07.719752: step 8620, loss = 0.75 (11834.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:08.148969: step 8630, loss = 0.77 (11894.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:08.576348: step 8640, loss = 0.60 (12021.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:09.004438: step 8650, loss = 0.76 (11934.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:09.434573: step 8660, loss = 0.75 (11950.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:09.865928: step 8670, loss = 0.59 (11975.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:10.299407: step 8680, loss = 0.66 (11938.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:10.737649: step 8690, loss = 0.73 (10179.9 examples/sec; 0.013 sec/batch)
2017-07-19 19:20:11.168905: step 8700, loss = 0.91 (12073.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:11.756791: step 8710, loss = 0.83 (12091.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:12.190580: step 8720, loss = 0.67 (12051.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:12.617416: step 8730, loss = 0.60 (12019.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:13.047839: step 8740, loss = 0.74 (12006.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:13.476161: step 8750, loss = 0.71 (12019.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:13.905383: step 8760, loss = 0.77 (11986.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:14.333266: step 8770, loss = 0.82 (11918.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:14.761198: step 8780, loss = 0.92 (11972.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:15.189562: step 8790, loss = 0.64 (12083.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:15.617518: step 8800, loss = 0.69 (12071.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:16.205992: step 8810, loss = 0.65 (11982.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:16.635143: step 8820, loss = 0.60 (12086.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:17.063024: step 8830, loss = 0.96 (11821.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:17.491849: step 8840, loss = 0.78 (11940.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:17.919658: step 8850, loss = 0.73 (12056.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:18.348025: step 8860, loss = 0.66 (11897.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:18.792365: step 8870, loss = 0.65 (8797.5 examples/sec; 0.015 sec/batch)
2017-07-19 19:20:19.225626: step 8880, loss = 0.65 (11969.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:19.652481: step 8890, loss = 0.83 (12263.8 examples/sec; 0.010 sec/batch)
2017-07-19 19:20:20.082792: step 8900, loss = 0.73 (12054.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:20.673879: step 8910, loss = 0.71 (11922.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:21.102863: step 8920, loss = 0.59 (11982.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:21.531976: step 8930, loss = 0.79 (11700.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:21.959933: step 8940, loss = 0.65 (12062.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:22.389268: step 8950, loss = 0.71 (11954.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:22.828233: step 8960, loss = 0.66 (11770.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:23.261381: step 8970, loss = 0.67 (11813.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:23.699228: step 8980, loss = 0.54 (11808.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:24.125896: step 8990, loss = 0.79 (12039.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:24.556066: step 9000, loss = 0.71 (11651.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:25.304602: step 9010, loss = 0.56 (12099.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:25.735358: step 9020, loss = 0.73 (11861.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:26.168067: step 9030, loss = 0.65 (11940.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:26.603978: step 9040, loss = 0.61 (12041.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:27.033518: step 9050, loss = 0.67 (11793.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:27.462712: step 9060, loss = 0.72 (11957.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:27.892240: step 9070, loss = 0.62 (12142.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:28.318000: step 9080, loss = 0.71 (11982.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:28.745651: step 9090, loss = 0.77 (12131.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:29.172855: step 9100, loss = 0.78 (12071.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:29.755804: step 9110, loss = 0.57 (12155.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:30.183386: step 9120, loss = 0.77 (12068.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:30.618556: step 9130, loss = 0.67 (10521.5 examples/sec; 0.012 sec/batch)
2017-07-19 19:20:31.047810: step 9140, loss = 0.71 (12023.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:31.477357: step 9150, loss = 0.87 (11944.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:31.906684: step 9160, loss = 0.61 (11864.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:32.336979: step 9170, loss = 0.69 (11907.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:32.767850: step 9180, loss = 0.58 (11910.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:33.196979: step 9190, loss = 0.75 (11856.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:33.626201: step 9200, loss = 0.89 (11964.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:34.222435: step 9210, loss = 0.71 (11879.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:34.651900: step 9220, loss = 0.70 (11922.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:35.078389: step 9230, loss = 0.57 (12058.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:35.505589: step 9240, loss = 0.70 (11916.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:35.941311: step 9250, loss = 0.68 (9909.6 examples/sec; 0.013 sec/batch)
2017-07-19 19:20:36.371558: step 9260, loss = 0.64 (11801.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:36.808052: step 9270, loss = 0.71 (11974.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:37.236855: step 9280, loss = 0.67 (12009.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:37.663411: step 9290, loss = 0.83 (11928.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:38.091387: step 9300, loss = 0.68 (11800.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:38.683871: step 9310, loss = 0.62 (11810.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:39.112910: step 9320, loss = 0.69 (12002.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:39.539647: step 9330, loss = 0.70 (11994.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:39.967681: step 9340, loss = 0.59 (12012.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:40.398163: step 9350, loss = 0.84 (11826.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:40.830109: step 9360, loss = 0.58 (12005.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:41.260831: step 9370, loss = 0.65 (11889.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:41.691078: step 9380, loss = 0.78 (11526.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:42.121120: step 9390, loss = 0.91 (12072.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:42.562955: step 9400, loss = 0.69 (11685.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:43.156581: step 9410, loss = 0.66 (12015.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:43.588406: step 9420, loss = 0.59 (12065.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:44.017389: step 9430, loss = 0.80 (11868.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:44.446282: step 9440, loss = 0.61 (11896.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:44.877478: step 9450, loss = 0.66 (12015.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:45.305483: step 9460, loss = 0.81 (11970.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:45.731810: step 9470, loss = 0.65 (12127.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:46.158660: step 9480, loss = 0.82 (11844.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:46.585739: step 9490, loss = 0.69 (12117.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:47.014826: step 9500, loss = 0.61 (11974.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:47.610954: step 9510, loss = 0.68 (12051.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:48.039443: step 9520, loss = 0.82 (11973.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:48.467434: step 9530, loss = 0.69 (12084.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:48.893305: step 9540, loss = 0.72 (12030.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:49.329398: step 9550, loss = 0.67 (11897.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:49.760807: step 9560, loss = 0.81 (11931.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:50.188190: step 9570, loss = 0.59 (11790.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:50.615473: step 9580, loss = 0.72 (12017.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:51.043458: step 9590, loss = 0.90 (11848.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:51.474059: step 9600, loss = 0.92 (12025.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:52.070399: step 9610, loss = 0.51 (11906.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:52.499985: step 9620, loss = 0.71 (12025.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:52.929700: step 9630, loss = 0.62 (11516.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:53.357844: step 9640, loss = 0.58 (12022.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:53.784198: step 9650, loss = 0.73 (11993.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:54.214012: step 9660, loss = 0.62 (11746.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:54.647781: step 9670, loss = 0.62 (11761.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:55.076790: step 9680, loss = 0.65 (11819.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:55.507100: step 9690, loss = 0.79 (11926.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:55.937937: step 9700, loss = 0.61 (11997.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:56.524328: step 9710, loss = 0.70 (12041.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:56.953357: step 9720, loss = 0.69 (11992.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:57.380472: step 9730, loss = 0.79 (11904.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:57.814011: step 9740, loss = 0.82 (11767.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:58.248849: step 9750, loss = 0.72 (11793.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:58.678150: step 9760, loss = 0.54 (12053.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:59.108801: step 9770, loss = 0.87 (11987.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:59.536527: step 9780, loss = 0.69 (11767.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:20:59.965152: step 9790, loss = 0.52 (11949.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:00.393822: step 9800, loss = 0.68 (12030.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:00.984572: step 9810, loss = 0.69 (12067.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:01.412383: step 9820, loss = 0.67 (11910.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:01.840116: step 9830, loss = 0.66 (11933.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:02.268421: step 9840, loss = 0.75 (11863.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:02.703301: step 9850, loss = 0.68 (11770.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:03.130538: step 9860, loss = 0.66 (11905.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:03.558421: step 9870, loss = 0.52 (12068.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:03.989226: step 9880, loss = 0.84 (11817.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:04.417949: step 9890, loss = 0.78 (12008.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:04.848582: step 9900, loss = 0.88 (11883.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:05.436471: step 9910, loss = 0.65 (11986.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:05.865117: step 9920, loss = 0.69 (11950.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:06.294887: step 9930, loss = 0.65 (12112.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:06.727753: step 9940, loss = 0.70 (11869.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:07.157905: step 9950, loss = 0.60 (11852.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:07.588631: step 9960, loss = 0.87 (11954.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:08.019438: step 9970, loss = 0.62 (11766.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:08.455477: step 9980, loss = 0.70 (11885.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:08.886968: step 9990, loss = 0.63 (11972.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:09.316810: step 10000, loss = 0.68 (12005.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:10.043056: step 10010, loss = 0.70 (11964.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:10.474110: step 10020, loss = 0.62 (12033.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:10.903186: step 10030, loss = 0.67 (11870.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:11.332063: step 10040, loss = 0.75 (11982.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:11.761165: step 10050, loss = 1.06 (11918.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:12.191982: step 10060, loss = 0.60 (11893.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:12.629288: step 10070, loss = 0.64 (11822.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:13.058277: step 10080, loss = 0.54 (11825.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:13.487377: step 10090, loss = 0.81 (12011.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:13.918484: step 10100, loss = 0.55 (11831.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:14.516823: step 10110, loss = 0.67 (11871.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:14.946163: step 10120, loss = 0.70 (11959.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:15.374290: step 10130, loss = 0.58 (12109.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:15.804491: step 10140, loss = 0.69 (11915.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:16.236213: step 10150, loss = 0.73 (11940.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:16.665723: step 10160, loss = 0.81 (11837.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:17.103130: step 10170, loss = 0.72 (11923.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:17.532903: step 10180, loss = 0.54 (11831.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:17.962312: step 10190, loss = 0.63 (11994.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:18.390832: step 10200, loss = 0.63 (11970.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:18.979161: step 10210, loss = 0.76 (11838.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:19.407540: step 10220, loss = 0.71 (12013.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:19.835245: step 10230, loss = 0.67 (11862.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:20.264619: step 10240, loss = 0.65 (11891.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:20.693307: step 10250, loss = 0.83 (11994.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:21.122312: step 10260, loss = 0.63 (12034.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:21.551430: step 10270, loss = 0.78 (11925.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:21.980990: step 10280, loss = 0.78 (11811.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:22.407706: step 10290, loss = 0.76 (12002.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:22.832701: step 10300, loss = 0.50 (12093.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:23.425304: step 10310, loss = 0.60 (12005.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:23.852266: step 10320, loss = 0.65 (12083.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:24.279755: step 10330, loss = 0.70 (12002.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:24.706814: step 10340, loss = 0.75 (12160.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:25.136464: step 10350, loss = 0.60 (11863.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:25.567056: step 10360, loss = 0.75 (11805.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:25.995447: step 10370, loss = 0.83 (11981.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:26.424383: step 10380, loss = 0.74 (11887.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:26.854149: step 10390, loss = 0.65 (12003.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:27.283904: step 10400, loss = 0.63 (11966.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:27.885074: step 10410, loss = 0.48 (12029.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:28.311789: step 10420, loss = 0.76 (12047.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:28.747304: step 10430, loss = 0.73 (11970.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:29.175494: step 10440, loss = 0.60 (11800.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:29.603127: step 10450, loss = 0.71 (12014.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:30.032368: step 10460, loss = 0.50 (11942.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:30.460879: step 10470, loss = 0.76 (12036.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:30.893687: step 10480, loss = 0.80 (12042.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:31.321332: step 10490, loss = 0.79 (12012.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:31.752197: step 10500, loss = 0.62 (11376.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:32.344990: step 10510, loss = 0.62 (11778.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:32.774629: step 10520, loss = 0.56 (11932.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:33.212687: step 10530, loss = 0.71 (11936.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:33.644142: step 10540, loss = 0.66 (11976.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:34.077235: step 10550, loss = 0.80 (11794.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:34.506509: step 10560, loss = 0.68 (11953.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:34.936496: step 10570, loss = 0.70 (11869.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:35.366516: step 10580, loss = 0.78 (11827.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:35.798786: step 10590, loss = 0.71 (11743.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:36.235403: step 10600, loss = 0.67 (11968.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:36.843119: step 10610, loss = 0.79 (11824.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:37.273016: step 10620, loss = 0.65 (11694.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:37.704602: step 10630, loss = 0.67 (11160.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:38.134922: step 10640, loss = 0.68 (11727.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:38.565150: step 10650, loss = 0.77 (11792.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:38.998762: step 10660, loss = 0.67 (11859.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:39.427645: step 10670, loss = 0.61 (11958.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:39.870540: step 10680, loss = 0.66 (11956.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:40.295897: step 10690, loss = 0.67 (12031.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:40.724555: step 10700, loss = 0.72 (12073.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:41.314165: step 10710, loss = 0.61 (12091.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:41.744167: step 10720, loss = 0.58 (11793.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:42.173302: step 10730, loss = 0.57 (12054.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:42.601033: step 10740, loss = 0.75 (11987.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:43.029644: step 10750, loss = 0.54 (12011.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:43.462921: step 10760, loss = 0.53 (11956.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:43.895665: step 10770, loss = 0.72 (12014.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:44.325729: step 10780, loss = 0.78 (11731.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:44.753398: step 10790, loss = 0.78 (11890.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:45.182986: step 10800, loss = 0.71 (12048.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:45.770269: step 10810, loss = 0.68 (11937.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:46.197605: step 10820, loss = 0.73 (11995.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:46.628096: step 10830, loss = 0.71 (11869.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:47.055775: step 10840, loss = 0.53 (12202.9 examples/sec; 0.010 sec/batch)
2017-07-19 19:21:47.484993: step 10850, loss = 0.54 (12010.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:47.913273: step 10860, loss = 0.52 (12046.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:48.341682: step 10870, loss = 0.73 (11590.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:48.778871: step 10880, loss = 0.57 (11851.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:49.206513: step 10890, loss = 0.63 (11969.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:49.635373: step 10900, loss = 0.62 (11850.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:50.227763: step 10910, loss = 0.66 (12017.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:50.658048: step 10920, loss = 0.67 (11933.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:51.085024: step 10930, loss = 0.70 (11932.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:51.517856: step 10940, loss = 0.63 (11786.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:51.957295: step 10950, loss = 0.66 (10997.8 examples/sec; 0.012 sec/batch)
2017-07-19 19:21:52.391793: step 10960, loss = 0.69 (11539.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:52.821055: step 10970, loss = 0.69 (12075.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:53.248213: step 10980, loss = 0.62 (12046.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:53.677114: step 10990, loss = 0.75 (11913.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:54.106312: step 11000, loss = 0.87 (11875.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:54.847211: step 11010, loss = 0.60 (11705.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:55.283928: step 11020, loss = 0.74 (11912.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:55.712823: step 11030, loss = 0.58 (12032.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:56.147009: step 11040, loss = 0.70 (11998.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:56.581536: step 11050, loss = 0.65 (11945.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:57.011413: step 11060, loss = 0.74 (12048.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:57.446603: step 11070, loss = 0.77 (12027.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:57.884808: step 11080, loss = 0.76 (11864.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:58.321784: step 11090, loss = 0.84 (11921.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:58.749614: step 11100, loss = 0.68 (11919.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:59.340381: step 11110, loss = 0.56 (11954.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:21:59.773037: step 11120, loss = 0.59 (11814.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:00.202028: step 11130, loss = 0.59 (12123.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:00.632063: step 11140, loss = 0.63 (12011.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:01.068985: step 11150, loss = 0.85 (11956.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:01.499221: step 11160, loss = 0.81 (11881.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:01.927404: step 11170, loss = 0.68 (11987.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:02.356209: step 11180, loss = 0.72 (11997.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:02.790968: step 11190, loss = 0.52 (11945.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:03.218595: step 11200, loss = 0.69 (12074.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:03.812162: step 11210, loss = 0.54 (11908.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:04.240669: step 11220, loss = 0.67 (12051.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:04.668936: step 11230, loss = 0.61 (11838.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:05.094980: step 11240, loss = 0.73 (12015.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:05.522203: step 11250, loss = 0.76 (11958.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:05.949059: step 11260, loss = 0.62 (12092.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:06.377802: step 11270, loss = 0.58 (11850.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:06.806372: step 11280, loss = 0.65 (12066.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:07.234146: step 11290, loss = 0.69 (11891.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:07.662979: step 11300, loss = 0.63 (11811.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:08.252932: step 11310, loss = 0.56 (11922.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:08.684472: step 11320, loss = 0.74 (11968.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:09.113113: step 11330, loss = 0.54 (11968.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:09.543283: step 11340, loss = 0.69 (11929.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:09.972848: step 11350, loss = 0.69 (12083.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:10.402421: step 11360, loss = 0.63 (11965.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:10.832418: step 11370, loss = 0.74 (11915.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:11.266957: step 11380, loss = 0.51 (11992.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:11.695438: step 11390, loss = 0.63 (11976.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:12.132704: step 11400, loss = 0.71 (12008.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:12.732225: step 11410, loss = 0.63 (11668.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:13.159577: step 11420, loss = 0.56 (12130.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:13.587300: step 11430, loss = 0.58 (12019.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:14.020223: step 11440, loss = 0.59 (11846.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:14.450250: step 11450, loss = 0.74 (11929.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:14.882211: step 11460, loss = 0.78 (11950.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:15.313155: step 11470, loss = 0.66 (11980.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:15.748191: step 11480, loss = 0.61 (11938.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:16.175317: step 11490, loss = 0.72 (11858.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:16.604517: step 11500, loss = 0.70 (11884.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:17.201173: step 11510, loss = 0.73 (11940.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:17.641773: step 11520, loss = 0.69 (11896.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:18.074044: step 11530, loss = 0.73 (12092.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:18.504111: step 11540, loss = 0.55 (11898.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:18.932893: step 11550, loss = 0.66 (11863.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:19.359823: step 11560, loss = 0.75 (12140.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:19.790062: step 11570, loss = 0.75 (11725.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:20.220923: step 11580, loss = 0.51 (11846.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:20.651438: step 11590, loss = 0.62 (11935.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:21.078414: step 11600, loss = 0.78 (11840.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:21.670599: step 11610, loss = 0.75 (11781.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:22.105458: step 11620, loss = 0.64 (12047.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:22.540424: step 11630, loss = 0.57 (11782.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:22.971131: step 11640, loss = 0.65 (11943.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:23.399039: step 11650, loss = 0.58 (12011.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:23.831370: step 11660, loss = 0.58 (11981.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:24.259296: step 11670, loss = 0.72 (11802.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:24.687615: step 11680, loss = 0.64 (11934.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:25.114114: step 11690, loss = 0.68 (12057.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:25.541059: step 11700, loss = 0.57 (12055.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:26.131825: step 11710, loss = 0.68 (11995.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:26.560980: step 11720, loss = 0.62 (12073.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:26.989241: step 11730, loss = 0.75 (11852.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:27.415644: step 11740, loss = 0.70 (12116.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:27.842227: step 11750, loss = 0.75 (12047.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:28.271699: step 11760, loss = 0.64 (11976.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:28.697527: step 11770, loss = 0.53 (12114.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:29.125664: step 11780, loss = 0.80 (11821.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:29.555376: step 11790, loss = 0.69 (12004.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:29.982953: step 11800, loss = 0.59 (11996.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:30.571966: step 11810, loss = 0.64 (12046.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:30.998020: step 11820, loss = 0.78 (12021.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:31.427891: step 11830, loss = 0.50 (12010.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:31.854748: step 11840, loss = 0.65 (12063.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:32.286603: step 11850, loss = 0.85 (11982.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:32.713373: step 11860, loss = 0.62 (12014.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:33.142659: step 11870, loss = 0.71 (11902.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:33.570498: step 11880, loss = 0.71 (12128.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:33.998583: step 11890, loss = 0.63 (12116.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:34.426013: step 11900, loss = 0.72 (12027.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:35.032697: step 11910, loss = 0.63 (12104.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:35.457509: step 11920, loss = 0.63 (11979.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:35.887392: step 11930, loss = 0.74 (11948.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:36.317356: step 11940, loss = 0.73 (11783.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:36.750478: step 11950, loss = 0.69 (11903.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:37.181587: step 11960, loss = 0.68 (11864.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:37.609631: step 11970, loss = 0.52 (11949.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:38.041295: step 11980, loss = 0.82 (11967.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:38.471802: step 11990, loss = 0.55 (11843.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:38.908729: step 12000, loss = 0.74 (11701.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:39.646422: step 12010, loss = 0.60 (11833.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:40.073390: step 12020, loss = 0.72 (12128.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:40.505013: step 12030, loss = 0.58 (12047.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:40.936369: step 12040, loss = 0.88 (12094.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:41.366817: step 12050, loss = 0.67 (12167.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:41.797400: step 12060, loss = 0.56 (11860.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:42.235873: step 12070, loss = 0.60 (11953.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:42.663499: step 12080, loss = 0.62 (12017.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:43.093637: step 12090, loss = 0.78 (11952.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:43.524030: step 12100, loss = 0.61 (11832.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:44.118402: step 12110, loss = 0.59 (12236.7 examples/sec; 0.010 sec/batch)
2017-07-19 19:22:44.549359: step 12120, loss = 0.53 (11905.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:44.981404: step 12130, loss = 0.63 (11971.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:45.408439: step 12140, loss = 0.56 (12122.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:45.834666: step 12150, loss = 0.76 (12064.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:46.270489: step 12160, loss = 0.63 (11967.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:46.700489: step 12170, loss = 0.61 (11817.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:47.128945: step 12180, loss = 0.80 (11887.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:47.561354: step 12190, loss = 0.62 (12058.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:47.991182: step 12200, loss = 0.68 (11964.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:48.577824: step 12210, loss = 0.70 (11750.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:49.007068: step 12220, loss = 0.71 (11986.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:49.433083: step 12230, loss = 0.66 (12046.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:49.860779: step 12240, loss = 0.67 (11972.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:50.290588: step 12250, loss = 0.71 (12024.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:50.718505: step 12260, loss = 0.64 (11854.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:51.146412: step 12270, loss = 0.60 (11932.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:51.573595: step 12280, loss = 0.77 (11999.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:52.001599: step 12290, loss = 0.66 (11872.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:52.429038: step 12300, loss = 0.72 (11874.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:53.025445: step 12310, loss = 0.61 (12048.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:53.453287: step 12320, loss = 0.57 (12046.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:53.884151: step 12330, loss = 0.75 (11845.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:54.313091: step 12340, loss = 0.63 (11996.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:54.742245: step 12350, loss = 0.61 (12052.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:55.170059: step 12360, loss = 0.64 (12085.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:55.598944: step 12370, loss = 0.62 (12088.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:56.026202: step 12380, loss = 0.66 (12034.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:56.452894: step 12390, loss = 0.71 (11903.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:56.879506: step 12400, loss = 0.56 (11925.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:57.465250: step 12410, loss = 0.73 (12042.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:57.893015: step 12420, loss = 0.80 (11994.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:58.321213: step 12430, loss = 0.67 (11996.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:58.748788: step 12440, loss = 0.63 (11897.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:59.176411: step 12450, loss = 0.60 (11977.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:22:59.603794: step 12460, loss = 0.75 (12070.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:00.035747: step 12470, loss = 0.76 (11990.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:00.465075: step 12480, loss = 0.62 (11894.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:00.896210: step 12490, loss = 0.52 (11866.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:01.325569: step 12500, loss = 0.69 (12021.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:01.922329: step 12510, loss = 0.70 (11990.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:02.352808: step 12520, loss = 0.65 (12100.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:02.784909: step 12530, loss = 0.69 (12042.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:03.212712: step 12540, loss = 0.57 (11931.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:03.643657: step 12550, loss = 0.76 (12034.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:04.073767: step 12560, loss = 0.65 (11920.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:04.503270: step 12570, loss = 0.76 (11959.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:04.933223: step 12580, loss = 0.61 (11944.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:05.360302: step 12590, loss = 0.58 (12087.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:05.789013: step 12600, loss = 0.63 (12069.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:06.375612: step 12610, loss = 0.53 (12032.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:06.803743: step 12620, loss = 0.57 (12042.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:07.233280: step 12630, loss = 0.61 (12063.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:07.661943: step 12640, loss = 0.64 (12018.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:08.090876: step 12650, loss = 0.53 (12085.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:08.519550: step 12660, loss = 0.52 (12042.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:08.949433: step 12670, loss = 0.64 (11942.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:09.378025: step 12680, loss = 0.85 (11923.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:09.805831: step 12690, loss = 0.71 (11964.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:10.243085: step 12700, loss = 0.57 (11847.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:10.832313: step 12710, loss = 0.72 (11713.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:11.261771: step 12720, loss = 0.59 (11954.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:11.690294: step 12730, loss = 0.76 (12143.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:12.115924: step 12740, loss = 0.57 (11896.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:12.544057: step 12750, loss = 0.52 (12035.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:12.970892: step 12760, loss = 0.65 (11945.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:13.399021: step 12770, loss = 0.60 (11962.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:13.827631: step 12780, loss = 0.74 (11971.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:14.262870: step 12790, loss = 0.55 (11915.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:14.693032: step 12800, loss = 0.64 (11724.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:15.285289: step 12810, loss = 0.54 (12009.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:15.712267: step 12820, loss = 0.64 (11985.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:16.140242: step 12830, loss = 0.52 (12107.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:16.569977: step 12840, loss = 0.56 (11982.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:16.997689: step 12850, loss = 0.61 (12000.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:17.427712: step 12860, loss = 0.63 (11867.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:17.859510: step 12870, loss = 0.77 (11796.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:18.293487: step 12880, loss = 0.55 (11573.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:18.725669: step 12890, loss = 0.57 (12081.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:19.153650: step 12900, loss = 0.58 (11671.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:19.748890: step 12910, loss = 0.51 (11924.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:20.179155: step 12920, loss = 0.59 (11595.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:20.608726: step 12930, loss = 0.68 (11950.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:21.044686: step 12940, loss = 0.56 (11984.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:21.471955: step 12950, loss = 0.73 (11973.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:21.902212: step 12960, loss = 0.64 (11893.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:22.331075: step 12970, loss = 0.59 (11945.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:22.761915: step 12980, loss = 0.61 (11685.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:23.195206: step 12990, loss = 0.79 (11868.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:23.624252: step 13000, loss = 0.73 (11967.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:24.333658: step 13010, loss = 0.82 (11919.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:24.761197: step 13020, loss = 0.55 (11910.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:25.191995: step 13030, loss = 0.67 (12020.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:25.621060: step 13040, loss = 0.63 (12098.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:26.053284: step 13050, loss = 0.51 (11945.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:26.479927: step 13060, loss = 0.72 (12075.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:26.906520: step 13070, loss = 0.74 (12092.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:27.332650: step 13080, loss = 0.76 (11902.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:27.759968: step 13090, loss = 0.70 (12030.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:28.193322: step 13100, loss = 0.74 (11911.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:28.780884: step 13110, loss = 0.68 (12062.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:29.209626: step 13120, loss = 0.64 (11988.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:29.637960: step 13130, loss = 0.72 (11994.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:30.069763: step 13140, loss = 0.60 (11846.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:30.497865: step 13150, loss = 0.67 (12073.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:30.925291: step 13160, loss = 0.64 (12059.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:31.353427: step 13170, loss = 0.54 (12017.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:31.785981: step 13180, loss = 0.68 (11935.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:32.210724: step 13190, loss = 0.53 (12010.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:32.639648: step 13200, loss = 0.49 (12029.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:33.241290: step 13210, loss = 0.75 (11996.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:33.673240: step 13220, loss = 0.51 (11928.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:34.102721: step 13230, loss = 0.60 (11929.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:34.533069: step 13240, loss = 0.68 (12014.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:34.960486: step 13250, loss = 0.56 (11951.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:35.392601: step 13260, loss = 0.66 (10567.2 examples/sec; 0.012 sec/batch)
2017-07-19 19:23:35.829797: step 13270, loss = 0.63 (11972.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:36.259370: step 13280, loss = 0.59 (12056.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:36.689896: step 13290, loss = 0.61 (11844.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:37.117474: step 13300, loss = 0.68 (12102.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:37.708742: step 13310, loss = 0.51 (11933.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:38.142210: step 13320, loss = 0.61 (11964.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:38.572157: step 13330, loss = 0.61 (11825.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:39.009033: step 13340, loss = 0.63 (11911.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:39.440111: step 13350, loss = 0.58 (11914.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:39.868499: step 13360, loss = 0.64 (12034.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:40.306691: step 13370, loss = 0.92 (11945.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:40.734025: step 13380, loss = 0.55 (12000.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:41.163627: step 13390, loss = 0.62 (11775.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:41.600708: step 13400, loss = 0.47 (10500.5 examples/sec; 0.012 sec/batch)
2017-07-19 19:23:42.195254: step 13410, loss = 0.72 (12031.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:42.621939: step 13420, loss = 0.73 (12182.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:43.050870: step 13430, loss = 0.75 (11939.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:43.480585: step 13440, loss = 0.74 (11912.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:43.910011: step 13450, loss = 0.66 (12006.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:44.344466: step 13460, loss = 0.62 (11999.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:44.774937: step 13470, loss = 0.55 (11913.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:45.203580: step 13480, loss = 0.54 (11975.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:45.631296: step 13490, loss = 0.80 (11807.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:46.060276: step 13500, loss = 0.57 (11782.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:46.654382: step 13510, loss = 0.72 (11800.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:47.083705: step 13520, loss = 0.82 (12017.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:47.512206: step 13530, loss = 0.65 (11984.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:47.942864: step 13540, loss = 0.64 (11768.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:48.371093: step 13550, loss = 0.58 (11965.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:48.797766: step 13560, loss = 0.62 (11975.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:49.231082: step 13570, loss = 0.62 (11904.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:49.663922: step 13580, loss = 0.51 (11918.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:50.094087: step 13590, loss = 0.69 (11904.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:50.523728: step 13600, loss = 0.63 (11759.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:51.117397: step 13610, loss = 0.54 (11914.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:51.548243: step 13620, loss = 0.44 (12063.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:51.979028: step 13630, loss = 0.71 (12168.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:52.409838: step 13640, loss = 0.55 (11965.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:52.841263: step 13650, loss = 0.67 (11365.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:53.268528: step 13660, loss = 0.67 (12065.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:53.695965: step 13670, loss = 0.52 (12043.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:54.124853: step 13680, loss = 0.65 (11904.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:54.555754: step 13690, loss = 0.59 (12051.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:54.985992: step 13700, loss = 0.56 (12022.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:55.590692: step 13710, loss = 0.69 (12007.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:56.020664: step 13720, loss = 0.55 (12057.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:56.450646: step 13730, loss = 0.57 (11955.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:56.877814: step 13740, loss = 0.68 (12057.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:57.308127: step 13750, loss = 0.56 (12010.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:57.737252: step 13760, loss = 0.75 (11757.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:58.167069: step 13770, loss = 0.64 (12053.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:58.597962: step 13780, loss = 0.57 (11850.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:59.027492: step 13790, loss = 0.60 (11857.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:23:59.454593: step 13800, loss = 0.59 (11945.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:00.054706: step 13810, loss = 0.65 (11989.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:00.490576: step 13820, loss = 0.88 (11983.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:00.919241: step 13830, loss = 0.60 (12002.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:01.347277: step 13840, loss = 0.64 (11999.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:01.780238: step 13850, loss = 0.59 (11972.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:02.209495: step 13860, loss = 0.51 (12059.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:02.638288: step 13870, loss = 0.76 (12043.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:03.067864: step 13880, loss = 0.65 (11842.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:03.497062: step 13890, loss = 0.59 (11693.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:03.924032: step 13900, loss = 0.68 (12065.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:04.518866: step 13910, loss = 0.61 (12085.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:04.949203: step 13920, loss = 0.63 (11960.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:05.381923: step 13930, loss = 0.51 (10607.0 examples/sec; 0.012 sec/batch)
2017-07-19 19:24:05.809686: step 13940, loss = 0.59 (11989.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:06.244132: step 13950, loss = 0.56 (11847.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:06.673919: step 13960, loss = 0.74 (12093.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:07.102240: step 13970, loss = 0.85 (11994.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:07.527785: step 13980, loss = 0.59 (12070.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:07.957035: step 13990, loss = 0.58 (12007.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:08.391876: step 14000, loss = 0.66 (11969.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:09.132922: step 14010, loss = 0.73 (11776.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:09.562553: step 14020, loss = 0.66 (11777.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:09.993599: step 14030, loss = 0.69 (11906.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:10.421419: step 14040, loss = 0.60 (11998.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:10.849125: step 14050, loss = 0.56 (11814.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:11.278658: step 14060, loss = 0.61 (11900.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:11.704703: step 14070, loss = 0.86 (11911.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:12.131649: step 14080, loss = 0.74 (12065.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:12.562200: step 14090, loss = 0.64 (11956.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:12.989812: step 14100, loss = 0.59 (12040.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:13.583167: step 14110, loss = 0.55 (11982.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:14.011311: step 14120, loss = 0.52 (12063.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:14.439475: step 14130, loss = 0.93 (11810.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:14.868685: step 14140, loss = 0.58 (11975.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:15.296166: step 14150, loss = 0.58 (12180.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:15.724999: step 14160, loss = 0.62 (11856.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:16.152474: step 14170, loss = 0.56 (11838.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:16.587729: step 14180, loss = 0.68 (11961.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:17.014880: step 14190, loss = 0.52 (11955.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:17.440875: step 14200, loss = 0.81 (12082.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:18.035973: step 14210, loss = 0.63 (12000.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:18.467950: step 14220, loss = 0.56 (11941.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:18.896241: step 14230, loss = 0.59 (12008.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:19.324029: step 14240, loss = 0.68 (11991.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:19.753337: step 14250, loss = 0.55 (11925.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:20.181984: step 14260, loss = 0.58 (11912.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:20.609352: step 14270, loss = 0.62 (12086.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:21.042639: step 14280, loss = 0.64 (12081.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:21.468702: step 14290, loss = 0.60 (12123.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:21.896045: step 14300, loss = 0.65 (11959.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:22.511149: step 14310, loss = 0.68 (11675.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:22.946488: step 14320, loss = 0.58 (11908.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:23.380069: step 14330, loss = 0.60 (11711.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:23.805621: step 14340, loss = 0.53 (12001.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:24.233627: step 14350, loss = 0.44 (12034.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:24.661338: step 14360, loss = 0.57 (11930.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:25.091083: step 14370, loss = 0.67 (11783.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:25.518782: step 14380, loss = 0.69 (11851.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:25.948818: step 14390, loss = 0.56 (11760.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:26.393364: step 14400, loss = 0.73 (11949.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:26.985627: step 14410, loss = 0.57 (10526.1 examples/sec; 0.012 sec/batch)
2017-07-19 19:24:27.429384: step 14420, loss = 0.66 (12007.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:27.862418: step 14430, loss = 0.56 (12013.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:28.292405: step 14440, loss = 0.66 (11953.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:28.721038: step 14450, loss = 0.59 (11980.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:29.158269: step 14460, loss = 0.64 (11909.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:29.592424: step 14470, loss = 0.60 (11882.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:30.023383: step 14480, loss = 0.48 (11871.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:30.451896: step 14490, loss = 0.72 (11811.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:30.879268: step 14500, loss = 0.80 (12024.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:31.471963: step 14510, loss = 0.63 (11962.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:31.899222: step 14520, loss = 0.53 (11868.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:32.325829: step 14530, loss = 0.72 (12044.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:32.751273: step 14540, loss = 0.80 (11838.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:33.177355: step 14550, loss = 0.69 (11993.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:33.603935: step 14560, loss = 0.55 (11983.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:34.032706: step 14570, loss = 0.63 (12009.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:34.461765: step 14580, loss = 0.74 (12127.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:34.891188: step 14590, loss = 0.59 (12087.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:35.325768: step 14600, loss = 0.60 (11893.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:35.920254: step 14610, loss = 0.56 (11966.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:36.347569: step 14620, loss = 0.65 (11875.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:36.775558: step 14630, loss = 0.71 (11978.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:37.203505: step 14640, loss = 0.74 (11948.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:37.630269: step 14650, loss = 0.61 (12112.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:38.059246: step 14660, loss = 0.58 (12030.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:38.492632: step 14670, loss = 0.59 (11868.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:38.920921: step 14680, loss = 0.71 (12119.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:39.350866: step 14690, loss = 0.45 (12008.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:39.780233: step 14700, loss = 0.60 (11918.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:40.374214: step 14710, loss = 0.60 (11903.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:40.803332: step 14720, loss = 0.48 (11958.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:41.230430: step 14730, loss = 0.77 (11973.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:41.658132: step 14740, loss = 0.78 (11886.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:42.090304: step 14750, loss = 0.75 (11926.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:42.520399: step 14760, loss = 0.60 (11814.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:42.951095: step 14770, loss = 0.62 (12084.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:43.380417: step 14780, loss = 0.73 (11860.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:43.810350: step 14790, loss = 0.64 (11881.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:44.238609: step 14800, loss = 0.59 (11965.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:44.831609: step 14810, loss = 0.50 (11786.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:45.260217: step 14820, loss = 0.58 (12037.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:45.688271: step 14830, loss = 0.66 (11650.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:46.114826: step 14840, loss = 0.68 (11985.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:46.541929: step 14850, loss = 0.62 (12068.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:46.969291: step 14860, loss = 0.62 (11718.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:47.404360: step 14870, loss = 0.58 (11841.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:47.832707: step 14880, loss = 0.58 (12082.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:48.268625: step 14890, loss = 0.57 (12201.2 examples/sec; 0.010 sec/batch)
2017-07-19 19:24:48.696210: step 14900, loss = 0.72 (12016.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:49.287727: step 14910, loss = 0.73 (11925.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:49.714219: step 14920, loss = 0.53 (11857.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:50.141193: step 14930, loss = 0.80 (11936.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:50.569776: step 14940, loss = 0.68 (12001.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:50.999335: step 14950, loss = 0.61 (12086.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:51.440185: step 14960, loss = 0.68 (11673.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:51.869364: step 14970, loss = 0.59 (11847.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:52.297883: step 14980, loss = 0.58 (11855.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:52.725215: step 14990, loss = 0.66 (11958.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:53.152771: step 15000, loss = 0.57 (11679.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:53.861933: step 15010, loss = 0.75 (12022.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:54.290601: step 15020, loss = 0.50 (12004.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:54.722417: step 15030, loss = 0.74 (11888.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:55.162148: step 15040, loss = 0.67 (11965.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:55.598584: step 15050, loss = 0.66 (11625.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:56.029344: step 15060, loss = 0.56 (11987.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:56.457805: step 15070, loss = 0.63 (11812.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:56.899060: step 15080, loss = 0.59 (12021.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:57.324869: step 15090, loss = 0.50 (12141.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:57.755413: step 15100, loss = 0.81 (11990.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:58.341753: step 15110, loss = 0.76 (11962.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:58.773208: step 15120, loss = 0.74 (11855.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:59.201760: step 15130, loss = 0.70 (12076.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:24:59.628699: step 15140, loss = 0.75 (11929.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:00.055193: step 15150, loss = 0.65 (12036.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:00.483746: step 15160, loss = 0.60 (11909.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:00.913061: step 15170, loss = 0.70 (11930.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:01.338575: step 15180, loss = 0.62 (12110.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:01.769063: step 15190, loss = 0.55 (11791.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:02.197777: step 15200, loss = 0.65 (11941.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:02.790368: step 15210, loss = 0.56 (12102.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:03.225970: step 15220, loss = 0.62 (11974.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:03.654706: step 15230, loss = 0.67 (11668.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:04.085324: step 15240, loss = 0.60 (11886.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:04.515622: step 15250, loss = 0.63 (11952.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:04.944228: step 15260, loss = 0.64 (11885.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:05.372568: step 15270, loss = 0.72 (12033.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:05.802842: step 15280, loss = 0.62 (12012.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:06.229175: step 15290, loss = 0.60 (12048.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:06.658995: step 15300, loss = 0.51 (11983.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:07.252146: step 15310, loss = 0.71 (11959.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:07.695654: step 15320, loss = 0.62 (11983.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:08.125679: step 15330, loss = 0.67 (12035.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:08.556149: step 15340, loss = 0.57 (11949.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:08.992802: step 15350, loss = 0.83 (10873.4 examples/sec; 0.012 sec/batch)
2017-07-19 19:25:09.423145: step 15360, loss = 0.62 (11756.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:09.853874: step 15370, loss = 0.52 (11954.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:10.283503: step 15380, loss = 0.61 (12141.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:10.711764: step 15390, loss = 0.65 (11927.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:11.138680: step 15400, loss = 0.61 (12036.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:11.731553: step 15410, loss = 0.68 (12179.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:12.160706: step 15420, loss = 0.58 (11826.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:12.590540: step 15430, loss = 0.74 (11406.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:13.022602: step 15440, loss = 0.61 (11954.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:13.450506: step 15450, loss = 0.53 (12075.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:13.883612: step 15460, loss = 0.66 (12048.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:14.313855: step 15470, loss = 0.58 (11847.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:14.744451: step 15480, loss = 0.62 (12015.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:15.172001: step 15490, loss = 0.77 (12043.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:15.601060: step 15500, loss = 0.61 (11901.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:16.204819: step 15510, loss = 0.67 (11881.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:16.633643: step 15520, loss = 0.68 (12075.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:17.061516: step 15530, loss = 0.53 (11972.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:17.498084: step 15540, loss = 0.82 (11989.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:17.924926: step 15550, loss = 0.75 (12018.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:18.353849: step 15560, loss = 0.53 (11875.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:18.784116: step 15570, loss = 0.63 (11889.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:19.215020: step 15580, loss = 0.83 (12019.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:19.644407: step 15590, loss = 0.56 (11702.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:20.073191: step 15600, loss = 0.55 (12203.0 examples/sec; 0.010 sec/batch)
2017-07-19 19:25:20.658319: step 15610, loss = 0.58 (12006.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:21.087560: step 15620, loss = 0.45 (11980.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:21.514456: step 15630, loss = 0.54 (12017.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:21.943039: step 15640, loss = 0.57 (11860.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:22.372689: step 15650, loss = 0.60 (11925.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:22.801110: step 15660, loss = 0.59 (11930.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:23.231147: step 15670, loss = 0.62 (11965.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:23.662876: step 15680, loss = 0.60 (12036.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:24.089691: step 15690, loss = 0.49 (11998.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:24.523946: step 15700, loss = 0.68 (11875.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:25.128693: step 15710, loss = 0.58 (11727.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:25.553590: step 15720, loss = 0.58 (12061.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:25.979868: step 15730, loss = 0.64 (11877.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:26.413316: step 15740, loss = 0.74 (10725.0 examples/sec; 0.012 sec/batch)
2017-07-19 19:25:26.840962: step 15750, loss = 0.65 (11703.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:27.268593: step 15760, loss = 0.54 (12012.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:27.696306: step 15770, loss = 0.77 (11996.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:28.126352: step 15780, loss = 0.58 (11631.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:28.553108: step 15790, loss = 0.65 (11940.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:28.980352: step 15800, loss = 0.66 (12035.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:29.567538: step 15810, loss = 0.69 (12000.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:29.997404: step 15820, loss = 0.53 (11900.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:30.422931: step 15830, loss = 0.48 (11914.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:30.850149: step 15840, loss = 0.75 (12092.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:31.279134: step 15850, loss = 0.67 (11809.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:31.707415: step 15860, loss = 0.51 (11983.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:32.135220: step 15870, loss = 0.71 (11877.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:32.563501: step 15880, loss = 0.60 (12087.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:32.991817: step 15890, loss = 0.60 (12011.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:33.420595: step 15900, loss = 0.69 (11930.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:34.011988: step 15910, loss = 0.80 (11975.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:34.439937: step 15920, loss = 0.64 (11920.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:34.869899: step 15930, loss = 0.59 (11990.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:35.301478: step 15940, loss = 0.54 (12005.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:35.729722: step 15950, loss = 0.66 (11967.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:36.158407: step 15960, loss = 0.68 (12120.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:36.591623: step 15970, loss = 0.59 (10868.5 examples/sec; 0.012 sec/batch)
2017-07-19 19:25:37.020805: step 15980, loss = 0.64 (12012.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:37.447717: step 15990, loss = 0.52 (12076.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:37.876376: step 16000, loss = 0.60 (11899.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:38.606030: step 16010, loss = 0.61 (11677.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:39.039444: step 16020, loss = 0.57 (11736.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:39.467258: step 16030, loss = 0.52 (11991.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:39.896202: step 16040, loss = 0.57 (12017.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:40.322821: step 16050, loss = 0.62 (12056.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:40.750391: step 16060, loss = 0.80 (12045.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:41.179097: step 16070, loss = 0.71 (11824.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:41.606726: step 16080, loss = 0.55 (12016.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:42.035044: step 16090, loss = 0.60 (11795.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:42.468658: step 16100, loss = 0.65 (11865.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:43.063243: step 16110, loss = 0.53 (11946.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:43.494587: step 16120, loss = 0.64 (11964.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:43.920268: step 16130, loss = 0.76 (12046.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:44.350517: step 16140, loss = 0.63 (11718.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:44.778215: step 16150, loss = 0.60 (12024.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:45.205478: step 16160, loss = 0.61 (12156.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:45.631106: step 16170, loss = 0.56 (12003.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:46.059380: step 16180, loss = 0.59 (12045.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:46.494561: step 16190, loss = 0.61 (11920.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:46.922137: step 16200, loss = 0.53 (12072.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:47.518184: step 16210, loss = 0.57 (11904.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:47.945011: step 16220, loss = 0.54 (11961.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:48.376192: step 16230, loss = 0.66 (12026.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:48.802927: step 16240, loss = 0.56 (12061.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:49.229895: step 16250, loss = 0.47 (12057.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:49.656600: step 16260, loss = 0.60 (11990.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:50.091200: step 16270, loss = 0.64 (11839.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:50.519521: step 16280, loss = 0.62 (12001.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:50.966148: step 16290, loss = 0.55 (11986.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:51.393431: step 16300, loss = 0.64 (11920.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:51.991019: step 16310, loss = 0.59 (11942.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:52.421508: step 16320, loss = 0.56 (11978.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:52.852124: step 16330, loss = 0.62 (11831.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:53.282130: step 16340, loss = 0.51 (11983.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:53.711164: step 16350, loss = 0.69 (11890.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:54.146941: step 16360, loss = 0.59 (11928.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:54.581633: step 16370, loss = 0.62 (11945.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:55.011487: step 16380, loss = 0.63 (12102.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:55.441529: step 16390, loss = 0.61 (12055.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:55.871808: step 16400, loss = 0.65 (11875.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:56.466014: step 16410, loss = 0.69 (12066.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:56.893835: step 16420, loss = 0.44 (12043.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:57.322571: step 16430, loss = 0.58 (12070.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:57.752479: step 16440, loss = 0.64 (11711.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:58.180485: step 16450, loss = 0.65 (12050.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:58.610280: step 16460, loss = 0.61 (11969.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:59.037046: step 16470, loss = 0.69 (12098.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:59.464572: step 16480, loss = 0.54 (11849.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:25:59.895409: step 16490, loss = 0.51 (11938.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:00.325625: step 16500, loss = 0.62 (11858.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:00.922833: step 16510, loss = 0.66 (11956.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:01.352022: step 16520, loss = 0.47 (12021.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:01.779938: step 16530, loss = 0.58 (12205.8 examples/sec; 0.010 sec/batch)
2017-07-19 19:26:02.206435: step 16540, loss = 0.72 (12101.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:02.631814: step 16550, loss = 0.58 (12116.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:03.058358: step 16560, loss = 0.48 (12112.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:03.487089: step 16570, loss = 0.58 (12004.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:03.913566: step 16580, loss = 0.48 (11888.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:04.341871: step 16590, loss = 0.62 (12073.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:04.774718: step 16600, loss = 0.72 (11991.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:05.363346: step 16610, loss = 0.58 (12070.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:05.790555: step 16620, loss = 0.52 (11898.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:06.219246: step 16630, loss = 0.52 (11924.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:06.646733: step 16640, loss = 0.49 (11907.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:07.073593: step 16650, loss = 0.52 (11984.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:07.500918: step 16660, loss = 0.61 (11985.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:07.929445: step 16670, loss = 0.63 (11801.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:08.360539: step 16680, loss = 0.70 (12017.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:08.792809: step 16690, loss = 0.69 (11995.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:09.221890: step 16700, loss = 0.54 (12015.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:09.809281: step 16710, loss = 0.85 (11891.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:10.237556: step 16720, loss = 0.67 (11962.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:10.664983: step 16730, loss = 0.55 (12191.7 examples/sec; 0.010 sec/batch)
2017-07-19 19:26:11.094127: step 16740, loss = 0.80 (11861.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:11.521801: step 16750, loss = 0.63 (11943.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:11.949469: step 16760, loss = 0.58 (11856.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:12.375895: step 16770, loss = 0.70 (12006.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:12.807151: step 16780, loss = 0.63 (11991.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:13.235406: step 16790, loss = 0.59 (12010.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:13.664901: step 16800, loss = 0.64 (11846.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:14.265474: step 16810, loss = 0.53 (11552.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:14.697453: step 16820, loss = 0.61 (11964.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:15.125494: step 16830, loss = 0.59 (11917.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:15.553678: step 16840, loss = 0.55 (11959.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:15.982950: step 16850, loss = 0.58 (11907.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:16.416952: step 16860, loss = 0.53 (11963.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:16.849776: step 16870, loss = 0.66 (11871.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:17.280416: step 16880, loss = 0.56 (11848.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:17.714466: step 16890, loss = 0.57 (12146.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:18.140904: step 16900, loss = 0.63 (12037.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:18.731099: step 16910, loss = 0.66 (12061.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:19.160270: step 16920, loss = 0.74 (12004.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:19.591015: step 16930, loss = 0.77 (12189.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:20.020356: step 16940, loss = 0.63 (11982.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:20.447504: step 16950, loss = 0.63 (12153.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:20.874300: step 16960, loss = 0.47 (11971.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:21.303600: step 16970, loss = 0.65 (11942.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:21.736482: step 16980, loss = 0.57 (12013.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:22.163630: step 16990, loss = 0.48 (11919.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:22.601853: step 17000, loss = 0.69 (11976.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:23.311426: step 17010, loss = 0.57 (12113.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:23.739535: step 17020, loss = 0.47 (12051.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:24.175560: step 17030, loss = 0.71 (10111.6 examples/sec; 0.013 sec/batch)
2017-07-19 19:26:24.602722: step 17040, loss = 0.74 (12051.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:25.030682: step 17050, loss = 0.64 (12021.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:25.456220: step 17060, loss = 0.61 (12004.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:25.883788: step 17070, loss = 0.59 (12105.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:26.310386: step 17080, loss = 0.58 (11866.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:26.742921: step 17090, loss = 0.70 (12061.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:27.171534: step 17100, loss = 0.62 (11662.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:27.764200: step 17110, loss = 0.57 (12061.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:28.200045: step 17120, loss = 0.45 (11940.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:28.628703: step 17130, loss = 0.63 (12001.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:29.061155: step 17140, loss = 0.65 (11802.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:29.490078: step 17150, loss = 0.68 (11966.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:29.920069: step 17160, loss = 0.66 (12010.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:30.346862: step 17170, loss = 0.57 (11948.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:30.774172: step 17180, loss = 0.51 (11944.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:31.200728: step 17190, loss = 0.53 (11992.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:31.628653: step 17200, loss = 0.65 (11987.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:32.220587: step 17210, loss = 0.56 (11903.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:32.647390: step 17220, loss = 0.54 (12079.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:33.075458: step 17230, loss = 0.50 (12028.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:33.503704: step 17240, loss = 0.62 (11917.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:33.929780: step 17250, loss = 0.59 (12077.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:34.356125: step 17260, loss = 0.67 (11906.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:34.782540: step 17270, loss = 0.63 (11900.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:35.208366: step 17280, loss = 0.74 (12126.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:35.635420: step 17290, loss = 0.68 (12131.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:36.065281: step 17300, loss = 0.67 (11913.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:36.655577: step 17310, loss = 0.77 (11945.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:37.084739: step 17320, loss = 0.53 (12059.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:37.511433: step 17330, loss = 0.62 (11866.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:37.940126: step 17340, loss = 0.69 (12003.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:38.368248: step 17350, loss = 0.68 (12135.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:38.796157: step 17360, loss = 0.60 (12045.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:39.237546: step 17370, loss = 0.66 (12081.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:39.667133: step 17380, loss = 0.70 (11687.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:40.095815: step 17390, loss = 0.51 (12086.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:40.522441: step 17400, loss = 0.56 (12116.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:41.111039: step 17410, loss = 0.60 (12001.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:41.541535: step 17420, loss = 0.64 (11985.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:41.971041: step 17430, loss = 0.58 (11898.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:42.401541: step 17440, loss = 0.55 (11954.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:42.830277: step 17450, loss = 0.59 (11891.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:43.256830: step 17460, loss = 0.54 (12048.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:43.685643: step 17470, loss = 0.50 (11943.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:44.113670: step 17480, loss = 0.55 (12060.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:44.543263: step 17490, loss = 0.54 (12008.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:44.973188: step 17500, loss = 0.52 (11896.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:45.573333: step 17510, loss = 0.72 (12038.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:45.999893: step 17520, loss = 0.61 (12126.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:46.426446: step 17530, loss = 0.58 (11923.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:46.855940: step 17540, loss = 0.62 (11915.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:47.285457: step 17550, loss = 0.57 (11901.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:47.716613: step 17560, loss = 0.63 (11988.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:48.145909: step 17570, loss = 0.72 (12130.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:48.573554: step 17580, loss = 0.72 (11984.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:49.001660: step 17590, loss = 0.61 (12094.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:49.432395: step 17600, loss = 0.56 (12052.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:50.033064: step 17610, loss = 0.53 (11786.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:50.464470: step 17620, loss = 0.54 (11991.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:50.896278: step 17630, loss = 0.45 (12062.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:51.322099: step 17640, loss = 0.59 (12005.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:51.747398: step 17650, loss = 0.51 (12120.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:52.174520: step 17660, loss = 0.56 (11993.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:52.602712: step 17670, loss = 0.77 (12034.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:53.030237: step 17680, loss = 0.71 (12060.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:53.457012: step 17690, loss = 0.73 (11913.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:53.886494: step 17700, loss = 0.59 (12010.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:54.472215: step 17710, loss = 0.55 (12128.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:54.900699: step 17720, loss = 0.57 (11779.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:55.327281: step 17730, loss = 0.75 (11872.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:55.754096: step 17740, loss = 0.58 (11899.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:56.180384: step 17750, loss = 0.72 (12019.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:56.607117: step 17760, loss = 0.66 (11997.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:57.034259: step 17770, loss = 0.70 (12059.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:57.465765: step 17780, loss = 0.55 (11934.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:57.894499: step 17790, loss = 0.68 (11859.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:58.321000: step 17800, loss = 0.63 (11931.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:58.919056: step 17810, loss = 0.52 (12055.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:59.347288: step 17820, loss = 0.57 (11962.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:26:59.775510: step 17830, loss = 0.59 (11883.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:00.202061: step 17840, loss = 0.65 (12173.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:00.631408: step 17850, loss = 0.64 (12098.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:01.059096: step 17860, loss = 0.77 (11954.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:01.491637: step 17870, loss = 0.79 (11995.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:01.919217: step 17880, loss = 0.60 (11917.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:02.347534: step 17890, loss = 0.65 (11940.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:02.774646: step 17900, loss = 0.66 (11942.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:03.363926: step 17910, loss = 0.55 (11898.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:03.792496: step 17920, loss = 0.71 (11991.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:04.224694: step 17930, loss = 0.54 (11879.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:04.659072: step 17940, loss = 0.61 (11630.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:05.094111: step 17950, loss = 0.57 (12026.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:05.521661: step 17960, loss = 0.60 (12075.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:05.951017: step 17970, loss = 0.50 (11999.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:06.382091: step 17980, loss = 0.67 (11815.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:06.811052: step 17990, loss = 0.63 (12039.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:07.239750: step 18000, loss = 0.60 (12106.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:07.954477: step 18010, loss = 0.62 (11965.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:08.385194: step 18020, loss = 0.50 (11915.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:08.815536: step 18030, loss = 0.66 (11844.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:09.251648: step 18040, loss = 0.60 (10349.0 examples/sec; 0.012 sec/batch)
2017-07-19 19:27:09.686522: step 18050, loss = 0.63 (11889.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:10.115033: step 18060, loss = 0.56 (11762.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:10.542635: step 18070, loss = 0.59 (11875.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:10.977393: step 18080, loss = 0.62 (10191.0 examples/sec; 0.013 sec/batch)
2017-07-19 19:27:11.411114: step 18090, loss = 0.65 (11825.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:11.840737: step 18100, loss = 0.53 (11818.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:12.437440: step 18110, loss = 0.63 (11857.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:12.869082: step 18120, loss = 0.59 (12030.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:13.296555: step 18130, loss = 0.57 (11824.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:13.724322: step 18140, loss = 0.53 (12010.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:14.151403: step 18150, loss = 0.52 (11839.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:14.580349: step 18160, loss = 0.62 (12085.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:15.009537: step 18170, loss = 0.73 (11984.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:15.436560: step 18180, loss = 0.59 (12221.5 examples/sec; 0.010 sec/batch)
2017-07-19 19:27:15.862396: step 18190, loss = 0.56 (12121.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:16.288557: step 18200, loss = 0.53 (12025.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:16.875081: step 18210, loss = 0.67 (12191.2 examples/sec; 0.010 sec/batch)
2017-07-19 19:27:17.304420: step 18220, loss = 0.49 (11876.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:17.732713: step 18230, loss = 0.47 (11889.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:18.160360: step 18240, loss = 0.72 (12099.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:18.587936: step 18250, loss = 0.67 (11977.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:19.015892: step 18260, loss = 0.72 (11988.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:19.447772: step 18270, loss = 0.54 (12113.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:19.876928: step 18280, loss = 0.62 (11914.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:20.304225: step 18290, loss = 0.57 (12078.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:20.732220: step 18300, loss = 0.63 (12005.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:21.324872: step 18310, loss = 0.58 (11775.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:21.750724: step 18320, loss = 0.57 (11970.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:22.178671: step 18330, loss = 0.80 (11955.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:22.606027: step 18340, loss = 0.65 (12041.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:23.035184: step 18350, loss = 0.49 (12040.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:23.464179: step 18360, loss = 0.75 (11961.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:23.900523: step 18370, loss = 0.64 (11791.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:24.332595: step 18380, loss = 0.57 (11911.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:24.762877: step 18390, loss = 0.69 (12216.0 examples/sec; 0.010 sec/batch)
2017-07-19 19:27:25.189835: step 18400, loss = 0.53 (11956.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:25.785773: step 18410, loss = 0.70 (12008.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:26.221578: step 18420, loss = 0.52 (10704.3 examples/sec; 0.012 sec/batch)
2017-07-19 19:27:26.662680: step 18430, loss = 0.73 (12004.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:27.090827: step 18440, loss = 0.68 (11968.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:27.517064: step 18450, loss = 0.55 (12073.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:27.945157: step 18460, loss = 0.64 (12073.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:28.375346: step 18470, loss = 0.55 (12053.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:28.804490: step 18480, loss = 0.70 (11978.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:29.233365: step 18490, loss = 0.48 (11933.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:29.664157: step 18500, loss = 0.58 (11892.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:30.261418: step 18510, loss = 0.68 (12129.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:30.690985: step 18520, loss = 0.60 (11979.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:31.118680: step 18530, loss = 0.45 (11959.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:31.544576: step 18540, loss = 0.58 (11975.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:31.974069: step 18550, loss = 0.64 (11986.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:32.403006: step 18560, loss = 0.48 (12107.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:32.831206: step 18570, loss = 0.64 (11988.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:33.262935: step 18580, loss = 0.78 (12169.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:33.691526: step 18590, loss = 0.46 (11772.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:34.119915: step 18600, loss = 0.56 (12036.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:34.714738: step 18610, loss = 0.57 (11613.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:35.149102: step 18620, loss = 0.63 (11936.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:35.575604: step 18630, loss = 0.66 (12082.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:36.004333: step 18640, loss = 0.55 (11949.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:36.430419: step 18650, loss = 0.58 (11951.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:36.858363: step 18660, loss = 0.63 (12059.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:37.288482: step 18670, loss = 0.63 (12031.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:37.716524: step 18680, loss = 0.57 (11939.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:38.144127: step 18690, loss = 0.55 (11965.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:38.571817: step 18700, loss = 0.71 (11982.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:39.159342: step 18710, loss = 0.55 (12044.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:39.586699: step 18720, loss = 0.65 (12060.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:40.014370: step 18730, loss = 0.60 (12003.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:40.443848: step 18740, loss = 0.66 (12096.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:40.871952: step 18750, loss = 0.55 (11952.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:41.300490: step 18760, loss = 0.55 (12005.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:41.729776: step 18770, loss = 0.74 (11795.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:42.161234: step 18780, loss = 0.55 (11879.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:42.586867: step 18790, loss = 0.56 (11813.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:43.023989: step 18800, loss = 0.57 (11791.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:43.611948: step 18810, loss = 0.58 (12095.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:44.041642: step 18820, loss = 0.62 (11865.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:44.470296: step 18830, loss = 0.52 (12011.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:44.899746: step 18840, loss = 0.72 (11893.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:45.326869: step 18850, loss = 0.65 (12099.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:45.754650: step 18860, loss = 0.49 (11918.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:46.186902: step 18870, loss = 0.59 (12105.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:46.615172: step 18880, loss = 0.58 (11889.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:47.048256: step 18890, loss = 0.59 (11797.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:47.479463: step 18900, loss = 0.64 (11793.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:48.074936: step 18910, loss = 0.94 (11974.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:48.501845: step 18920, loss = 0.64 (12286.5 examples/sec; 0.010 sec/batch)
2017-07-19 19:27:48.929943: step 18930, loss = 0.51 (11954.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:49.357046: step 18940, loss = 0.74 (11965.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:49.785996: step 18950, loss = 0.56 (11966.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:50.212286: step 18960, loss = 0.61 (11707.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:50.639147: step 18970, loss = 0.58 (11953.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:51.067606: step 18980, loss = 0.58 (12104.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:51.494516: step 18990, loss = 0.45 (12061.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:51.921408: step 19000, loss = 0.60 (12008.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:52.629611: step 19010, loss = 0.55 (11880.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:53.055973: step 19020, loss = 0.73 (11994.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:53.486021: step 19030, loss = 0.74 (11802.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:53.915481: step 19040, loss = 0.54 (12069.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:54.344651: step 19050, loss = 0.69 (11973.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:54.774612: step 19060, loss = 0.64 (11835.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:55.202796: step 19070, loss = 0.55 (11874.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:55.634974: step 19080, loss = 0.65 (11808.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:56.061116: step 19090, loss = 0.55 (12123.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:56.489343: step 19100, loss = 0.59 (11977.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:57.079034: step 19110, loss = 0.64 (12255.1 examples/sec; 0.010 sec/batch)
2017-07-19 19:27:57.507880: step 19120, loss = 0.61 (11847.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:57.936605: step 19130, loss = 0.73 (12007.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:58.379257: step 19140, loss = 0.59 (11894.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:58.806816: step 19150, loss = 0.50 (11869.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:59.240302: step 19160, loss = 0.50 (11920.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:27:59.670614: step 19170, loss = 0.55 (11867.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:00.102955: step 19180, loss = 0.68 (11905.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:00.531441: step 19190, loss = 0.65 (11982.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:00.960797: step 19200, loss = 0.76 (12021.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:01.549780: step 19210, loss = 0.71 (11850.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:01.983874: step 19220, loss = 0.54 (12062.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:02.410129: step 19230, loss = 0.58 (11926.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:02.841314: step 19240, loss = 0.61 (12015.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:03.272252: step 19250, loss = 0.57 (11995.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:03.702062: step 19260, loss = 0.74 (11890.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:04.134539: step 19270, loss = 0.68 (10886.0 examples/sec; 0.012 sec/batch)
2017-07-19 19:28:04.563207: step 19280, loss = 0.72 (11982.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:04.990262: step 19290, loss = 0.65 (12072.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:05.417364: step 19300, loss = 0.61 (11968.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:06.024921: step 19310, loss = 0.56 (12018.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:06.453161: step 19320, loss = 0.74 (11574.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:06.883499: step 19330, loss = 0.74 (11958.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:07.311394: step 19340, loss = 0.71 (11965.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:07.739518: step 19350, loss = 0.54 (11879.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:08.176574: step 19360, loss = 0.69 (11864.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:08.614194: step 19370, loss = 0.50 (11782.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:09.049489: step 19380, loss = 0.43 (11956.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:09.483033: step 19390, loss = 0.47 (11957.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:09.913415: step 19400, loss = 0.68 (11993.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:10.499444: step 19410, loss = 0.60 (11802.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:10.929393: step 19420, loss = 0.61 (11824.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:11.357529: step 19430, loss = 0.47 (12042.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:11.787592: step 19440, loss = 0.67 (11813.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:12.215481: step 19450, loss = 0.49 (12025.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:12.646698: step 19460, loss = 0.72 (11931.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:13.073197: step 19470, loss = 0.74 (11853.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:13.500456: step 19480, loss = 0.62 (11965.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:13.928863: step 19490, loss = 0.60 (11971.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:14.356669: step 19500, loss = 0.59 (12062.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:14.944014: step 19510, loss = 0.61 (11951.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:15.372174: step 19520, loss = 0.57 (12007.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:15.799883: step 19530, loss = 0.53 (11831.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:16.231809: step 19540, loss = 0.48 (11922.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:16.663839: step 19550, loss = 0.63 (12039.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:17.092392: step 19560, loss = 0.61 (12008.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:17.518188: step 19570, loss = 0.52 (11995.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:17.947065: step 19580, loss = 0.68 (11906.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:18.376744: step 19590, loss = 0.58 (12117.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:18.809620: step 19600, loss = 0.69 (12002.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:19.403571: step 19610, loss = 0.53 (12033.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:19.835208: step 19620, loss = 0.69 (12037.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:20.264963: step 19630, loss = 0.71 (12042.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:20.693987: step 19640, loss = 0.62 (11743.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:21.121867: step 19650, loss = 0.76 (11847.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:21.547989: step 19660, loss = 0.64 (11974.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:21.974157: step 19670, loss = 0.65 (12087.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:22.400179: step 19680, loss = 0.57 (11989.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:22.837863: step 19690, loss = 0.64 (11929.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:23.265541: step 19700, loss = 0.60 (12001.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:23.851733: step 19710, loss = 0.54 (12042.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:24.279542: step 19720, loss = 0.68 (11961.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:24.708011: step 19730, loss = 0.56 (11913.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:25.144786: step 19740, loss = 0.44 (12108.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:25.572128: step 19750, loss = 0.53 (12051.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:25.998237: step 19760, loss = 0.55 (11930.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:26.426276: step 19770, loss = 0.49 (12111.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:26.853265: step 19780, loss = 0.51 (11920.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:27.284631: step 19790, loss = 0.57 (12174.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:27.710628: step 19800, loss = 0.53 (12106.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:28.308846: step 19810, loss = 0.61 (11999.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:28.736912: step 19820, loss = 0.48 (12016.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:29.167877: step 19830, loss = 0.51 (12003.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:29.594905: step 19840, loss = 0.59 (12037.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:30.023099: step 19850, loss = 0.63 (11882.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:30.455525: step 19860, loss = 0.74 (12114.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:30.883850: step 19870, loss = 0.55 (12059.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:31.310055: step 19880, loss = 0.73 (12063.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:31.735860: step 19890, loss = 0.58 (12004.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:32.161748: step 19900, loss = 0.50 (11996.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:32.759369: step 19910, loss = 0.56 (11813.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:33.189801: step 19920, loss = 0.53 (11898.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:33.615108: step 19930, loss = 0.53 (12097.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:34.042701: step 19940, loss = 0.61 (11899.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:34.470236: step 19950, loss = 0.51 (11831.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:34.899696: step 19960, loss = 0.75 (11878.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:35.329785: step 19970, loss = 0.56 (11585.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:35.758684: step 19980, loss = 0.61 (11835.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:36.184902: step 19990, loss = 0.58 (12095.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:36.613418: step 20000, loss = 0.48 (11705.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:37.339930: step 20010, loss = 0.69 (11966.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:37.768588: step 20020, loss = 0.59 (11802.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:38.198712: step 20030, loss = 0.48 (11877.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:38.628041: step 20040, loss = 0.58 (12009.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:39.055357: step 20050, loss = 0.54 (11901.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:39.484064: step 20060, loss = 0.58 (12015.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:39.914726: step 20070, loss = 0.56 (12015.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:40.343135: step 20080, loss = 0.60 (11863.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:40.774307: step 20090, loss = 0.49 (11876.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:41.200624: step 20100, loss = 0.54 (11924.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:41.788140: step 20110, loss = 0.68 (12163.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:42.215610: step 20120, loss = 0.59 (11977.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:42.643877: step 20130, loss = 0.62 (11907.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:43.071714: step 20140, loss = 0.64 (11857.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:43.497894: step 20150, loss = 0.59 (11936.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:43.924437: step 20160, loss = 0.55 (12154.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:44.355575: step 20170, loss = 0.53 (11968.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:44.782471: step 20180, loss = 0.58 (12025.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:45.208642: step 20190, loss = 0.53 (12004.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:45.634588: step 20200, loss = 0.62 (11909.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:46.221533: step 20210, loss = 0.55 (12066.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:46.648422: step 20220, loss = 0.69 (11995.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:47.081786: step 20230, loss = 0.57 (11885.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:47.507822: step 20240, loss = 0.63 (12269.3 examples/sec; 0.010 sec/batch)
2017-07-19 19:28:47.936256: step 20250, loss = 0.50 (11916.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:48.362409: step 20260, loss = 0.63 (11951.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:48.789502: step 20270, loss = 0.70 (12037.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:49.217086: step 20280, loss = 0.50 (12000.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:49.646439: step 20290, loss = 0.55 (11802.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:50.074693: step 20300, loss = 0.53 (12062.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:50.673389: step 20310, loss = 0.71 (11807.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:51.101489: step 20320, loss = 0.45 (12044.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:51.536317: step 20330, loss = 0.53 (11810.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:51.966554: step 20340, loss = 0.48 (11976.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:52.393724: step 20350, loss = 0.57 (11994.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:52.822957: step 20360, loss = 0.61 (12085.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:53.256245: step 20370, loss = 0.49 (11918.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:53.683949: step 20380, loss = 0.71 (11650.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:54.111952: step 20390, loss = 0.64 (11991.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:54.540716: step 20400, loss = 0.59 (11922.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:55.137598: step 20410, loss = 0.61 (12037.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:55.567912: step 20420, loss = 0.54 (12067.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:55.997034: step 20430, loss = 0.53 (11940.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:56.426011: step 20440, loss = 0.66 (11889.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:56.856774: step 20450, loss = 0.61 (11730.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:57.283840: step 20460, loss = 0.82 (11964.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:57.718590: step 20470, loss = 0.57 (10611.6 examples/sec; 0.012 sec/batch)
2017-07-19 19:28:58.147662: step 20480, loss = 0.70 (12135.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:58.573662: step 20490, loss = 0.55 (12055.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:59.005506: step 20500, loss = 0.52 (12047.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:28:59.596487: step 20510, loss = 0.57 (12019.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:00.026801: step 20520, loss = 0.57 (11906.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:00.455458: step 20530, loss = 0.58 (11870.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:00.884111: step 20540, loss = 0.64 (12086.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:01.314855: step 20550, loss = 0.63 (11988.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:01.744490: step 20560, loss = 0.60 (12038.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:02.171754: step 20570, loss = 0.57 (12005.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:02.597847: step 20580, loss = 0.50 (12003.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:03.026000: step 20590, loss = 0.63 (12088.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:03.452075: step 20600, loss = 0.52 (12075.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:04.038517: step 20610, loss = 0.77 (11943.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:04.466895: step 20620, loss = 0.58 (11997.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:04.895982: step 20630, loss = 0.63 (12020.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:05.323167: step 20640, loss = 0.64 (12011.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:05.750672: step 20650, loss = 0.70 (12038.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:06.187206: step 20660, loss = 0.61 (10508.4 examples/sec; 0.012 sec/batch)
2017-07-19 19:29:06.613689: step 20670, loss = 0.62 (12133.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:07.042389: step 20680, loss = 0.62 (12059.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:07.467752: step 20690, loss = 0.48 (12037.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:07.895666: step 20700, loss = 0.67 (11876.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:08.482951: step 20710, loss = 0.56 (11959.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:08.916785: step 20720, loss = 0.47 (11833.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:09.346962: step 20730, loss = 0.51 (11903.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:09.776801: step 20740, loss = 0.63 (11839.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:10.205404: step 20750, loss = 0.58 (11884.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:10.636506: step 20760, loss = 0.62 (12128.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:11.063783: step 20770, loss = 0.81 (12043.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:11.491897: step 20780, loss = 0.64 (12019.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:11.921047: step 20790, loss = 0.71 (11898.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:12.347334: step 20800, loss = 0.57 (12013.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:12.943558: step 20810, loss = 0.64 (12016.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:13.372191: step 20820, loss = 0.77 (12080.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:13.799993: step 20830, loss = 0.56 (12017.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:14.227385: step 20840, loss = 0.63 (11887.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:14.655714: step 20850, loss = 0.68 (11869.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:15.085767: step 20860, loss = 0.57 (11908.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:15.514480: step 20870, loss = 0.57 (11829.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:15.944404: step 20880, loss = 0.70 (11917.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:16.372042: step 20890, loss = 0.57 (11939.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:16.801888: step 20900, loss = 0.53 (11907.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:17.391708: step 20910, loss = 0.46 (12031.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:17.817661: step 20920, loss = 0.56 (12010.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:18.244898: step 20930, loss = 0.69 (11912.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:18.674711: step 20940, loss = 0.56 (11951.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:19.104804: step 20950, loss = 0.52 (12026.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:19.534159: step 20960, loss = 0.65 (11734.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:19.962794: step 20970, loss = 0.58 (11950.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:20.395130: step 20980, loss = 0.65 (11910.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:20.824886: step 20990, loss = 0.60 (11888.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:21.254708: step 21000, loss = 0.61 (11976.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:21.965644: step 21010, loss = 0.63 (12038.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:22.393623: step 21020, loss = 0.63 (12073.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:22.825143: step 21030, loss = 0.53 (11974.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:23.255084: step 21040, loss = 0.59 (11995.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:23.685062: step 21050, loss = 0.59 (11900.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:24.119521: step 21060, loss = 0.60 (10331.5 examples/sec; 0.012 sec/batch)
2017-07-19 19:29:24.545457: step 21070, loss = 0.65 (12021.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:24.972073: step 21080, loss = 0.56 (12048.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:25.400967: step 21090, loss = 0.63 (11905.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:25.829347: step 21100, loss = 0.70 (12022.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:26.419169: step 21110, loss = 0.49 (11861.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:26.848435: step 21120, loss = 0.81 (12077.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:27.274480: step 21130, loss = 0.48 (12030.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:27.714925: step 21140, loss = 0.60 (11947.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:28.143039: step 21150, loss = 0.60 (12010.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:28.570500: step 21160, loss = 0.47 (12114.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:28.998835: step 21170, loss = 0.61 (11976.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:29.426975: step 21180, loss = 0.69 (11910.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:29.857280: step 21190, loss = 0.54 (11694.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:30.284391: step 21200, loss = 0.72 (11995.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:30.872940: step 21210, loss = 0.64 (12145.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:31.302770: step 21220, loss = 0.41 (11926.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:31.732408: step 21230, loss = 0.59 (11993.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:32.167465: step 21240, loss = 0.65 (10204.0 examples/sec; 0.013 sec/batch)
2017-07-19 19:29:32.596225: step 21250, loss = 0.60 (12024.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:33.025178: step 21260, loss = 0.61 (12020.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:33.454570: step 21270, loss = 0.52 (11860.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:33.882164: step 21280, loss = 0.60 (11944.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:34.312349: step 21290, loss = 0.59 (11765.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:34.741237: step 21300, loss = 0.47 (11903.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:35.330320: step 21310, loss = 0.53 (12080.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:35.757968: step 21320, loss = 0.58 (11986.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:36.201106: step 21330, loss = 0.52 (12036.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:36.631792: step 21340, loss = 0.55 (11903.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:37.061118: step 21350, loss = 0.53 (11834.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:37.488883: step 21360, loss = 0.67 (12051.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:37.917551: step 21370, loss = 0.80 (11901.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:38.343457: step 21380, loss = 0.64 (11966.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:38.771122: step 21390, loss = 0.62 (11913.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:39.200638: step 21400, loss = 0.60 (11904.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:39.791073: step 21410, loss = 0.64 (11892.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:40.218764: step 21420, loss = 0.61 (12040.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:40.647393: step 21430, loss = 0.55 (12000.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:41.076379: step 21440, loss = 0.53 (11672.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:41.507336: step 21450, loss = 0.52 (11898.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:41.936881: step 21460, loss = 0.52 (11986.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:42.368091: step 21470, loss = 0.67 (11842.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:42.795702: step 21480, loss = 0.52 (11960.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:43.223724: step 21490, loss = 0.56 (12260.4 examples/sec; 0.010 sec/batch)
2017-07-19 19:29:43.651681: step 21500, loss = 0.53 (11912.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:44.252684: step 21510, loss = 0.60 (12059.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:44.680878: step 21520, loss = 0.68 (11977.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:45.109910: step 21530, loss = 0.62 (12019.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:45.540142: step 21540, loss = 0.66 (11863.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:45.969986: step 21550, loss = 0.49 (11988.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:46.398055: step 21560, loss = 0.65 (11957.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:46.826138: step 21570, loss = 0.60 (12009.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:47.254261: step 21580, loss = 0.58 (11939.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:47.682166: step 21590, loss = 0.52 (11969.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:48.108977: step 21600, loss = 0.48 (12000.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:48.696359: step 21610, loss = 0.63 (11961.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:49.123917: step 21620, loss = 0.52 (12059.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:49.551401: step 21630, loss = 0.66 (11998.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:49.978161: step 21640, loss = 0.79 (11980.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:50.407944: step 21650, loss = 0.69 (11939.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:50.837760: step 21660, loss = 0.58 (12001.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:51.271832: step 21670, loss = 0.62 (12108.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:51.699005: step 21680, loss = 0.70 (12087.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:52.127512: step 21690, loss = 0.71 (11877.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:52.563325: step 21700, loss = 0.49 (12012.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:53.161570: step 21710, loss = 0.66 (12033.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:53.591241: step 21720, loss = 0.56 (11907.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:54.021086: step 21730, loss = 0.62 (11813.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:54.449010: step 21740, loss = 0.44 (12028.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:54.886604: step 21750, loss = 0.64 (11706.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:55.315785: step 21760, loss = 0.63 (12009.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:55.744941: step 21770, loss = 0.54 (12050.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:56.172203: step 21780, loss = 0.56 (11917.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:56.601020: step 21790, loss = 0.81 (11963.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:57.029587: step 21800, loss = 0.67 (12113.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:57.615649: step 21810, loss = 0.57 (11386.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:58.053117: step 21820, loss = 0.69 (11837.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:58.483709: step 21830, loss = 0.61 (11903.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:58.915947: step 21840, loss = 0.55 (12064.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:59.344772: step 21850, loss = 0.51 (12060.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:29:59.774275: step 21860, loss = 0.55 (12024.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:00.203930: step 21870, loss = 0.41 (12059.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:00.633226: step 21880, loss = 0.52 (11854.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:01.062565: step 21890, loss = 0.43 (12109.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:01.491978: step 21900, loss = 0.49 (12071.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:02.094420: step 21910, loss = 0.71 (11926.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:02.523402: step 21920, loss = 0.57 (12046.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:02.951483: step 21930, loss = 0.61 (12113.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:03.381604: step 21940, loss = 0.66 (12065.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:03.814992: step 21950, loss = 0.75 (12067.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:04.250340: step 21960, loss = 0.48 (11860.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:04.679881: step 21970, loss = 0.60 (11882.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:05.107616: step 21980, loss = 0.51 (11987.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:05.544133: step 21990, loss = 0.54 (12014.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:05.974101: step 22000, loss = 0.45 (12022.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:06.706391: step 22010, loss = 0.57 (11828.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:07.135344: step 22020, loss = 0.61 (12094.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:07.562374: step 22030, loss = 0.63 (11890.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:07.992821: step 22040, loss = 0.63 (12005.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:08.420455: step 22050, loss = 0.60 (12165.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:08.848927: step 22060, loss = 0.45 (12002.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:09.278430: step 22070, loss = 0.60 (11980.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:09.708616: step 22080, loss = 0.56 (12054.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:10.136328: step 22090, loss = 0.65 (12087.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:10.565843: step 22100, loss = 0.84 (12003.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:11.155498: step 22110, loss = 0.56 (11975.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:11.582109: step 22120, loss = 0.58 (12021.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:12.016971: step 22130, loss = 0.52 (11894.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:12.445008: step 22140, loss = 0.48 (11995.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:12.874864: step 22150, loss = 0.48 (11897.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:13.303393: step 22160, loss = 0.66 (12052.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:13.732982: step 22170, loss = 0.60 (12135.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:14.161259: step 22180, loss = 0.56 (12044.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:14.587430: step 22190, loss = 0.58 (12086.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:15.012753: step 22200, loss = 0.60 (11987.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:15.601759: step 22210, loss = 0.69 (11998.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:16.029166: step 22220, loss = 0.67 (11887.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:16.457544: step 22230, loss = 0.68 (11843.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:16.884843: step 22240, loss = 0.50 (12090.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:17.311770: step 22250, loss = 0.65 (11901.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:17.742591: step 22260, loss = 0.62 (11949.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:18.174300: step 22270, loss = 0.53 (11966.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:18.606455: step 22280, loss = 0.43 (12085.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:19.036623: step 22290, loss = 0.73 (11941.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:19.462783: step 22300, loss = 0.54 (11999.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:20.057212: step 22310, loss = 0.56 (11782.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:20.486488: step 22320, loss = 0.57 (11964.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:20.922237: step 22330, loss = 0.58 (12107.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:21.352335: step 22340, loss = 0.50 (11940.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:21.780745: step 22350, loss = 0.53 (11972.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:22.209434: step 22360, loss = 0.62 (11986.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:22.638165: step 22370, loss = 0.53 (11857.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:23.068954: step 22380, loss = 0.53 (12083.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:23.498486: step 22390, loss = 0.56 (12047.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:23.927694: step 22400, loss = 0.71 (12044.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:24.521244: step 22410, loss = 0.47 (11814.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:24.947585: step 22420, loss = 0.56 (12051.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:25.375037: step 22430, loss = 0.52 (12006.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:25.801726: step 22440, loss = 0.64 (12162.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:26.229165: step 22450, loss = 0.49 (11866.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:26.656843: step 22460, loss = 0.56 (12039.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:27.086243: step 22470, loss = 0.58 (11914.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:27.513426: step 22480, loss = 0.62 (12010.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:27.941964: step 22490, loss = 0.56 (12099.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:28.368812: step 22500, loss = 0.72 (12019.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:28.957712: step 22510, loss = 0.70 (11802.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:29.394281: step 22520, loss = 0.64 (12007.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:29.823334: step 22530, loss = 0.43 (12018.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:30.253643: step 22540, loss = 0.60 (11755.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:30.684628: step 22550, loss = 0.56 (11875.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:31.115846: step 22560, loss = 0.63 (11890.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:31.547722: step 22570, loss = 0.42 (11836.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:31.974420: step 22580, loss = 0.72 (12043.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:32.402333: step 22590, loss = 0.63 (11957.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:32.829868: step 22600, loss = 0.54 (11895.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:33.423308: step 22610, loss = 0.63 (11767.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:33.851507: step 22620, loss = 0.71 (11919.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:34.291227: step 22630, loss = 0.47 (11876.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:34.719443: step 22640, loss = 0.59 (11955.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:35.147098: step 22650, loss = 0.69 (11907.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:35.575460: step 22660, loss = 0.71 (12114.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:36.009619: step 22670, loss = 0.73 (11981.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:36.438863: step 22680, loss = 0.53 (12079.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:36.868565: step 22690, loss = 0.58 (11897.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:37.297032: step 22700, loss = 0.51 (12011.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:37.900235: step 22710, loss = 0.61 (11728.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:38.328884: step 22720, loss = 0.48 (11852.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:38.761640: step 22730, loss = 0.80 (12032.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:39.194285: step 22740, loss = 0.66 (12117.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:39.624689: step 22750, loss = 0.56 (11735.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:40.054722: step 22760, loss = 0.56 (11913.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:40.482312: step 22770, loss = 0.55 (11958.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:40.917341: step 22780, loss = 0.59 (11995.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:41.347368: step 22790, loss = 0.47 (11705.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:41.777262: step 22800, loss = 0.60 (11790.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:42.370589: step 22810, loss = 0.63 (11784.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:42.799067: step 22820, loss = 0.44 (12084.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:43.225706: step 22830, loss = 0.54 (11941.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:43.654983: step 22840, loss = 0.42 (11867.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:44.082608: step 22850, loss = 0.50 (11984.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:44.512975: step 22860, loss = 0.61 (12096.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:44.948624: step 22870, loss = 0.56 (11759.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:45.378317: step 22880, loss = 0.68 (11942.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:45.804371: step 22890, loss = 0.59 (12135.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:46.235645: step 22900, loss = 0.60 (11789.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:46.845416: step 22910, loss = 0.85 (11892.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:47.271059: step 22920, loss = 0.60 (12057.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:47.708919: step 22930, loss = 0.57 (11761.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:48.142000: step 22940, loss = 0.59 (11996.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:48.568549: step 22950, loss = 0.58 (12057.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:48.998249: step 22960, loss = 0.56 (12003.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:49.427981: step 22970, loss = 0.56 (12160.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:49.860639: step 22980, loss = 0.47 (12063.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:50.287701: step 22990, loss = 0.64 (11823.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:50.717188: step 23000, loss = 0.61 (11926.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:51.445391: step 23010, loss = 0.54 (11908.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:51.873003: step 23020, loss = 0.48 (11949.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:52.301145: step 23030, loss = 0.77 (11923.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:52.729109: step 23040, loss = 0.53 (11902.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:53.155994: step 23050, loss = 0.54 (11976.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:53.583796: step 23060, loss = 0.55 (11814.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:54.012605: step 23070, loss = 0.52 (11980.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:54.441051: step 23080, loss = 0.64 (12061.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:54.868964: step 23090, loss = 0.62 (11995.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:55.297138: step 23100, loss = 0.68 (11817.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:55.883939: step 23110, loss = 0.53 (12048.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:56.314333: step 23120, loss = 0.52 (11964.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:56.744118: step 23130, loss = 0.55 (12029.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:57.171457: step 23140, loss = 0.56 (12091.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:57.611268: step 23150, loss = 0.46 (11850.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:58.039564: step 23160, loss = 0.57 (11932.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:58.466594: step 23170, loss = 0.59 (11846.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:58.895113: step 23180, loss = 0.85 (11857.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:59.321918: step 23190, loss = 0.55 (11967.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:30:59.749324: step 23200, loss = 0.57 (12043.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:00.336300: step 23210, loss = 0.57 (12063.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:00.768618: step 23220, loss = 0.61 (12150.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:01.196035: step 23230, loss = 0.68 (11967.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:01.621954: step 23240, loss = 0.65 (12030.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:02.046973: step 23250, loss = 0.59 (12061.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:02.474681: step 23260, loss = 0.65 (12070.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:02.908856: step 23270, loss = 0.53 (12103.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:03.334378: step 23280, loss = 0.60 (12096.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:03.762497: step 23290, loss = 0.63 (12071.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:04.191385: step 23300, loss = 0.52 (12047.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:04.776292: step 23310, loss = 0.66 (12079.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:05.204738: step 23320, loss = 0.74 (12045.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:05.631262: step 23330, loss = 0.75 (11756.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:06.059625: step 23340, loss = 0.64 (12026.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:06.486800: step 23350, loss = 0.42 (11952.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:06.914411: step 23360, loss = 0.56 (11918.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:07.342308: step 23370, loss = 0.46 (12057.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:07.771255: step 23380, loss = 0.57 (11922.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:08.200361: step 23390, loss = 0.70 (11975.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:08.627001: step 23400, loss = 0.63 (12103.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:09.221019: step 23410, loss = 0.49 (11861.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:09.653697: step 23420, loss = 0.69 (12118.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:10.082583: step 23430, loss = 0.72 (12032.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:10.510250: step 23440, loss = 0.56 (12058.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:10.937777: step 23450, loss = 0.67 (11943.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:11.365011: step 23460, loss = 0.57 (11859.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:11.793309: step 23470, loss = 0.72 (12001.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:12.219705: step 23480, loss = 0.44 (12039.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:12.645609: step 23490, loss = 0.52 (11979.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:13.073421: step 23500, loss = 0.52 (12053.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:13.668078: step 23510, loss = 0.52 (11967.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:14.100602: step 23520, loss = 0.67 (11978.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:14.539941: step 23530, loss = 0.59 (12076.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:14.968154: step 23540, loss = 0.47 (12109.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:15.397591: step 23550, loss = 0.61 (11777.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:15.825884: step 23560, loss = 0.61 (11849.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:16.258168: step 23570, loss = 0.46 (11971.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:16.688509: step 23580, loss = 0.42 (11739.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:17.122157: step 23590, loss = 0.55 (12109.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:17.553322: step 23600, loss = 0.59 (12003.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:18.163702: step 23610, loss = 0.66 (12045.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:18.590597: step 23620, loss = 0.61 (12034.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:19.023620: step 23630, loss = 0.49 (10633.0 examples/sec; 0.012 sec/batch)
2017-07-19 19:31:19.455188: step 23640, loss = 0.52 (11958.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:19.883396: step 23650, loss = 0.73 (11946.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:20.312914: step 23660, loss = 0.66 (11868.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:20.741815: step 23670, loss = 0.61 (11990.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:21.170292: step 23680, loss = 0.59 (12108.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:21.601057: step 23690, loss = 0.68 (12003.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:22.030160: step 23700, loss = 0.56 (12057.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:22.617660: step 23710, loss = 0.58 (12073.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:23.046795: step 23720, loss = 0.46 (11915.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:23.477087: step 23730, loss = 0.62 (11932.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:23.906681: step 23740, loss = 0.62 (12066.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:24.337863: step 23750, loss = 0.77 (12088.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:24.766805: step 23760, loss = 0.65 (12092.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:25.195349: step 23770, loss = 0.50 (12092.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:25.621944: step 23780, loss = 0.54 (11903.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:26.049956: step 23790, loss = 0.50 (11956.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:26.476725: step 23800, loss = 0.64 (11974.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:27.070538: step 23810, loss = 0.76 (11698.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:27.499545: step 23820, loss = 0.66 (12067.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:27.927467: step 23830, loss = 0.66 (11751.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:28.360302: step 23840, loss = 0.47 (11890.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:28.790452: step 23850, loss = 0.70 (11869.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:29.219938: step 23860, loss = 0.54 (12029.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:29.648298: step 23870, loss = 0.49 (12067.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:30.076348: step 23880, loss = 0.67 (11996.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:30.503065: step 23890, loss = 0.52 (12013.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:30.930708: step 23900, loss = 0.41 (11953.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:31.530894: step 23910, loss = 0.61 (11899.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:31.959782: step 23920, loss = 0.55 (11939.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:32.388428: step 23930, loss = 0.74 (11958.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:32.814098: step 23940, loss = 0.67 (12067.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:33.240790: step 23950, loss = 0.57 (11937.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:33.668268: step 23960, loss = 0.48 (11855.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:34.097841: step 23970, loss = 0.57 (11995.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:34.528171: step 23980, loss = 0.52 (11921.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:34.956571: step 23990, loss = 0.51 (11942.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:35.386429: step 24000, loss = 0.55 (12021.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:36.117147: step 24010, loss = 0.49 (11956.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:36.546475: step 24020, loss = 0.62 (11843.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:36.976413: step 24030, loss = 0.52 (11942.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:37.405600: step 24040, loss = 0.66 (12015.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:37.840425: step 24050, loss = 0.57 (11883.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:38.276572: step 24060, loss = 0.54 (11673.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:38.710433: step 24070, loss = 0.66 (11963.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:39.137908: step 24080, loss = 0.63 (11868.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:39.570562: step 24090, loss = 0.58 (11890.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:40.002414: step 24100, loss = 0.57 (12031.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:40.597371: step 24110, loss = 0.49 (12009.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:41.024416: step 24120, loss = 0.67 (12021.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:41.450610: step 24130, loss = 0.63 (12048.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:41.878997: step 24140, loss = 0.63 (11993.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:42.306415: step 24150, loss = 0.60 (12051.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:42.734242: step 24160, loss = 0.60 (12133.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:43.165308: step 24170, loss = 0.49 (12098.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:43.593527: step 24180, loss = 0.61 (12011.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:44.021557: step 24190, loss = 0.56 (11900.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:44.450116: step 24200, loss = 0.49 (11813.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:45.046435: step 24210, loss = 0.69 (11813.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:45.473637: step 24220, loss = 0.61 (11910.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:45.903400: step 24230, loss = 0.61 (12000.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:46.330015: step 24240, loss = 0.52 (11950.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:46.759573: step 24250, loss = 0.65 (12074.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:47.194683: step 24260, loss = 0.61 (11764.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:47.624180: step 24270, loss = 0.50 (11890.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:48.062159: step 24280, loss = 0.45 (11746.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:48.491438: step 24290, loss = 0.55 (12000.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:48.922148: step 24300, loss = 0.59 (12031.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:49.519592: step 24310, loss = 0.54 (12050.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:49.948191: step 24320, loss = 0.50 (11875.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:50.378733: step 24330, loss = 0.59 (12009.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:50.808687: step 24340, loss = 0.52 (11691.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:51.237969: step 24350, loss = 0.67 (11995.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:51.666326: step 24360, loss = 0.49 (11766.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:52.094983: step 24370, loss = 0.47 (12044.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:52.519542: step 24380, loss = 0.80 (12144.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:52.953206: step 24390, loss = 0.60 (11872.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:53.385972: step 24400, loss = 0.53 (10526.6 examples/sec; 0.012 sec/batch)
2017-07-19 19:31:53.992094: step 24410, loss = 0.55 (12044.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:54.418638: step 24420, loss = 0.71 (12097.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:54.845860: step 24430, loss = 0.52 (12080.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:55.272449: step 24440, loss = 0.56 (11952.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:55.701048: step 24450, loss = 0.56 (11978.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:56.128441: step 24460, loss = 0.52 (12070.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:56.555527: step 24470, loss = 0.71 (12133.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:56.985192: step 24480, loss = 0.54 (12029.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:57.416398: step 24490, loss = 0.57 (11900.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:57.846170: step 24500, loss = 0.56 (12026.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:58.446643: step 24510, loss = 0.58 (11745.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:58.874997: step 24520, loss = 0.50 (11962.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:59.301034: step 24530, loss = 0.57 (11979.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:31:59.730626: step 24540, loss = 0.44 (12048.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:00.158670: step 24550, loss = 0.72 (11944.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:00.585011: step 24560, loss = 0.51 (11928.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:01.012833: step 24570, loss = 0.54 (11851.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:01.440891: step 24580, loss = 0.53 (12070.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:01.868343: step 24590, loss = 0.50 (12303.8 examples/sec; 0.010 sec/batch)
2017-07-19 19:32:02.295971: step 24600, loss = 0.68 (12097.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:02.881365: step 24610, loss = 0.53 (12126.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:03.309457: step 24620, loss = 0.64 (11977.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:03.736501: step 24630, loss = 0.53 (12055.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:04.162814: step 24640, loss = 0.61 (12034.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:04.592323: step 24650, loss = 0.48 (12078.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:05.021688: step 24660, loss = 0.44 (11946.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:05.451041: step 24670, loss = 0.49 (11485.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:05.881160: step 24680, loss = 0.56 (12208.3 examples/sec; 0.010 sec/batch)
2017-07-19 19:32:06.308473: step 24690, loss = 0.50 (11996.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:06.738745: step 24700, loss = 0.58 (11991.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:07.329739: step 24710, loss = 0.58 (12104.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:07.759741: step 24720, loss = 0.56 (12196.9 examples/sec; 0.010 sec/batch)
2017-07-19 19:32:08.194327: step 24730, loss = 0.64 (11855.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:08.622681: step 24740, loss = 0.53 (11997.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:09.051171: step 24750, loss = 0.56 (11989.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:09.480854: step 24760, loss = 0.45 (11940.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:09.908823: step 24770, loss = 0.59 (11907.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:10.344323: step 24780, loss = 0.62 (11855.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:10.771653: step 24790, loss = 0.52 (11936.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:11.197564: step 24800, loss = 0.61 (12182.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:11.790781: step 24810, loss = 0.59 (11914.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:12.219525: step 24820, loss = 0.65 (11990.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:12.648936: step 24830, loss = 0.65 (11888.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:13.077395: step 24840, loss = 0.49 (12016.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:13.507691: step 24850, loss = 0.52 (11790.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:13.951045: step 24860, loss = 0.62 (11779.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:14.379114: step 24870, loss = 0.59 (11947.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:14.808052: step 24880, loss = 0.48 (11918.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:15.237912: step 24890, loss = 0.55 (12005.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:15.669189: step 24900, loss = 0.63 (11790.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:16.259606: step 24910, loss = 0.69 (12043.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:16.688351: step 24920, loss = 0.67 (11957.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:17.117103: step 24930, loss = 0.49 (11882.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:17.546353: step 24940, loss = 0.48 (11953.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:17.973229: step 24950, loss = 0.74 (12121.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:18.408209: step 24960, loss = 0.47 (12025.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:18.836081: step 24970, loss = 0.74 (11958.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:19.264482: step 24980, loss = 0.47 (12099.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:19.693331: step 24990, loss = 0.42 (12025.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:20.122983: step 25000, loss = 0.66 (11990.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:20.875112: step 25010, loss = 0.57 (11870.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:21.302448: step 25020, loss = 0.64 (12071.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:21.729608: step 25030, loss = 0.46 (11976.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:22.156998: step 25040, loss = 0.60 (12043.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:22.585903: step 25050, loss = 0.58 (11996.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:23.012804: step 25060, loss = 0.62 (11766.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:23.441393: step 25070, loss = 0.57 (11850.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:23.867056: step 25080, loss = 0.48 (12050.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:24.293977: step 25090, loss = 0.48 (12029.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:24.726020: step 25100, loss = 0.76 (11971.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:25.308734: step 25110, loss = 0.57 (11998.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:25.742116: step 25120, loss = 0.59 (12070.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:26.171088: step 25130, loss = 0.62 (11920.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:26.604299: step 25140, loss = 0.64 (11970.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:27.032818: step 25150, loss = 0.60 (12029.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:27.458762: step 25160, loss = 0.55 (12170.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:27.885926: step 25170, loss = 0.53 (11940.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:28.313099: step 25180, loss = 0.60 (12029.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:28.744291: step 25190, loss = 0.57 (10961.1 examples/sec; 0.012 sec/batch)
2017-07-19 19:32:29.183347: step 25200, loss = 0.54 (11972.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:29.777108: step 25210, loss = 0.59 (11950.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:30.204454: step 25220, loss = 0.57 (12002.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:30.631637: step 25230, loss = 0.51 (12074.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:31.063100: step 25240, loss = 0.63 (11851.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:31.489273: step 25250, loss = 0.59 (12005.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:31.916550: step 25260, loss = 0.54 (11988.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:32.342946: step 25270, loss = 0.70 (12081.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:32.772204: step 25280, loss = 0.58 (11790.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:33.199482: step 25290, loss = 0.58 (12084.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:33.625433: step 25300, loss = 0.61 (12030.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:34.212686: step 25310, loss = 0.66 (11994.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:34.642627: step 25320, loss = 0.64 (11808.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:35.071182: step 25330, loss = 0.54 (12041.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:35.498895: step 25340, loss = 0.53 (12038.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:35.929542: step 25350, loss = 0.73 (12095.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:36.359163: step 25360, loss = 0.56 (11860.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:36.785734: step 25370, loss = 0.53 (12004.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:37.214467: step 25380, loss = 0.66 (11877.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:37.644033: step 25390, loss = 0.54 (11731.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:38.078471: step 25400, loss = 0.48 (11914.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:38.676640: step 25410, loss = 0.61 (12023.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:39.105576: step 25420, loss = 0.57 (11993.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:39.540022: step 25430, loss = 0.59 (11953.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:39.971026: step 25440, loss = 0.48 (11857.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:40.401228: step 25450, loss = 0.56 (11967.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:40.833368: step 25460, loss = 0.59 (11732.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:41.265558: step 25470, loss = 0.61 (11916.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:41.694887: step 25480, loss = 0.47 (11955.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:42.128371: step 25490, loss = 0.59 (11959.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:42.556455: step 25500, loss = 0.47 (11950.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:43.157913: step 25510, loss = 0.45 (12124.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:43.586647: step 25520, loss = 0.56 (12000.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:44.014407: step 25530, loss = 0.51 (11822.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:44.440750: step 25540, loss = 0.50 (12117.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:44.867632: step 25550, loss = 0.63 (12077.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:45.300983: step 25560, loss = 0.58 (11940.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:45.726978: step 25570, loss = 0.64 (12071.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:46.152587: step 25580, loss = 0.69 (12021.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:46.581206: step 25590, loss = 0.58 (11948.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:47.011543: step 25600, loss = 0.72 (11941.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:47.594980: step 25610, loss = 0.58 (12101.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:48.028500: step 25620, loss = 0.54 (12072.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:48.465538: step 25630, loss = 0.56 (11940.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:48.894191: step 25640, loss = 0.48 (12006.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:49.319912: step 25650, loss = 0.59 (11970.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:49.749082: step 25660, loss = 0.56 (11850.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:50.179221: step 25670, loss = 0.60 (11957.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:50.607051: step 25680, loss = 0.52 (12083.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:51.035885: step 25690, loss = 0.65 (11969.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:51.462592: step 25700, loss = 0.65 (12026.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:52.045358: step 25710, loss = 0.55 (11984.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:52.472794: step 25720, loss = 0.52 (12080.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:52.899915: step 25730, loss = 0.64 (11989.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:53.325916: step 25740, loss = 0.58 (12083.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:53.752282: step 25750, loss = 0.51 (11942.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:54.181197: step 25760, loss = 0.51 (11636.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:54.608360: step 25770, loss = 0.64 (12113.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:55.035441: step 25780, loss = 0.52 (12178.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:55.461623: step 25790, loss = 0.63 (12030.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:55.890152: step 25800, loss = 0.56 (11852.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:56.470812: step 25810, loss = 0.68 (11961.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:56.896836: step 25820, loss = 0.55 (12054.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:57.322818: step 25830, loss = 0.57 (12109.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:57.751069: step 25840, loss = 0.63 (12046.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:58.177627: step 25850, loss = 0.49 (11878.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:58.608936: step 25860, loss = 0.63 (11946.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:59.036357: step 25870, loss = 0.61 (12023.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:59.465122: step 25880, loss = 0.53 (11934.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:32:59.892144: step 25890, loss = 0.51 (12166.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:00.320022: step 25900, loss = 0.48 (12066.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:00.905170: step 25910, loss = 0.62 (12000.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:01.332843: step 25920, loss = 0.51 (12087.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:01.759390: step 25930, loss = 0.42 (11981.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:02.187509: step 25940, loss = 0.50 (12122.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:02.614621: step 25950, loss = 0.65 (11950.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:03.039950: step 25960, loss = 0.66 (12094.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:03.468236: step 25970, loss = 0.51 (11993.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:03.897830: step 25980, loss = 0.62 (12047.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:04.324870: step 25990, loss = 0.54 (11964.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:04.754414: step 26000, loss = 0.66 (11903.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:05.468261: step 26010, loss = 0.48 (11965.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:05.897681: step 26020, loss = 0.50 (12099.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:06.323054: step 26030, loss = 0.64 (11975.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:06.749199: step 26040, loss = 0.65 (11975.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:07.178705: step 26050, loss = 0.69 (11981.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:07.606614: step 26060, loss = 0.68 (11814.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:08.036981: step 26070, loss = 0.71 (11936.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:08.468704: step 26080, loss = 0.57 (11762.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:08.899460: step 26090, loss = 0.59 (11836.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:09.326598: step 26100, loss = 0.44 (11938.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:09.911819: step 26110, loss = 0.52 (12178.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:10.341484: step 26120, loss = 0.57 (11819.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:10.775067: step 26130, loss = 0.58 (12016.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:11.206208: step 26140, loss = 0.68 (12004.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:11.632113: step 26150, loss = 0.44 (11946.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:12.061848: step 26160, loss = 0.62 (11954.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:12.489392: step 26170, loss = 0.65 (11934.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:12.917934: step 26180, loss = 0.53 (11983.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:13.345885: step 26190, loss = 0.62 (11971.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:13.772131: step 26200, loss = 0.55 (12071.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:14.377244: step 26210, loss = 0.60 (12052.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:14.805731: step 26220, loss = 0.47 (11919.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:15.235231: step 26230, loss = 0.71 (11921.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:15.661889: step 26240, loss = 0.62 (11931.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:16.089279: step 26250, loss = 0.56 (11798.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:16.518619: step 26260, loss = 0.43 (11931.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:16.944495: step 26270, loss = 0.60 (12142.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:17.368714: step 26280, loss = 0.57 (12191.6 examples/sec; 0.010 sec/batch)
2017-07-19 19:33:17.797239: step 26290, loss = 0.61 (11794.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:18.227935: step 26300, loss = 0.52 (12021.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:18.826900: step 26310, loss = 0.60 (12013.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:19.256057: step 26320, loss = 0.49 (11847.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:19.689719: step 26330, loss = 0.55 (12108.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:20.124106: step 26340, loss = 0.64 (12136.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:20.551238: step 26350, loss = 0.64 (11996.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:20.979262: step 26360, loss = 0.48 (11881.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:21.412163: step 26370, loss = 0.55 (11979.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:21.843798: step 26380, loss = 0.60 (12034.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:22.276886: step 26390, loss = 0.50 (10672.8 examples/sec; 0.012 sec/batch)
2017-07-19 19:33:22.703603: step 26400, loss = 0.49 (11944.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:23.289756: step 26410, loss = 0.61 (11919.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:23.715413: step 26420, loss = 0.59 (12082.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:24.140723: step 26430, loss = 0.44 (12068.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:24.566557: step 26440, loss = 0.47 (11978.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:24.992625: step 26450, loss = 0.60 (12111.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:25.421462: step 26460, loss = 0.60 (12087.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:25.847494: step 26470, loss = 0.77 (11990.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:26.274827: step 26480, loss = 0.57 (11962.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:26.703409: step 26490, loss = 0.65 (11874.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:27.130774: step 26500, loss = 0.58 (11976.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:27.716884: step 26510, loss = 0.53 (11878.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:28.146566: step 26520, loss = 0.57 (12032.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:28.574627: step 26530, loss = 0.72 (12018.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:29.002412: step 26540, loss = 0.59 (12022.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:29.429968: step 26550, loss = 0.47 (11975.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:29.857368: step 26560, loss = 0.75 (11989.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:30.284934: step 26570, loss = 0.68 (12235.3 examples/sec; 0.010 sec/batch)
2017-07-19 19:33:30.724196: step 26580, loss = 0.51 (12023.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:31.150560: step 26590, loss = 0.61 (12075.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:31.578009: step 26600, loss = 0.72 (11950.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:32.166462: step 26610, loss = 0.51 (11888.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:32.594719: step 26620, loss = 0.70 (12165.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:33.022451: step 26630, loss = 0.57 (12023.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:33.449228: step 26640, loss = 0.54 (12063.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:33.876674: step 26650, loss = 0.50 (11861.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:34.302954: step 26660, loss = 0.76 (12012.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:34.731061: step 26670, loss = 0.90 (11957.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:35.161601: step 26680, loss = 0.57 (11969.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:35.587031: step 26690, loss = 0.72 (12148.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:36.012401: step 26700, loss = 0.53 (12062.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:36.615379: step 26710, loss = 0.52 (11936.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:37.045996: step 26720, loss = 0.56 (11856.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:37.477015: step 26730, loss = 0.45 (11784.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:37.903648: step 26740, loss = 0.57 (11994.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:38.338433: step 26750, loss = 0.60 (12029.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:38.767418: step 26760, loss = 0.60 (11967.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:39.195774: step 26770, loss = 0.51 (11931.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:39.624466: step 26780, loss = 0.57 (11894.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:40.054955: step 26790, loss = 0.52 (11898.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:40.482148: step 26800, loss = 0.56 (12091.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:41.078150: step 26810, loss = 0.47 (12042.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:41.503826: step 26820, loss = 0.48 (11971.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:41.936303: step 26830, loss = 0.48 (11818.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:42.365715: step 26840, loss = 0.56 (12037.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:42.796384: step 26850, loss = 0.59 (12074.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:43.225584: step 26860, loss = 0.58 (11568.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:43.654558: step 26870, loss = 0.48 (11996.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:44.084124: step 26880, loss = 0.56 (11979.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:44.513590: step 26890, loss = 0.52 (11830.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:44.943005: step 26900, loss = 0.72 (11886.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:45.548567: step 26910, loss = 0.61 (11963.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:45.978210: step 26920, loss = 0.52 (12065.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:46.405834: step 26930, loss = 0.61 (11985.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:46.842468: step 26940, loss = 0.63 (12002.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:47.271213: step 26950, loss = 0.68 (12052.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:47.700395: step 26960, loss = 0.55 (11983.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:48.129653: step 26970, loss = 0.50 (11940.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:48.560886: step 26980, loss = 0.53 (11861.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:49.001352: step 26990, loss = 0.54 (11856.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:49.432590: step 27000, loss = 0.71 (10809.1 examples/sec; 0.012 sec/batch)
2017-07-19 19:33:50.155130: step 27010, loss = 0.47 (11935.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:50.590699: step 27020, loss = 0.73 (11827.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:51.021294: step 27030, loss = 0.62 (11952.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:51.450745: step 27040, loss = 0.51 (11881.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:51.878072: step 27050, loss = 0.60 (12153.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:52.305138: step 27060, loss = 0.53 (12090.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:52.742950: step 27070, loss = 0.50 (11921.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:53.167719: step 27080, loss = 0.46 (11952.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:53.593979: step 27090, loss = 0.62 (12183.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:54.024271: step 27100, loss = 0.47 (12068.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:54.612738: step 27110, loss = 0.70 (11940.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:55.038179: step 27120, loss = 0.58 (12176.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:55.465581: step 27130, loss = 0.67 (12120.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:55.897463: step 27140, loss = 0.46 (11932.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:56.324552: step 27150, loss = 0.63 (11971.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:56.751994: step 27160, loss = 0.65 (11939.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:57.177747: step 27170, loss = 0.64 (12150.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:57.606827: step 27180, loss = 0.55 (11734.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:58.034522: step 27190, loss = 0.55 (11984.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:58.461532: step 27200, loss = 0.58 (11967.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:59.071489: step 27210, loss = 0.55 (11863.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:59.502975: step 27220, loss = 0.68 (11863.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:33:59.933380: step 27230, loss = 0.62 (12104.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:00.363402: step 27240, loss = 0.59 (11837.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:00.792661: step 27250, loss = 0.59 (11890.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:01.221395: step 27260, loss = 0.43 (11875.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:01.649470: step 27270, loss = 0.59 (11951.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:02.084490: step 27280, loss = 0.52 (12007.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:02.518688: step 27290, loss = 0.69 (12044.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:02.957471: step 27300, loss = 0.69 (12029.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:03.553218: step 27310, loss = 0.66 (11816.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:03.983661: step 27320, loss = 0.60 (11994.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:04.413371: step 27330, loss = 0.49 (11914.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:04.841596: step 27340, loss = 0.67 (12176.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:05.275904: step 27350, loss = 0.49 (11906.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:05.703269: step 27360, loss = 0.47 (11967.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:06.130578: step 27370, loss = 0.58 (11986.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:06.560867: step 27380, loss = 0.54 (12045.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:06.989679: step 27390, loss = 0.61 (12110.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:07.420078: step 27400, loss = 0.61 (11836.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:08.019412: step 27410, loss = 0.61 (10638.4 examples/sec; 0.012 sec/batch)
2017-07-19 19:34:08.448342: step 27420, loss = 0.53 (11869.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:08.877549: step 27430, loss = 0.60 (11650.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:09.307650: step 27440, loss = 0.54 (11955.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:09.739238: step 27450, loss = 0.67 (11930.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:10.169657: step 27460, loss = 0.55 (11820.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:10.606027: step 27470, loss = 0.66 (11891.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:11.034315: step 27480, loss = 0.69 (12159.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:11.463300: step 27490, loss = 0.74 (11929.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:11.889903: step 27500, loss = 0.47 (12137.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:12.492540: step 27510, loss = 0.57 (12053.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:12.920968: step 27520, loss = 0.55 (11926.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:13.349669: step 27530, loss = 0.56 (12102.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:13.783319: step 27540, loss = 0.56 (12051.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:14.212900: step 27550, loss = 0.71 (11956.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:14.642308: step 27560, loss = 0.49 (11615.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:15.068816: step 27570, loss = 0.41 (11964.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:15.496814: step 27580, loss = 0.50 (12031.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:15.921770: step 27590, loss = 0.51 (12120.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:16.349332: step 27600, loss = 0.53 (12152.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:16.934995: step 27610, loss = 0.56 (11958.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:17.362464: step 27620, loss = 0.47 (12012.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:17.788923: step 27630, loss = 0.66 (12109.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:18.217867: step 27640, loss = 0.57 (11915.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:18.646949: step 27650, loss = 0.59 (11913.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:19.075795: step 27660, loss = 0.53 (11903.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:19.502088: step 27670, loss = 0.62 (12095.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:19.934883: step 27680, loss = 0.52 (12097.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:20.361933: step 27690, loss = 0.62 (12014.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:20.791375: step 27700, loss = 0.72 (12006.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:21.388531: step 27710, loss = 0.50 (11969.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:21.818298: step 27720, loss = 0.56 (11979.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:22.245953: step 27730, loss = 0.77 (12050.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:22.673799: step 27740, loss = 0.49 (11972.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:23.106094: step 27750, loss = 0.55 (12034.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:23.534553: step 27760, loss = 0.49 (12002.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:23.963392: step 27770, loss = 0.53 (11926.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:24.391707: step 27780, loss = 0.47 (12088.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:24.825983: step 27790, loss = 0.52 (10609.4 examples/sec; 0.012 sec/batch)
2017-07-19 19:34:25.254179: step 27800, loss = 0.48 (12158.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:25.845161: step 27810, loss = 0.51 (12093.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:26.273305: step 27820, loss = 0.61 (12051.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:26.700831: step 27830, loss = 0.49 (12060.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:27.130525: step 27840, loss = 0.58 (11805.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:27.557195: step 27850, loss = 0.48 (11977.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:27.985870: step 27860, loss = 0.57 (12035.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:28.414344: step 27870, loss = 0.49 (11998.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:28.842376: step 27880, loss = 0.47 (12009.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:29.269828: step 27890, loss = 0.53 (12051.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:29.699397: step 27900, loss = 0.48 (12206.6 examples/sec; 0.010 sec/batch)
2017-07-19 19:34:30.291571: step 27910, loss = 0.45 (12027.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:30.723470: step 27920, loss = 0.53 (11978.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:31.154031: step 27930, loss = 0.56 (12045.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:31.579730: step 27940, loss = 0.53 (12084.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:32.008472: step 27950, loss = 0.46 (12122.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:32.434382: step 27960, loss = 0.53 (11674.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:32.863811: step 27970, loss = 0.54 (12192.9 examples/sec; 0.010 sec/batch)
2017-07-19 19:34:33.290100: step 27980, loss = 0.67 (12044.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:33.716425: step 27990, loss = 0.55 (11985.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:34.142870: step 28000, loss = 0.52 (11940.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:34.848917: step 28010, loss = 0.48 (12002.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:35.279494: step 28020, loss = 0.57 (12033.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:35.705997: step 28030, loss = 0.63 (12009.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:36.136016: step 28040, loss = 0.63 (12019.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:36.562215: step 28050, loss = 0.46 (11874.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:36.989998: step 28060, loss = 0.61 (11799.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:37.416424: step 28070, loss = 0.51 (12085.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:37.844273: step 28080, loss = 0.58 (12017.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:38.283130: step 28090, loss = 0.55 (12124.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:38.713205: step 28100, loss = 0.49 (11800.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:39.322387: step 28110, loss = 0.52 (11979.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:39.748997: step 28120, loss = 0.51 (12068.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:40.175445: step 28130, loss = 0.57 (12068.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:40.602456: step 28140, loss = 0.50 (11996.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:41.033204: step 28150, loss = 0.56 (11913.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:41.463900: step 28160, loss = 0.56 (11894.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:41.895743: step 28170, loss = 0.72 (11730.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:42.327581: step 28180, loss = 0.49 (11903.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:42.756113: step 28190, loss = 0.63 (11828.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:43.183365: step 28200, loss = 0.58 (12047.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:43.771876: step 28210, loss = 0.55 (11980.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:44.204584: step 28220, loss = 0.50 (10561.0 examples/sec; 0.012 sec/batch)
2017-07-19 19:34:44.630305: step 28230, loss = 0.56 (11973.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:45.058883: step 28240, loss = 0.41 (11930.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:45.484024: step 28250, loss = 0.58 (12112.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:45.920665: step 28260, loss = 0.60 (11885.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:46.346959: step 28270, loss = 0.61 (12084.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:46.774452: step 28280, loss = 0.65 (12142.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:47.204489: step 28290, loss = 0.58 (12085.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:47.630850: step 28300, loss = 0.70 (11977.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:48.215873: step 28310, loss = 0.52 (12075.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:48.642551: step 28320, loss = 0.55 (11991.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:49.069840: step 28330, loss = 0.52 (11938.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:49.497147: step 28340, loss = 0.75 (11902.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:49.924392: step 28350, loss = 0.40 (12085.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:50.353281: step 28360, loss = 0.53 (11880.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:50.781248: step 28370, loss = 0.59 (12030.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:51.208806: step 28380, loss = 0.55 (11948.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:51.634089: step 28390, loss = 0.79 (11868.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:52.061082: step 28400, loss = 0.58 (11964.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:52.648218: step 28410, loss = 0.65 (12010.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:53.075819: step 28420, loss = 0.79 (11918.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:53.503637: step 28430, loss = 0.54 (11998.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:53.930827: step 28440, loss = 0.57 (12010.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:54.357970: step 28450, loss = 0.61 (12050.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:54.784726: step 28460, loss = 0.72 (12075.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:55.211221: step 28470, loss = 0.55 (11764.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:55.642337: step 28480, loss = 0.49 (11811.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:56.072818: step 28490, loss = 0.48 (12065.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:56.504566: step 28500, loss = 0.57 (11972.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:57.087184: step 28510, loss = 0.63 (12056.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:57.514686: step 28520, loss = 0.61 (12008.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:57.946921: step 28530, loss = 0.49 (11899.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:58.379613: step 28540, loss = 0.52 (11860.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:58.814024: step 28550, loss = 0.60 (11612.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:59.251076: step 28560, loss = 0.58 (12181.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:34:59.686339: step 28570, loss = 0.44 (11964.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:00.114818: step 28580, loss = 0.59 (12025.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:00.543804: step 28590, loss = 0.60 (11964.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:00.971202: step 28600, loss = 0.61 (12105.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:01.560951: step 28610, loss = 0.50 (12072.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:01.989944: step 28620, loss = 0.65 (12038.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:02.414952: step 28630, loss = 0.62 (12073.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:02.847506: step 28640, loss = 0.61 (12153.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:03.276811: step 28650, loss = 0.46 (11934.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:03.710515: step 28660, loss = 0.50 (11956.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:04.138492: step 28670, loss = 0.56 (11972.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:04.575333: step 28680, loss = 0.47 (11968.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:05.002340: step 28690, loss = 0.63 (12170.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:05.430622: step 28700, loss = 0.53 (11857.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:06.031342: step 28710, loss = 0.54 (11983.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:06.457795: step 28720, loss = 0.43 (11892.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:06.884190: step 28730, loss = 0.52 (11936.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:07.308889: step 28740, loss = 0.58 (12122.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:07.736176: step 28750, loss = 0.55 (12073.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:08.173366: step 28760, loss = 0.54 (9875.0 examples/sec; 0.013 sec/batch)
2017-07-19 19:35:08.603537: step 28770, loss = 0.40 (11896.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:09.030821: step 28780, loss = 0.58 (11967.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:09.459443: step 28790, loss = 0.60 (11929.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:09.887297: step 28800, loss = 0.49 (11856.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:10.477321: step 28810, loss = 0.59 (12027.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:10.906044: step 28820, loss = 0.53 (11917.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:11.332182: step 28830, loss = 0.54 (12075.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:11.757173: step 28840, loss = 0.51 (12084.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:12.183478: step 28850, loss = 0.44 (12012.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:12.617327: step 28860, loss = 0.67 (10850.3 examples/sec; 0.012 sec/batch)
2017-07-19 19:35:13.047167: step 28870, loss = 0.59 (12089.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:13.473748: step 28880, loss = 0.69 (12076.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:13.899089: step 28890, loss = 0.68 (12110.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:14.328020: step 28900, loss = 0.63 (11707.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:14.917193: step 28910, loss = 0.67 (12029.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:15.345279: step 28920, loss = 0.62 (11987.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:15.772429: step 28930, loss = 0.47 (11799.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:16.201382: step 28940, loss = 0.51 (11897.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:16.635684: step 28950, loss = 0.50 (11727.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:17.069865: step 28960, loss = 0.64 (11965.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:17.498315: step 28970, loss = 0.61 (11768.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:17.928185: step 28980, loss = 0.48 (11977.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:18.357653: step 28990, loss = 0.61 (11862.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:18.788176: step 29000, loss = 0.46 (11834.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:19.504385: step 29010, loss = 0.53 (11964.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:19.938282: step 29020, loss = 0.62 (12020.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:20.367743: step 29030, loss = 0.68 (11741.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:20.804138: step 29040, loss = 0.54 (11974.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:21.232708: step 29050, loss = 0.52 (11905.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:21.662272: step 29060, loss = 0.58 (11882.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:22.091036: step 29070, loss = 0.51 (12044.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:22.517536: step 29080, loss = 0.62 (11922.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:22.943931: step 29090, loss = 0.49 (12034.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:23.372848: step 29100, loss = 0.49 (11925.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:23.969651: step 29110, loss = 0.42 (12068.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:24.399055: step 29120, loss = 0.62 (11938.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:24.826665: step 29130, loss = 0.71 (11925.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:25.254624: step 29140, loss = 0.54 (11764.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:25.683101: step 29150, loss = 0.47 (11733.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:26.110379: step 29160, loss = 0.64 (11892.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:26.547988: step 29170, loss = 0.63 (12022.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:26.976826: step 29180, loss = 0.53 (11932.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:27.401728: step 29190, loss = 0.68 (12142.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:27.834090: step 29200, loss = 0.46 (11940.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:28.419483: step 29210, loss = 0.58 (12039.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:28.850048: step 29220, loss = 0.41 (11966.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:29.278020: step 29230, loss = 0.67 (11994.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:29.707104: step 29240, loss = 0.57 (11871.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:30.134460: step 29250, loss = 0.49 (12145.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:30.562860: step 29260, loss = 0.49 (11975.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:30.989574: step 29270, loss = 0.71 (11877.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:31.422584: step 29280, loss = 0.51 (10700.5 examples/sec; 0.012 sec/batch)
2017-07-19 19:35:31.850527: step 29290, loss = 0.62 (11877.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:32.277596: step 29300, loss = 0.54 (12005.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:32.862700: step 29310, loss = 0.48 (12055.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:33.290271: step 29320, loss = 0.56 (11938.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:33.717494: step 29330, loss = 0.58 (12063.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:34.144871: step 29340, loss = 0.82 (11943.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:34.574995: step 29350, loss = 0.53 (11874.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:35.007224: step 29360, loss = 0.54 (11899.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:35.433110: step 29370, loss = 0.43 (12052.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:35.859356: step 29380, loss = 0.67 (11966.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:36.288350: step 29390, loss = 0.49 (11964.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:36.716729: step 29400, loss = 0.52 (11907.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:37.301972: step 29410, loss = 0.42 (11849.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:37.742136: step 29420, loss = 0.66 (11606.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:38.167931: step 29430, loss = 0.56 (12003.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:38.595934: step 29440, loss = 0.64 (11970.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:39.038531: step 29450, loss = 0.49 (11922.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:39.463549: step 29460, loss = 0.43 (12065.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:39.896518: step 29470, loss = 0.58 (12115.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:40.326438: step 29480, loss = 0.57 (11365.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:40.753706: step 29490, loss = 0.58 (12065.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:41.179509: step 29500, loss = 0.45 (12041.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:41.765848: step 29510, loss = 0.48 (11875.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:42.193675: step 29520, loss = 0.53 (12068.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:42.618685: step 29530, loss = 0.50 (12056.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:43.043645: step 29540, loss = 0.45 (12090.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:43.469475: step 29550, loss = 0.73 (11993.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:43.903114: step 29560, loss = 0.61 (10661.8 examples/sec; 0.012 sec/batch)
2017-07-19 19:35:44.332371: step 29570, loss = 0.62 (11907.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:44.760139: step 29580, loss = 0.59 (11879.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:45.188643: step 29590, loss = 0.47 (12202.0 examples/sec; 0.010 sec/batch)
2017-07-19 19:35:45.616208: step 29600, loss = 0.60 (11825.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:46.209838: step 29610, loss = 0.54 (12024.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:46.636068: step 29620, loss = 0.54 (11965.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:47.063810: step 29630, loss = 0.51 (11936.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:47.492884: step 29640, loss = 0.60 (12065.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:47.919184: step 29650, loss = 0.50 (12136.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:48.345579: step 29660, loss = 0.49 (11807.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:48.773763: step 29670, loss = 0.68 (11901.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:49.198899: step 29680, loss = 0.47 (12109.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:49.623818: step 29690, loss = 0.64 (12121.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:50.055850: step 29700, loss = 0.53 (12035.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:50.647752: step 29710, loss = 0.58 (11943.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:51.073994: step 29720, loss = 0.57 (11999.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:51.500521: step 29730, loss = 0.59 (12099.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:51.927536: step 29740, loss = 0.59 (11937.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:52.354294: step 29750, loss = 0.42 (12148.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:52.782793: step 29760, loss = 0.54 (12000.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:53.210051: step 29770, loss = 0.55 (11974.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:53.639954: step 29780, loss = 0.60 (11175.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:54.065561: step 29790, loss = 0.73 (12088.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:54.490644: step 29800, loss = 0.59 (12110.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:55.079690: step 29810, loss = 0.50 (12020.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:55.507463: step 29820, loss = 0.68 (11893.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:55.934338: step 29830, loss = 0.63 (12028.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:56.360927: step 29840, loss = 0.48 (11873.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:56.787028: step 29850, loss = 0.85 (12018.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:57.218970: step 29860, loss = 0.58 (12073.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:57.648084: step 29870, loss = 0.55 (11886.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:58.079124: step 29880, loss = 0.61 (11921.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:58.515402: step 29890, loss = 0.61 (11898.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:58.946970: step 29900, loss = 0.47 (11806.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:59.535714: step 29910, loss = 0.74 (11872.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:35:59.964804: step 29920, loss = 0.53 (11825.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:00.392867: step 29930, loss = 0.61 (11986.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:00.821958: step 29940, loss = 0.63 (12013.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:01.248453: step 29950, loss = 0.75 (12032.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:01.686325: step 29960, loss = 0.53 (11929.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:02.113125: step 29970, loss = 0.60 (12053.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:02.539215: step 29980, loss = 0.75 (11990.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:02.967831: step 29990, loss = 0.59 (11844.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:03.395148: step 30000, loss = 0.66 (11965.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:04.122952: step 30010, loss = 0.63 (11977.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:04.553392: step 30020, loss = 0.48 (11991.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:04.982635: step 30030, loss = 0.57 (11991.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:05.410285: step 30040, loss = 0.48 (11998.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:05.843236: step 30050, loss = 0.59 (11940.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:06.271517: step 30060, loss = 0.55 (11980.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:06.702914: step 30070, loss = 0.62 (11894.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:07.129610: step 30080, loss = 0.47 (12056.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:07.556348: step 30090, loss = 0.57 (11978.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:07.985525: step 30100, loss = 0.49 (11898.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:08.598505: step 30110, loss = 0.41 (11827.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:09.027148: step 30120, loss = 0.42 (12063.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:09.458337: step 30130, loss = 0.65 (11979.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:09.886262: step 30140, loss = 0.57 (12085.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:10.312872: step 30150, loss = 0.48 (12148.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:10.738872: step 30160, loss = 0.60 (12131.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:11.165374: step 30170, loss = 0.61 (12117.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:11.591622: step 30180, loss = 0.52 (11965.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:12.017300: step 30190, loss = 0.52 (12018.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:12.447383: step 30200, loss = 0.59 (11851.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:13.028690: step 30210, loss = 0.60 (12021.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:13.453798: step 30220, loss = 0.55 (12042.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:13.882633: step 30230, loss = 0.53 (11935.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:14.310039: step 30240, loss = 0.48 (12097.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:14.735614: step 30250, loss = 0.48 (12085.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:15.162377: step 30260, loss = 0.66 (12098.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:15.588347: step 30270, loss = 0.72 (11874.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:16.012223: step 30280, loss = 0.54 (12179.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:16.443214: step 30290, loss = 0.46 (12077.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:16.870596: step 30300, loss = 0.62 (11948.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:17.454487: step 30310, loss = 0.53 (12027.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:17.881941: step 30320, loss = 0.59 (11987.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:18.310030: step 30330, loss = 0.52 (11836.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:18.737469: step 30340, loss = 0.62 (12025.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:19.164539: step 30350, loss = 0.48 (11983.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:19.593404: step 30360, loss = 0.50 (11425.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:20.023366: step 30370, loss = 0.58 (11964.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:20.449804: step 30380, loss = 0.52 (11918.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:20.876410: step 30390, loss = 0.59 (12060.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:21.303462: step 30400, loss = 0.57 (12039.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:21.891743: step 30410, loss = 0.54 (12019.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:22.317627: step 30420, loss = 0.56 (12077.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:22.746009: step 30430, loss = 0.60 (11892.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:23.177608: step 30440, loss = 0.49 (11772.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:23.606011: step 30450, loss = 0.61 (11910.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:24.040247: step 30460, loss = 0.55 (11948.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:24.469024: step 30470, loss = 0.52 (12034.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:24.897143: step 30480, loss = 0.69 (11803.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:25.325596: step 30490, loss = 0.47 (11910.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:25.753851: step 30500, loss = 0.50 (11929.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:26.348379: step 30510, loss = 0.52 (12085.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:26.780359: step 30520, loss = 0.50 (11921.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:27.208080: step 30530, loss = 0.60 (11853.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:27.635698: step 30540, loss = 0.56 (11859.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:28.064345: step 30550, loss = 0.58 (12019.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:28.493645: step 30560, loss = 0.52 (11949.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:28.920146: step 30570, loss = 0.53 (12051.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:29.349331: step 30580, loss = 0.44 (11922.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:29.778826: step 30590, loss = 0.61 (12003.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:30.207868: step 30600, loss = 0.89 (11806.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:30.807196: step 30610, loss = 0.58 (11850.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:31.241790: step 30620, loss = 0.57 (10292.4 examples/sec; 0.012 sec/batch)
2017-07-19 19:36:31.667315: step 30630, loss = 0.50 (12125.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:32.093477: step 30640, loss = 0.42 (12038.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:32.527399: step 30650, loss = 0.51 (11957.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:32.955752: step 30660, loss = 0.53 (12014.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:33.382424: step 30670, loss = 0.46 (11901.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:33.809223: step 30680, loss = 0.54 (11928.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:34.237175: step 30690, loss = 0.61 (11893.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:34.664755: step 30700, loss = 0.64 (11925.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:35.253396: step 30710, loss = 0.69 (11947.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:35.680482: step 30720, loss = 0.44 (11967.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:36.106315: step 30730, loss = 0.59 (12046.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:36.532306: step 30740, loss = 0.56 (12069.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:36.959981: step 30750, loss = 0.61 (11968.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:37.386925: step 30760, loss = 0.53 (11990.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:37.814328: step 30770, loss = 0.59 (11961.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:38.242787: step 30780, loss = 0.54 (11557.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:38.669150: step 30790, loss = 0.56 (11901.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:39.096064: step 30800, loss = 0.47 (11903.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:39.684256: step 30810, loss = 0.60 (11894.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:40.109674: step 30820, loss = 0.70 (12130.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:40.537871: step 30830, loss = 0.63 (12008.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:40.964117: step 30840, loss = 0.61 (12078.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:41.390774: step 30850, loss = 0.62 (11907.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:41.818080: step 30860, loss = 0.56 (12008.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:42.253090: step 30870, loss = 0.64 (12025.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:42.688626: step 30880, loss = 0.55 (12014.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:43.116544: step 30890, loss = 0.52 (11978.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:43.544722: step 30900, loss = 0.57 (11893.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:44.143221: step 30910, loss = 0.50 (11886.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:44.580464: step 30920, loss = 0.72 (11876.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:45.008616: step 30930, loss = 0.64 (11889.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:45.445790: step 30940, loss = 0.55 (11125.6 examples/sec; 0.012 sec/batch)
2017-07-19 19:36:45.882534: step 30950, loss = 0.49 (11998.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:46.312807: step 30960, loss = 0.50 (11719.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:46.742144: step 30970, loss = 0.57 (11841.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:47.170197: step 30980, loss = 0.62 (11817.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:47.597013: step 30990, loss = 0.52 (11905.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:48.025004: step 31000, loss = 0.62 (12065.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:48.748971: step 31010, loss = 0.57 (12070.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:49.178120: step 31020, loss = 0.56 (11905.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:49.607815: step 31030, loss = 0.61 (11741.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:50.033399: step 31040, loss = 0.60 (11975.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:50.462217: step 31050, loss = 0.56 (11832.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:50.892604: step 31060, loss = 0.49 (11985.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:51.321560: step 31070, loss = 0.45 (11916.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:51.747779: step 31080, loss = 0.50 (12066.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:52.176998: step 31090, loss = 0.57 (11764.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:52.606022: step 31100, loss = 0.52 (11957.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:53.188744: step 31110, loss = 0.56 (12042.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:53.616625: step 31120, loss = 0.53 (12122.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:54.047846: step 31130, loss = 0.55 (12099.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:54.475152: step 31140, loss = 0.64 (12065.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:54.904007: step 31150, loss = 0.51 (12085.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:55.336240: step 31160, loss = 0.55 (12014.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:55.763664: step 31170, loss = 0.55 (12058.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:56.189947: step 31180, loss = 0.53 (12024.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:56.618679: step 31190, loss = 0.58 (11942.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:57.044610: step 31200, loss = 0.49 (12122.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:57.627849: step 31210, loss = 0.52 (11977.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:58.060686: step 31220, loss = 0.47 (12084.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:58.488491: step 31230, loss = 0.53 (12006.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:58.919661: step 31240, loss = 0.62 (10785.4 examples/sec; 0.012 sec/batch)
2017-07-19 19:36:59.347391: step 31250, loss = 0.57 (11990.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:36:59.780091: step 31260, loss = 0.40 (11020.1 examples/sec; 0.012 sec/batch)
2017-07-19 19:37:00.213498: step 31270, loss = 0.47 (11882.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:00.641457: step 31280, loss = 0.47 (12130.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:01.074678: step 31290, loss = 0.49 (11813.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:01.499963: step 31300, loss = 0.51 (12004.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:02.085570: step 31310, loss = 0.56 (11996.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:02.512207: step 31320, loss = 0.60 (12032.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:02.942212: step 31330, loss = 0.56 (11990.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:03.368441: step 31340, loss = 0.57 (11773.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:03.797718: step 31350, loss = 0.48 (12044.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:04.223369: step 31360, loss = 0.44 (12026.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:04.653502: step 31370, loss = 0.51 (11880.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:05.081547: step 31380, loss = 0.65 (11921.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:05.509188: step 31390, loss = 0.53 (12060.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:05.936452: step 31400, loss = 0.62 (12066.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:06.524842: step 31410, loss = 0.62 (12074.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:06.949957: step 31420, loss = 0.64 (11972.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:07.376939: step 31430, loss = 0.72 (12052.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:07.803956: step 31440, loss = 0.40 (11942.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:08.232164: step 31450, loss = 0.61 (11960.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:08.658119: step 31460, loss = 0.48 (12120.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:09.084784: step 31470, loss = 0.54 (12025.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:09.523692: step 31480, loss = 0.48 (12045.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:09.950805: step 31490, loss = 0.58 (12007.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:10.384117: step 31500, loss = 0.47 (11800.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:10.975022: step 31510, loss = 0.56 (11832.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:11.404202: step 31520, loss = 0.51 (11908.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:11.830982: step 31530, loss = 0.55 (11868.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:12.260352: step 31540, loss = 0.58 (11869.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:12.691874: step 31550, loss = 0.45 (11990.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:13.120518: step 31560, loss = 0.50 (12020.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:13.547609: step 31570, loss = 0.60 (11968.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:13.975832: step 31580, loss = 0.53 (11977.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:14.402197: step 31590, loss = 0.63 (12193.7 examples/sec; 0.010 sec/batch)
2017-07-19 19:37:14.828318: step 31600, loss = 0.57 (12004.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:15.412729: step 31610, loss = 0.48 (11924.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:15.839700: step 31620, loss = 0.50 (11930.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:16.265524: step 31630, loss = 0.61 (12050.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:16.692339: step 31640, loss = 0.63 (12086.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:17.119862: step 31650, loss = 0.63 (11945.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:17.550848: step 31660, loss = 0.55 (10718.1 examples/sec; 0.012 sec/batch)
2017-07-19 19:37:17.977975: step 31670, loss = 0.68 (12010.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:18.408106: step 31680, loss = 0.53 (11991.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:18.842945: step 31690, loss = 0.53 (10571.8 examples/sec; 0.012 sec/batch)
2017-07-19 19:37:19.273410: step 31700, loss = 0.53 (11740.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:19.862239: step 31710, loss = 0.51 (11857.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:20.290466: step 31720, loss = 0.64 (11890.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:20.716800: step 31730, loss = 0.56 (12248.3 examples/sec; 0.010 sec/batch)
2017-07-19 19:37:21.146208: step 31740, loss = 0.54 (11960.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:21.576065: step 31750, loss = 0.46 (12055.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:22.002197: step 31760, loss = 0.62 (12052.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:22.428785: step 31770, loss = 0.53 (11916.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:22.857168: step 31780, loss = 0.63 (12003.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:23.284265: step 31790, loss = 0.47 (12013.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:23.712211: step 31800, loss = 0.49 (12034.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:24.298902: step 31810, loss = 0.61 (12031.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:24.731156: step 31820, loss = 0.48 (10963.4 examples/sec; 0.012 sec/batch)
2017-07-19 19:37:25.164680: step 31830, loss = 0.44 (11109.9 examples/sec; 0.012 sec/batch)
2017-07-19 19:37:25.592323: step 31840, loss = 0.39 (12122.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:26.021064: step 31850, loss = 0.59 (12036.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:26.448010: step 31860, loss = 0.54 (12200.3 examples/sec; 0.010 sec/batch)
2017-07-19 19:37:26.875558: step 31870, loss = 0.55 (11973.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:27.305026: step 31880, loss = 0.54 (11780.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:27.740447: step 31890, loss = 0.62 (12120.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:28.169014: step 31900, loss = 0.54 (12049.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:28.766324: step 31910, loss = 0.64 (11777.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:29.192119: step 31920, loss = 0.55 (12022.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:29.621575: step 31930, loss = 0.59 (11966.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:30.051048: step 31940, loss = 0.61 (11918.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:30.477527: step 31950, loss = 0.59 (11947.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:30.905960: step 31960, loss = 0.59 (12005.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:31.333937: step 31970, loss = 0.56 (11931.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:31.771113: step 31980, loss = 0.53 (11935.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:32.214215: step 31990, loss = 0.50 (11858.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:32.643358: step 32000, loss = 0.56 (11959.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:33.375175: step 32010, loss = 0.49 (11918.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:33.802655: step 32020, loss = 0.50 (11959.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:34.230482: step 32030, loss = 0.55 (12046.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:34.659624: step 32040, loss = 0.50 (11843.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:35.086740: step 32050, loss = 0.60 (12092.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:35.514367: step 32060, loss = 0.52 (12083.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:35.944946: step 32070, loss = 0.74 (11959.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:36.374151: step 32080, loss = 0.63 (12028.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:36.801093: step 32090, loss = 0.47 (11904.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:37.231156: step 32100, loss = 0.62 (11990.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:37.816409: step 32110, loss = 0.52 (12037.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:38.242679: step 32120, loss = 0.49 (12069.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:38.668747: step 32130, loss = 0.49 (12182.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:39.096651: step 32140, loss = 0.46 (12156.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:39.524845: step 32150, loss = 0.47 (11800.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:39.952538: step 32160, loss = 0.59 (11873.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:40.379029: step 32170, loss = 0.53 (12093.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:40.807125: step 32180, loss = 0.56 (11869.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:41.235908: step 32190, loss = 0.55 (11929.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:41.662426: step 32200, loss = 0.59 (11952.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:42.249698: step 32210, loss = 0.49 (11147.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:42.674699: step 32220, loss = 0.53 (12108.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:43.100750: step 32230, loss = 0.51 (12005.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:43.528386: step 32240, loss = 0.48 (12025.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:43.955097: step 32250, loss = 0.52 (11968.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:44.385302: step 32260, loss = 0.66 (12027.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:44.817476: step 32270, loss = 0.43 (12037.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:45.244057: step 32280, loss = 0.59 (11994.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:45.669847: step 32290, loss = 0.62 (12064.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:46.098519: step 32300, loss = 0.57 (12153.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:46.688917: step 32310, loss = 0.55 (11727.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:47.113599: step 32320, loss = 0.55 (12106.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:47.539566: step 32330, loss = 0.76 (11925.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:47.966893: step 32340, loss = 0.51 (11896.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:48.394133: step 32350, loss = 0.47 (12062.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:48.823966: step 32360, loss = 0.53 (11636.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:49.258492: step 32370, loss = 0.55 (11847.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:49.689759: step 32380, loss = 0.54 (11828.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:50.120996: step 32390, loss = 0.50 (11856.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:50.548880: step 32400, loss = 0.57 (11884.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:51.142005: step 32410, loss = 0.58 (12071.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:51.572735: step 32420, loss = 0.66 (12046.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:52.000745: step 32430, loss = 0.52 (11913.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:52.429927: step 32440, loss = 0.66 (11995.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:52.859431: step 32450, loss = 0.63 (11914.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:53.287564: step 32460, loss = 0.61 (11832.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:53.715870: step 32470, loss = 0.53 (11998.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:54.167439: step 32480, loss = 0.58 (11717.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:54.601523: step 32490, loss = 0.65 (11981.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:55.029379: step 32500, loss = 0.59 (12047.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:55.616286: step 32510, loss = 0.53 (11726.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:56.044671: step 32520, loss = 0.44 (11883.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:56.474337: step 32530, loss = 0.47 (11863.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:56.912138: step 32540, loss = 0.60 (11925.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:57.342194: step 32550, loss = 0.52 (12062.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:57.774214: step 32560, loss = 0.58 (11888.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:58.201491: step 32570, loss = 0.55 (11948.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:58.629668: step 32580, loss = 0.57 (11800.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:59.059430: step 32590, loss = 0.55 (12025.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:37:59.486907: step 32600, loss = 0.57 (11972.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:00.074303: step 32610, loss = 0.44 (11991.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:00.507394: step 32620, loss = 0.57 (12048.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:00.934056: step 32630, loss = 0.43 (12023.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:01.362390: step 32640, loss = 0.52 (11783.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:01.791634: step 32650, loss = 0.65 (12109.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:02.218921: step 32660, loss = 0.65 (11876.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:02.645106: step 32670, loss = 0.51 (11964.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:03.071914: step 32680, loss = 0.51 (12126.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:03.500927: step 32690, loss = 0.55 (11953.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:03.933991: step 32700, loss = 0.55 (11957.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:04.519019: step 32710, loss = 0.49 (12097.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:04.946352: step 32720, loss = 0.50 (12065.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:05.375327: step 32730, loss = 0.48 (11986.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:05.805558: step 32740, loss = 0.54 (11942.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:06.253976: step 32750, loss = 0.60 (11875.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:06.682951: step 32760, loss = 0.48 (11964.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:07.111864: step 32770, loss = 0.48 (12008.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:07.540445: step 32780, loss = 0.51 (11970.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:07.971596: step 32790, loss = 0.51 (12073.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:08.404946: step 32800, loss = 0.60 (11843.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:09.003057: step 32810, loss = 0.57 (11977.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:09.439334: step 32820, loss = 0.52 (12163.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:09.866627: step 32830, loss = 0.62 (12052.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:10.295723: step 32840, loss = 0.56 (11804.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:10.724924: step 32850, loss = 0.50 (11914.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:11.154412: step 32860, loss = 0.59 (12088.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:11.583607: step 32870, loss = 0.66 (11498.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:12.011977: step 32880, loss = 0.44 (11966.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:12.439505: step 32890, loss = 0.58 (12065.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:12.867665: step 32900, loss = 0.62 (12008.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:13.461430: step 32910, loss = 0.54 (11889.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:13.889609: step 32920, loss = 0.55 (12066.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:14.318138: step 32930, loss = 0.56 (11978.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:14.747504: step 32940, loss = 0.56 (12099.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:15.182163: step 32950, loss = 0.70 (11674.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:15.611314: step 32960, loss = 0.54 (11965.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:16.040984: step 32970, loss = 0.69 (11706.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:16.469726: step 32980, loss = 0.47 (11716.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:16.897252: step 32990, loss = 0.71 (11954.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:17.320912: step 33000, loss = 0.66 (12323.0 examples/sec; 0.010 sec/batch)
2017-07-19 19:38:18.025028: step 33010, loss = 0.52 (11968.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:18.453775: step 33020, loss = 0.55 (11897.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:18.889902: step 33030, loss = 0.64 (11716.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:19.317957: step 33040, loss = 0.58 (11991.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:19.747724: step 33050, loss = 0.58 (11995.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:20.175699: step 33060, loss = 0.61 (11971.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:20.603853: step 33070, loss = 0.50 (11943.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:21.031587: step 33080, loss = 0.49 (12141.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:21.457966: step 33090, loss = 0.59 (12053.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:21.884931: step 33100, loss = 0.62 (12078.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:22.486808: step 33110, loss = 0.66 (11849.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:22.914756: step 33120, loss = 0.67 (12014.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:23.340419: step 33130, loss = 0.52 (11952.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:23.765858: step 33140, loss = 0.60 (11966.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:24.192774: step 33150, loss = 0.49 (12025.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:24.619297: step 33160, loss = 0.65 (12198.6 examples/sec; 0.010 sec/batch)
2017-07-19 19:38:25.047729: step 33170, loss = 0.67 (12006.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:25.474368: step 33180, loss = 0.60 (11977.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:25.900101: step 33190, loss = 0.61 (11977.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:26.327723: step 33200, loss = 0.55 (11821.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:26.923520: step 33210, loss = 0.58 (10216.4 examples/sec; 0.013 sec/batch)
2017-07-19 19:38:27.351627: step 33220, loss = 0.63 (12083.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:27.778048: step 33230, loss = 0.43 (12089.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:28.203454: step 33240, loss = 0.46 (11963.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:28.631282: step 33250, loss = 0.62 (12046.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:29.056642: step 33260, loss = 0.50 (11987.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:29.484716: step 33270, loss = 0.52 (12065.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:29.913834: step 33280, loss = 0.61 (12032.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:30.342290: step 33290, loss = 0.57 (12067.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:30.767510: step 33300, loss = 0.60 (12100.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:31.354029: step 33310, loss = 0.56 (11695.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:31.781089: step 33320, loss = 0.61 (12063.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:32.207476: step 33330, loss = 0.66 (11994.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:32.636943: step 33340, loss = 0.48 (11837.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:33.074543: step 33350, loss = 0.59 (11869.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:33.507497: step 33360, loss = 0.51 (11975.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:33.934799: step 33370, loss = 0.61 (12046.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:34.366054: step 33380, loss = 0.47 (11974.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:34.795832: step 33390, loss = 0.48 (11747.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:35.222726: step 33400, loss = 0.49 (11925.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:35.815319: step 33410, loss = 0.74 (11944.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:36.244635: step 33420, loss = 0.63 (11872.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:36.674934: step 33430, loss = 0.69 (11824.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:37.104150: step 33440, loss = 0.69 (11864.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:37.530967: step 33450, loss = 0.44 (12052.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:37.960379: step 33460, loss = 0.66 (11974.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:38.396280: step 33470, loss = 0.70 (11953.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:38.826208: step 33480, loss = 0.56 (11820.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:39.256625: step 33490, loss = 0.50 (11882.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:39.691889: step 33500, loss = 0.50 (11824.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:40.282414: step 33510, loss = 0.60 (12078.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:40.712069: step 33520, loss = 0.60 (11938.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:41.139249: step 33530, loss = 0.51 (11998.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:41.566785: step 33540, loss = 0.55 (12097.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:41.992898: step 33550, loss = 0.64 (12094.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:42.420569: step 33560, loss = 0.50 (11742.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:42.848138: step 33570, loss = 0.54 (11984.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:43.280712: step 33580, loss = 0.44 (12076.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:43.713933: step 33590, loss = 0.38 (11977.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:44.142108: step 33600, loss = 0.87 (11929.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:44.728460: step 33610, loss = 0.46 (12115.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:45.155605: step 33620, loss = 0.50 (12026.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:45.592570: step 33630, loss = 0.51 (12176.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:46.025888: step 33640, loss = 0.60 (11969.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:46.452273: step 33650, loss = 0.62 (12009.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:46.880826: step 33660, loss = 0.61 (11950.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:47.305790: step 33670, loss = 0.48 (12030.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:47.734268: step 33680, loss = 0.71 (12035.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:48.158812: step 33690, loss = 0.62 (12132.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:48.585973: step 33700, loss = 0.52 (11901.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:49.184691: step 33710, loss = 0.50 (11968.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:49.624622: step 33720, loss = 0.60 (12136.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:50.056861: step 33730, loss = 0.58 (11871.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:50.486161: step 33740, loss = 0.57 (11933.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:50.914579: step 33750, loss = 0.54 (12046.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:51.341898: step 33760, loss = 0.44 (12012.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:51.773531: step 33770, loss = 0.50 (12102.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:52.202022: step 33780, loss = 0.62 (11769.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:52.632475: step 33790, loss = 0.53 (12042.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:53.061494: step 33800, loss = 0.56 (12007.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:53.649744: step 33810, loss = 0.53 (11946.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:54.077144: step 33820, loss = 0.55 (11968.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:54.507336: step 33830, loss = 0.46 (11899.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:54.935724: step 33840, loss = 0.41 (11945.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:55.362224: step 33850, loss = 0.61 (11991.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:55.794099: step 33860, loss = 0.46 (12020.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:56.220804: step 33870, loss = 0.51 (12021.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:56.651674: step 33880, loss = 0.52 (11954.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:57.080245: step 33890, loss = 0.76 (11981.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:57.506592: step 33900, loss = 0.53 (11998.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:58.101497: step 33910, loss = 0.53 (11910.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:58.531744: step 33920, loss = 0.48 (11795.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:58.959995: step 33930, loss = 0.66 (11861.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:59.388609: step 33940, loss = 0.66 (11826.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:38:59.818974: step 33950, loss = 0.55 (11807.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:00.248710: step 33960, loss = 0.56 (12031.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:00.677404: step 33970, loss = 0.68 (11789.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:01.106141: step 33980, loss = 0.51 (11856.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:01.533038: step 33990, loss = 0.48 (12011.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:01.965764: step 34000, loss = 0.69 (11956.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:02.670741: step 34010, loss = 0.52 (12086.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:03.100704: step 34020, loss = 0.61 (12031.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:03.529222: step 34030, loss = 0.51 (11940.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:03.956496: step 34040, loss = 0.48 (12106.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:04.387231: step 34050, loss = 0.51 (12082.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:04.814929: step 34060, loss = 0.74 (11901.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:05.246343: step 34070, loss = 0.55 (12071.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:05.674328: step 34080, loss = 0.62 (11845.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:06.103642: step 34090, loss = 0.47 (12071.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:06.532137: step 34100, loss = 0.57 (11892.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:07.120546: step 34110, loss = 0.51 (11986.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:07.547388: step 34120, loss = 0.47 (11953.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:07.976864: step 34130, loss = 0.59 (11659.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:08.407753: step 34140, loss = 0.69 (11587.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:08.843859: step 34150, loss = 0.54 (11960.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:09.273685: step 34160, loss = 0.54 (11805.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:09.700039: step 34170, loss = 0.50 (12067.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:10.132257: step 34180, loss = 0.49 (11950.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:10.561027: step 34190, loss = 0.60 (11974.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:10.988337: step 34200, loss = 0.60 (12095.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:11.578661: step 34210, loss = 0.56 (11967.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:12.005981: step 34220, loss = 0.50 (11964.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:12.432225: step 34230, loss = 0.46 (12099.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:12.859245: step 34240, loss = 0.62 (11918.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:13.285724: step 34250, loss = 0.59 (12041.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:13.714968: step 34260, loss = 0.50 (12084.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:14.145261: step 34270, loss = 0.49 (11877.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:14.574499: step 34280, loss = 0.81 (12006.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:15.008912: step 34290, loss = 0.48 (12005.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:15.437189: step 34300, loss = 0.57 (12012.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:16.028025: step 34310, loss = 0.56 (12023.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:16.454229: step 34320, loss = 0.54 (12142.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:16.881137: step 34330, loss = 0.60 (12032.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:17.309723: step 34340, loss = 0.46 (12177.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:17.739005: step 34350, loss = 0.61 (11993.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:18.163862: step 34360, loss = 0.60 (11995.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:18.592280: step 34370, loss = 0.58 (12028.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:19.023234: step 34380, loss = 0.57 (11961.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:19.452871: step 34390, loss = 0.64 (11917.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:19.888141: step 34400, loss = 0.67 (11967.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:20.481135: step 34410, loss = 0.56 (12035.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:20.908886: step 34420, loss = 0.56 (12025.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:21.338019: step 34430, loss = 0.57 (11948.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:21.765579: step 34440, loss = 0.48 (12145.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:22.193819: step 34450, loss = 0.63 (12048.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:22.621023: step 34460, loss = 0.58 (12059.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:23.049623: step 34470, loss = 0.73 (11866.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:23.478185: step 34480, loss = 0.75 (11810.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:23.905512: step 34490, loss = 0.48 (12109.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:24.332805: step 34500, loss = 0.50 (11986.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:24.918053: step 34510, loss = 0.61 (12012.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:25.345111: step 34520, loss = 0.41 (12145.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:25.771893: step 34530, loss = 0.48 (12116.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:26.199489: step 34540, loss = 0.60 (12103.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:26.631755: step 34550, loss = 0.62 (12013.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:27.058759: step 34560, loss = 0.61 (12009.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:27.485559: step 34570, loss = 0.52 (11860.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:27.917060: step 34580, loss = 0.64 (11845.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:28.343774: step 34590, loss = 0.55 (11889.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:28.771983: step 34600, loss = 0.66 (12030.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:29.354188: step 34610, loss = 0.58 (12100.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:29.782061: step 34620, loss = 0.43 (11795.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:30.210874: step 34630, loss = 0.61 (11841.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:30.638173: step 34640, loss = 0.63 (12006.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:31.064940: step 34650, loss = 0.50 (11813.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:31.493304: step 34660, loss = 0.58 (12064.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:31.920385: step 34670, loss = 0.42 (12015.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:32.348183: step 34680, loss = 0.51 (12137.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:32.773776: step 34690, loss = 0.47 (11926.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:33.200559: step 34700, loss = 0.58 (12133.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:33.786770: step 34710, loss = 0.51 (11949.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:34.215339: step 34720, loss = 0.52 (12074.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:34.649481: step 34730, loss = 0.54 (11895.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:35.078507: step 34740, loss = 0.54 (11998.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:35.509773: step 34750, loss = 0.63 (11923.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:35.952074: step 34760, loss = 0.56 (11930.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:36.383498: step 34770, loss = 0.42 (11651.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:36.813380: step 34780, loss = 0.54 (11984.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:37.242528: step 34790, loss = 0.55 (11963.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:37.671476: step 34800, loss = 0.50 (11967.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:38.262280: step 34810, loss = 0.64 (12081.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:38.691967: step 34820, loss = 0.88 (12095.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:39.119553: step 34830, loss = 0.51 (12020.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:39.551602: step 34840, loss = 0.43 (11931.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:39.980621: step 34850, loss = 0.48 (11861.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:40.410683: step 34860, loss = 0.54 (11494.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:40.838101: step 34870, loss = 0.59 (12072.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:41.267653: step 34880, loss = 0.53 (11957.8 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:41.696243: step 34890, loss = 0.72 (12102.5 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:42.124181: step 34900, loss = 0.57 (11967.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:42.715630: step 34910, loss = 0.53 (12034.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:43.145082: step 34920, loss = 0.66 (12002.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:43.572904: step 34930, loss = 0.70 (11819.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:44.001137: step 34940, loss = 0.52 (12013.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:44.429396: step 34950, loss = 0.56 (11988.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:44.856528: step 34960, loss = 0.53 (11932.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:45.282772: step 34970, loss = 0.57 (12006.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:45.709219: step 34980, loss = 0.54 (11960.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:46.140154: step 34990, loss = 0.58 (11977.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:46.566542: step 35000, loss = 0.56 (12102.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:47.271340: step 35010, loss = 0.48 (12034.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:47.697540: step 35020, loss = 0.67 (12038.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:48.125301: step 35030, loss = 0.48 (11952.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:48.552871: step 35040, loss = 0.54 (11990.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:48.985298: step 35050, loss = 0.52 (11893.0 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:49.413547: step 35060, loss = 0.49 (11773.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:49.841827: step 35070, loss = 0.61 (12055.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:50.268291: step 35080, loss = 0.47 (12078.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:50.695820: step 35090, loss = 0.54 (11995.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:51.125720: step 35100, loss = 0.54 (12010.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:51.719548: step 35110, loss = 0.55 (12156.2 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:52.149698: step 35120, loss = 0.47 (12032.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:52.578088: step 35130, loss = 0.54 (11978.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:53.005247: step 35140, loss = 0.57 (12112.7 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:53.438262: step 35150, loss = 0.68 (12057.1 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:53.870432: step 35160, loss = 0.57 (11809.4 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:54.299476: step 35170, loss = 0.46 (11835.3 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:54.730082: step 35180, loss = 0.60 (12059.9 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:55.159687: step 35190, loss = 0.45 (11961.6 examples/sec; 0.011 sec/batch)
2017-07-19 19:39:55.586908: step 35200, loss = 0.51 (12017.8 examples/sec; 0.011 sec/batch)